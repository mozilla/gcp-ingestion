{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GCP Ingestion GCP Ingestion is a monorepo for documentation and implementation of the Mozilla telemetry ingestion system deployed to Google Cloud Platform (GCP). The components are: ingestion-edge : a simple Python service for accepting HTTP messages and delivering to Google Cloud Pub/Sub ingestion-beam : a Java module defining Apache Beam jobs for streaming and batch transformations of ingested messages ingestion-sink (documentation pending): a Java application that runs in Kubernetes, reading input from Google Cloud Pub/Sub and emitting records to batch-oriented outputs like GCS or BigQuery The design behind the system along with various trade offs are documented in the architecture section . Feel free to ask us in #fx-metrics on Slack or #telemetry on chat.mozilla.org if you have specific questions.","title":"Home"},{"location":"#gcp-ingestion","text":"GCP Ingestion is a monorepo for documentation and implementation of the Mozilla telemetry ingestion system deployed to Google Cloud Platform (GCP). The components are: ingestion-edge : a simple Python service for accepting HTTP messages and delivering to Google Cloud Pub/Sub ingestion-beam : a Java module defining Apache Beam jobs for streaming and batch transformations of ingested messages ingestion-sink (documentation pending): a Java application that runs in Kubernetes, reading input from Google Cloud Pub/Sub and emitting records to batch-oriented outputs like GCS or BigQuery The design behind the system along with various trade offs are documented in the architecture section . Feel free to ask us in #fx-metrics on Slack or #telemetry on chat.mozilla.org if you have specific questions.","title":"GCP Ingestion"},{"location":"architecture/bigquery_sink_specification/","text":"Live Sink Service Specification This document specifies the behavior of the service that delivers decoded messages into BigQuery. Data Flow Consume messages from a PubSub topic or Cloud Storage location or BigQuery table and insert them into BigQuery. Send errors to another configurable location. Implementation Execute this as an Apache Beam job. Note: As of February 2020, we are transitioning this sink to a custom Java application running on GKE. Configuration Require configuration for: The input PubSub topic, Cloud Storage location, or BigQuery table The route map from PubSub message attributes to output BigQuery table The error output PubSub topic, Cloud Storage location, or BigQuery table Accept optional configuration for: The fallback output PubSub topic for messages with no route The output mode for BigQuery, default to mixed List of document types to opt-in for streaming when running in mixed output mode The triggering frequency for writing to BigQuery, when output mode is file_loads Coerce Types Reprocess the JSON payload in each message to match the schema of the destination table found in BigQuery as codified by the jsonschema-transpiler . Support the following logical transformations: Transform key names to replace - and . with _ Transform key names beginning with a number by prefixing with _ Transform map types to arrays of key/value maps when the destination field is a repeated STRUCT<key, value> Transform complex types to JSON strings when the destination field in BigQuery expects a string Accumulate Unknown Values As additional_properties Accumulate values that are not present in the destination BigQuery table schema and inject as a JSON string into the payload as additional_properties . This should make it possible to backfill a new column by using JSON operators in the case that a new field was added to a ping in the client before being added to the relevant JSON schema. Unexpected fields should never cause the message to fail insertion. Errors Send all messages that trigger an error described below to the error output. Handle any exceptions when routing and decoding messages by returning them in a separate PCollection . We detect messages that are too large to send to BigQuery and route them to error output by raising a PayloadTooLarge exception. Errors when writing to BigQuery via streaming inserts are returned as a PCollection via the getFailedInserts method. Use InsertRetryPolicy.retryTransientErrors when writing to BigQuery so that retries are handled automatically and all errors returned are non-transient. Error Message Schema Always include the error attributes specified in the Decoded Error Message Schema . Encode errors received as type TableRow as JSON in the payload of a PubsubMessage , and add error attributes. Do not modify errors received as type PubsubMessage except to add error attributes. Other Considerations Message Acks Acknowledge messages in the PubSub topic subscription only after successful delivery to an output. Only deliver messages to a single output.","title":"BigQuery Sink Specification"},{"location":"architecture/bigquery_sink_specification/#live-sink-service-specification","text":"This document specifies the behavior of the service that delivers decoded messages into BigQuery.","title":"Live Sink Service Specification"},{"location":"architecture/bigquery_sink_specification/#data-flow","text":"Consume messages from a PubSub topic or Cloud Storage location or BigQuery table and insert them into BigQuery. Send errors to another configurable location.","title":"Data Flow"},{"location":"architecture/bigquery_sink_specification/#implementation","text":"Execute this as an Apache Beam job. Note: As of February 2020, we are transitioning this sink to a custom Java application running on GKE.","title":"Implementation"},{"location":"architecture/bigquery_sink_specification/#configuration","text":"Require configuration for: The input PubSub topic, Cloud Storage location, or BigQuery table The route map from PubSub message attributes to output BigQuery table The error output PubSub topic, Cloud Storage location, or BigQuery table Accept optional configuration for: The fallback output PubSub topic for messages with no route The output mode for BigQuery, default to mixed List of document types to opt-in for streaming when running in mixed output mode The triggering frequency for writing to BigQuery, when output mode is file_loads","title":"Configuration"},{"location":"architecture/bigquery_sink_specification/#coerce-types","text":"Reprocess the JSON payload in each message to match the schema of the destination table found in BigQuery as codified by the jsonschema-transpiler . Support the following logical transformations: Transform key names to replace - and . with _ Transform key names beginning with a number by prefixing with _ Transform map types to arrays of key/value maps when the destination field is a repeated STRUCT<key, value> Transform complex types to JSON strings when the destination field in BigQuery expects a string","title":"Coerce Types"},{"location":"architecture/bigquery_sink_specification/#accumulate-unknown-values-as-additional_properties","text":"Accumulate values that are not present in the destination BigQuery table schema and inject as a JSON string into the payload as additional_properties . This should make it possible to backfill a new column by using JSON operators in the case that a new field was added to a ping in the client before being added to the relevant JSON schema. Unexpected fields should never cause the message to fail insertion.","title":"Accumulate Unknown Values As additional_properties"},{"location":"architecture/bigquery_sink_specification/#errors","text":"Send all messages that trigger an error described below to the error output. Handle any exceptions when routing and decoding messages by returning them in a separate PCollection . We detect messages that are too large to send to BigQuery and route them to error output by raising a PayloadTooLarge exception. Errors when writing to BigQuery via streaming inserts are returned as a PCollection via the getFailedInserts method. Use InsertRetryPolicy.retryTransientErrors when writing to BigQuery so that retries are handled automatically and all errors returned are non-transient.","title":"Errors"},{"location":"architecture/bigquery_sink_specification/#error-message-schema","text":"Always include the error attributes specified in the Decoded Error Message Schema . Encode errors received as type TableRow as JSON in the payload of a PubsubMessage , and add error attributes. Do not modify errors received as type PubsubMessage except to add error attributes.","title":"Error Message Schema"},{"location":"architecture/bigquery_sink_specification/#other-considerations","text":"","title":"Other Considerations"},{"location":"architecture/bigquery_sink_specification/#message-acks","text":"Acknowledge messages in the PubSub topic subscription only after successful delivery to an output. Only deliver messages to a single output.","title":"Message Acks"},{"location":"architecture/decoder_service_specification/","text":"Decoder Service Specification This document specifies the behavior of the service that decodes messages in the Structured Ingestion pipeline. Data Flow Consume messages from Google Cloud PubSub raw topic Perform GeoIP lookup and drop x_forwarded_for and remote_addr and optionally geo_city based on population Parse the uri attribute to determine document type, etc. Decode the body from base64, optionally decompress, and parse as JSON Scrub the message, checking the content against a list of known signatures that should cause the message to be dropped as toxic, sent to error output, or to have specific fields redacted Validate the schema of the body Extract user agent information and drop user_agent Add metadata fields to message Deduplicate message by docId Generate docId for submission types that don't have one Write message to PubSub decoded topic based on namespace and docType Implementation The above steps will be executed as a single Apache Beam job that can accept either a streaming input from PubSub or a batch input from Cloud Storage. Message deduplication will be done by checking for the presence of ids as keys in Cloud Memory Store (managed Redis). Decoding Errors All messages that are rejected at any step of the Data Flow above will be forwarded to a PubSub error topic for backfill and monitoring purposes. If we determine that a message has already been successfully processed based on docId , we drop the duplicated body and publish just the metadata to the error topic. Error message schema The message that failed decoding, with several additional attributes: ... required group attributes { ... required string error_type // example: \"schema\" required string error_message // example: \"message did not match json schema for <namespace>/<docVersion>/<docType>\" required string exception_class // example: \"java.lang.RuntimeException\" required string stack_trace optional string stack_trace_cause_1 optional string stack_trace_cause_2 optional string stack_trace_cause_3 optional string stack_trace_cause_4 optional string stack_trace_cause_5 } Raw message schema See Edge Service PubSub Message Schema . Decoded message metadata schema Decoded messages published to Pub/Sub will contain the following attributes: required group attributes { ... required string document_version // from uri for non-Telemetry, from message for Telemetry required string document_id // from uri required string document_namespace // from uri required string document_type // from uri optional string app_name // from uri for Telemetry optional string app_version // from uri for Telemetry optional string app_update_channel // from uri for Telemetry optional string app_build_id // from uri for Telemetry optional string geo_country // from geoip lookup optional string geo_subdivision1 // from geoip lookup optional string geo_subdivision2 // from geoip lookup optional string geo_city // from geoip lookup required string submission_timestamp // from edge metadata optional string date // header from client optional string dnt // header from client optional string x_pingsender_version // header from client optional string x_debug_id // header from client optional string user_agent_browser // from user_agent optional string user_agent_browser_version // from user_agent optional string user_agent_os // from user_agent optional string user_agent_os_version // from user_agent optional string normalized_app_name // based on parsed json payload optional string normalized_channel // based on parsed json payload or URI required string normalized_country_code // from geoip lookup optional string normalized_os // based on parsed json payload optional string normalized_os_version // based on parsed json payload optional string sample_id // based on parsed json payload } Many of these fields are also injected into the JSON payload either at the top level or nested inside a metadata object. The schema for injected metadata is maintained under the metadata namespace in mozilla-pipeline-schemas . Other Considerations Message Acks Messages should only be acknowledged in the PubSub raw topic subscription after delivery to either a decoded topic or the error topic. If this is not possible then any time a message is not successfully delivered to PubSub it should by treated as lost data and the appropriate time window will be backfilled from Cloud Storage in batch mode, and appropriate steps will be taken downstream to handle the backfill. Deployments should always terminate functional pipelines using the drain method, to ensure ack'd messages are fully delivered. Deduplication Each docId will be allowed through \"at least once\", and only be rejected as a duplicate if we have completed delivery of a message with the same docId . Duplicates will be considered errors and sent to the error topic. \"Exactly once\" semantics can be applied to derived data sets using SQL in BigQuery, and GroupByKey in Beam and Spark. Note that deduplication is only provided with a \"best effort\" quality of service. In the ideal case, we hold 24 hours of history for seen document IDs, but that buffer is allowed to degrade to a shorter time window when the pipeline is under high load.","title":"Decoder Service Specification"},{"location":"architecture/decoder_service_specification/#decoder-service-specification","text":"This document specifies the behavior of the service that decodes messages in the Structured Ingestion pipeline.","title":"Decoder Service Specification"},{"location":"architecture/decoder_service_specification/#data-flow","text":"Consume messages from Google Cloud PubSub raw topic Perform GeoIP lookup and drop x_forwarded_for and remote_addr and optionally geo_city based on population Parse the uri attribute to determine document type, etc. Decode the body from base64, optionally decompress, and parse as JSON Scrub the message, checking the content against a list of known signatures that should cause the message to be dropped as toxic, sent to error output, or to have specific fields redacted Validate the schema of the body Extract user agent information and drop user_agent Add metadata fields to message Deduplicate message by docId Generate docId for submission types that don't have one Write message to PubSub decoded topic based on namespace and docType","title":"Data Flow"},{"location":"architecture/decoder_service_specification/#implementation","text":"The above steps will be executed as a single Apache Beam job that can accept either a streaming input from PubSub or a batch input from Cloud Storage. Message deduplication will be done by checking for the presence of ids as keys in Cloud Memory Store (managed Redis).","title":"Implementation"},{"location":"architecture/decoder_service_specification/#decoding-errors","text":"All messages that are rejected at any step of the Data Flow above will be forwarded to a PubSub error topic for backfill and monitoring purposes. If we determine that a message has already been successfully processed based on docId , we drop the duplicated body and publish just the metadata to the error topic.","title":"Decoding Errors"},{"location":"architecture/decoder_service_specification/#error-message-schema","text":"The message that failed decoding, with several additional attributes: ... required group attributes { ... required string error_type // example: \"schema\" required string error_message // example: \"message did not match json schema for <namespace>/<docVersion>/<docType>\" required string exception_class // example: \"java.lang.RuntimeException\" required string stack_trace optional string stack_trace_cause_1 optional string stack_trace_cause_2 optional string stack_trace_cause_3 optional string stack_trace_cause_4 optional string stack_trace_cause_5 }","title":"Error message schema"},{"location":"architecture/decoder_service_specification/#raw-message-schema","text":"See Edge Service PubSub Message Schema .","title":"Raw message schema"},{"location":"architecture/decoder_service_specification/#decoded-message-metadata-schema","text":"Decoded messages published to Pub/Sub will contain the following attributes: required group attributes { ... required string document_version // from uri for non-Telemetry, from message for Telemetry required string document_id // from uri required string document_namespace // from uri required string document_type // from uri optional string app_name // from uri for Telemetry optional string app_version // from uri for Telemetry optional string app_update_channel // from uri for Telemetry optional string app_build_id // from uri for Telemetry optional string geo_country // from geoip lookup optional string geo_subdivision1 // from geoip lookup optional string geo_subdivision2 // from geoip lookup optional string geo_city // from geoip lookup required string submission_timestamp // from edge metadata optional string date // header from client optional string dnt // header from client optional string x_pingsender_version // header from client optional string x_debug_id // header from client optional string user_agent_browser // from user_agent optional string user_agent_browser_version // from user_agent optional string user_agent_os // from user_agent optional string user_agent_os_version // from user_agent optional string normalized_app_name // based on parsed json payload optional string normalized_channel // based on parsed json payload or URI required string normalized_country_code // from geoip lookup optional string normalized_os // based on parsed json payload optional string normalized_os_version // based on parsed json payload optional string sample_id // based on parsed json payload } Many of these fields are also injected into the JSON payload either at the top level or nested inside a metadata object. The schema for injected metadata is maintained under the metadata namespace in mozilla-pipeline-schemas .","title":"Decoded message metadata schema"},{"location":"architecture/decoder_service_specification/#other-considerations","text":"","title":"Other Considerations"},{"location":"architecture/decoder_service_specification/#message-acks","text":"Messages should only be acknowledged in the PubSub raw topic subscription after delivery to either a decoded topic or the error topic. If this is not possible then any time a message is not successfully delivered to PubSub it should by treated as lost data and the appropriate time window will be backfilled from Cloud Storage in batch mode, and appropriate steps will be taken downstream to handle the backfill. Deployments should always terminate functional pipelines using the drain method, to ensure ack'd messages are fully delivered.","title":"Message Acks"},{"location":"architecture/decoder_service_specification/#deduplication","text":"Each docId will be allowed through \"at least once\", and only be rejected as a duplicate if we have completed delivery of a message with the same docId . Duplicates will be considered errors and sent to the error topic. \"Exactly once\" semantics can be applied to derived data sets using SQL in BigQuery, and GroupByKey in Beam and Spark. Note that deduplication is only provided with a \"best effort\" quality of service. In the ideal case, we hold 24 hours of history for seen document IDs, but that buffer is allowed to degrade to a shorter time window when the pipeline is under high load.","title":"Deduplication"},{"location":"architecture/differences_from_aws/","text":"Differences from AWS This document explains how GCP Ingestion differs from the AWS Data Platform Architecture . Replace Heka Framed Protobuf with newline delimited JSON Heka framed protobuf requires special code to read and write. Newline delimited JSON is readable by BigQuery, Dataflow, and Spark using standard libraries. JSON doesn't enforce a schema, so it can be used to store data with an incomplete schema and be used to backfill missing columns. Replace EC2 Edge with Kubernetes Edge The AWS data platform uses EC2 instances running an NGinX module to encode HTTP requests as Heka messages and then write them to Kafka using librdkafka and directly to files on disk. librdkafka handles buffering and batching when writing to Kafka. Files on disk are rotated with cron and uploaded to S3. On shutdown files are forcefully rotated and uploaded. The sizing of the EC2 instance cluster is effectively static, but is configured to scale up if needed. The EC2 instances have been replaced with a Kubernetes cluster. This decision was made by the Cloud Operations team to simplify operational support for them. The NGinX module has been replaced by an HTTP service running in Docker. A number of factors informed the decision to rewrite the edge, including: The PubSub equivalent of librdkafka is the google client libraries , which do not have a C implementation We can simplify the edge by uploading to landfill after PubSub while remaining resilient to Dataflow and PubSub failures, because PubSub durably stores unacknowledged messages for 7 days We can simplify disaster recovery by Ensuring that all data eventually flows through PubSub In the AWS edge data only flows to at least one of Kafka or landfill We can allow Kubernetes to auto scale when PubSub is available by only queuing requests to disk only when they cannot be delivered to PubSub We can ensure that data is not lost on shutdown by disabling auto scaling down when there are requests on disk Replace Kafka with PubSub Comparison: Kafka in AWS Data Pipeline PubSub Managed by Ops Google Access control Security groups, all-or-nothing Cloud IAM, per-topic Scaling Manual Automatic Cost Per EC2 instance Per GB, min charge 1 KB/req Data Storage Configured in GB per EC2 instance 7 days for unacknowledged Cross region no yes Replace Hindsight Data Warehouse Loaders with Dataflow Dataflow advantages: Connectors for PubSub, Cloud Storage, and BigQuery built-in Seamlessly supports streaming and batch sources and sinks Runs on managed service and has simple local runner for testing and development Auto scales on input volume Replace S3 with Cloud Storage They are equivalent products for the different cloud vendors. Messages Always Delivered to Message Queue In AWS the edge aimed to ensure messages were always delivered to either Kafka or landfill, and in the case of an outage one could be backfilled from the other. On GCP the Kubernetes edge aims to ensure messages are always delivered to PubSub. This ensures that consumers from PubSub never miss data unless they fall too far behind. It also allows landfill to be handled downstream from PubSub (see below). Landfill is Downstream from Message Queue In AWS, the failsafe data store was upstream of the message queue (Kafka). On GCP, the failsafe data store is downstream from the message queue (PubSub). This makes the edge and Dataflow landfill loader simpler. The edge doesn\u2019t have to ensure that pending messages are safely offloaded on shutdown, because messages are only left pending when PubSub is unavailable, and scaling down is disabled while messages are pending. The Dataflow landfill loader doesn\u2019t have to ensure pending messages are safely offloaded because it only acks messages after they are uploaded, ensuring at least once delivery. This design is possible because of two changes compared to our AWS implementation. First, the Kubernetes edge eventually delivers all messages to PubSub. In AWS if Kafka were down then messages would only be delivered directly to landfill, and would never flow through Kafka. This change ensures that if all messages are consumed from PubSub then no messages have been skipped. Second, PubSub stores unacknowledged messages for 7 days. In AWS Kafka stores messages for 2-3 days, depending on topic and total message volume. This change ensures that we have sufficient time to reliably consume all messages before they are dropped from the queue, even if total message volume changes dramatically or consumers are not highly available and suffer an outage over a holiday weekend.","title":"Differences from AWS"},{"location":"architecture/differences_from_aws/#differences-from-aws","text":"This document explains how GCP Ingestion differs from the AWS Data Platform Architecture .","title":"Differences from AWS"},{"location":"architecture/differences_from_aws/#replace-heka-framed-protobuf-with-newline-delimited-json","text":"Heka framed protobuf requires special code to read and write. Newline delimited JSON is readable by BigQuery, Dataflow, and Spark using standard libraries. JSON doesn't enforce a schema, so it can be used to store data with an incomplete schema and be used to backfill missing columns.","title":"Replace Heka Framed Protobuf with newline delimited JSON"},{"location":"architecture/differences_from_aws/#replace-ec2-edge-with-kubernetes-edge","text":"The AWS data platform uses EC2 instances running an NGinX module to encode HTTP requests as Heka messages and then write them to Kafka using librdkafka and directly to files on disk. librdkafka handles buffering and batching when writing to Kafka. Files on disk are rotated with cron and uploaded to S3. On shutdown files are forcefully rotated and uploaded. The sizing of the EC2 instance cluster is effectively static, but is configured to scale up if needed. The EC2 instances have been replaced with a Kubernetes cluster. This decision was made by the Cloud Operations team to simplify operational support for them. The NGinX module has been replaced by an HTTP service running in Docker. A number of factors informed the decision to rewrite the edge, including: The PubSub equivalent of librdkafka is the google client libraries , which do not have a C implementation We can simplify the edge by uploading to landfill after PubSub while remaining resilient to Dataflow and PubSub failures, because PubSub durably stores unacknowledged messages for 7 days We can simplify disaster recovery by Ensuring that all data eventually flows through PubSub In the AWS edge data only flows to at least one of Kafka or landfill We can allow Kubernetes to auto scale when PubSub is available by only queuing requests to disk only when they cannot be delivered to PubSub We can ensure that data is not lost on shutdown by disabling auto scaling down when there are requests on disk","title":"Replace EC2 Edge with Kubernetes Edge"},{"location":"architecture/differences_from_aws/#replace-kafka-with-pubsub","text":"Comparison: Kafka in AWS Data Pipeline PubSub Managed by Ops Google Access control Security groups, all-or-nothing Cloud IAM, per-topic Scaling Manual Automatic Cost Per EC2 instance Per GB, min charge 1 KB/req Data Storage Configured in GB per EC2 instance 7 days for unacknowledged Cross region no yes","title":"Replace Kafka with PubSub"},{"location":"architecture/differences_from_aws/#replace-hindsight-data-warehouse-loaders-with-dataflow","text":"Dataflow advantages: Connectors for PubSub, Cloud Storage, and BigQuery built-in Seamlessly supports streaming and batch sources and sinks Runs on managed service and has simple local runner for testing and development Auto scales on input volume","title":"Replace Hindsight Data Warehouse Loaders with Dataflow"},{"location":"architecture/differences_from_aws/#replace-s3-with-cloud-storage","text":"They are equivalent products for the different cloud vendors.","title":"Replace S3 with Cloud Storage"},{"location":"architecture/differences_from_aws/#messages-always-delivered-to-message-queue","text":"In AWS the edge aimed to ensure messages were always delivered to either Kafka or landfill, and in the case of an outage one could be backfilled from the other. On GCP the Kubernetes edge aims to ensure messages are always delivered to PubSub. This ensures that consumers from PubSub never miss data unless they fall too far behind. It also allows landfill to be handled downstream from PubSub (see below).","title":"Messages Always Delivered to Message Queue"},{"location":"architecture/differences_from_aws/#landfill-is-downstream-from-message-queue","text":"In AWS, the failsafe data store was upstream of the message queue (Kafka). On GCP, the failsafe data store is downstream from the message queue (PubSub). This makes the edge and Dataflow landfill loader simpler. The edge doesn\u2019t have to ensure that pending messages are safely offloaded on shutdown, because messages are only left pending when PubSub is unavailable, and scaling down is disabled while messages are pending. The Dataflow landfill loader doesn\u2019t have to ensure pending messages are safely offloaded because it only acks messages after they are uploaded, ensuring at least once delivery. This design is possible because of two changes compared to our AWS implementation. First, the Kubernetes edge eventually delivers all messages to PubSub. In AWS if Kafka were down then messages would only be delivered directly to landfill, and would never flow through Kafka. This change ensures that if all messages are consumed from PubSub then no messages have been skipped. Second, PubSub stores unacknowledged messages for 7 days. In AWS Kafka stores messages for 2-3 days, depending on topic and total message volume. This change ensures that we have sufficient time to reliably consume all messages before they are dropped from the queue, even if total message volume changes dramatically or consumers are not highly available and suffer an outage over a holiday weekend.","title":"Landfill is Downstream from Message Queue"},{"location":"architecture/edge_migration_plan/","text":"Plans for migrating edge traffic to GCP Ingestion This document outlines plans to migrate edge traffic from AWS to GCP using the code in this repository. Current state Today, data producers send data to the ingestion stack on AWS as described here . Phase 1 Timeline: Q4 2018 Data Producers -> AWS Edge -> Kafka -> AWS Hindsight -> PubSub -> GCP Hindsight w/ HTTP Output -> GCP Edge In this configuration we get 100% of \"expected\" data going to the GCP pipeline without any risk of affecting production AWS processing. This \"expected\" data does not include recording and replaying traffic that we currently throw away at the edge (e.g. data from Firefox versions prior to unified telemetry). There may be some other subtle differences from the original incoming data from producers, such as missing some headers that are not currently being stored in Landfill. On the whole, this is a good approximation of 100% of the data we actually care about. We also need to test operation of the new system while processing data we don't care about; see more detail in Phase 3 below. During this phase, we will collect logging info to determine exactly what data is being ignored by limiting to this \"expected\" data. Why have a GCP PubSub topic instead of running an HTTP output from the AWS consumer directly? The main reason is that we want 100% of data in a PubSub topic anyway, for staging purposes. This way we can have a production GCP Edge ingestion stack writing to production GCP resources, while being able to e.g. simulate load and various outage conditions using the same data in a staging environment. We could in theory do the stage testing using the prod GCP edge output topic, assuming the edge has no issues. This will be the eventual end state of the system when there's no more AWS, but we are currently reusing most of the hindsight tooling for stage testing since it's already written and working in production. Phase 2 Timeline: possibly Q4 2018 Data Producers -> AWS Edge -> Kafka -> AWS Hindsight w/ HTTP Output -> GCP Edge We continue to write the Hindsight PubSub topic from Phase 1, but we move the HTTP output to the AWS side. This will help us better empirically determine performance and various other implications of cross-cloud requests without potentially affecting production AWS processing. We can still use the GCP PubSub topic for stage testing, but it won't necessarily be actively used and the production GCP Edge will be receiving its data directly from AWS via HTTP POST. Phase 3 Timeline: 2019 Data Producers -> AWS Tee -> AWS Edge \\ `-> GCP Edge This is how we did the last major migration from Heka to Hindsight . This architecture introduces risk of data loss, so should be considered more dangerous than previous phases. This is why we are not planning on doing this until we're reasonably confident in the efficacy of the GCP infrastructure. In active tee mode, the client will be affected by GCP edge processing, particularly in request processing time and potentially by its status code. Depending on how we configure the tee, the AWS data ingestion infrastructure is susceptible to data duplication or loss due to client retry behavior. We should ensure that we are sufficiently confident in the behavior, performance, and stability of the GCP Edge before we move to this phase to ensure things don't go south. The previous phases are safer and should let us discover any major issues before we proceed to this phase. This does the \"last mile\" testing of most of the major missing pieces from earlier phases, in addition to prepping for the eventual fourth phase. Phase 3 (alternative) Data Producers -> Weighted DNS -> AWS Edge \\ `-> GCP edge This alternative does not make use of a Tee to duplicate traffic. Depending on the results of Phase 2, an alternative strategy to teeing would be weighted DNS and running two GCP edges. In this configuration we could e.g. tee 1% of data directly to one of the GCP edges, while having the other 99% be processed by the other edge as per earlier phases. We would then need to do the reverse of Phase 1 and write that 1% back to AWS + Kafka for the AWS ingestion stack to process. This strategy can be just as dangerous as using the OpenResty tee because data loss on either side may result in partial data to both, and also requires writing code to convert the GCP representation back to the AWS representation. The risk can be mitigated by using the DNS weights to adjust the amount of data being sent directly to GCP, which is an advantage. This is an interesting variation more similar to a standard DNS cut-over, if required. Phase 4 Timeline: To Be Determined Data Producers -> GCP Edge This will happen after we are confident that there is no risk of data loss by switching the endpoint to the GCP stack. It will also depend on the logistics and timing of sunsetting systems and components in AWS.","title":"Edge Migration Plan"},{"location":"architecture/edge_migration_plan/#plans-for-migrating-edge-traffic-to-gcp-ingestion","text":"This document outlines plans to migrate edge traffic from AWS to GCP using the code in this repository.","title":"Plans for migrating edge traffic to GCP Ingestion"},{"location":"architecture/edge_migration_plan/#current-state","text":"Today, data producers send data to the ingestion stack on AWS as described here .","title":"Current state"},{"location":"architecture/edge_migration_plan/#phase-1","text":"Timeline: Q4 2018 Data Producers -> AWS Edge -> Kafka -> AWS Hindsight -> PubSub -> GCP Hindsight w/ HTTP Output -> GCP Edge In this configuration we get 100% of \"expected\" data going to the GCP pipeline without any risk of affecting production AWS processing. This \"expected\" data does not include recording and replaying traffic that we currently throw away at the edge (e.g. data from Firefox versions prior to unified telemetry). There may be some other subtle differences from the original incoming data from producers, such as missing some headers that are not currently being stored in Landfill. On the whole, this is a good approximation of 100% of the data we actually care about. We also need to test operation of the new system while processing data we don't care about; see more detail in Phase 3 below. During this phase, we will collect logging info to determine exactly what data is being ignored by limiting to this \"expected\" data. Why have a GCP PubSub topic instead of running an HTTP output from the AWS consumer directly? The main reason is that we want 100% of data in a PubSub topic anyway, for staging purposes. This way we can have a production GCP Edge ingestion stack writing to production GCP resources, while being able to e.g. simulate load and various outage conditions using the same data in a staging environment. We could in theory do the stage testing using the prod GCP edge output topic, assuming the edge has no issues. This will be the eventual end state of the system when there's no more AWS, but we are currently reusing most of the hindsight tooling for stage testing since it's already written and working in production.","title":"Phase 1"},{"location":"architecture/edge_migration_plan/#phase-2","text":"Timeline: possibly Q4 2018 Data Producers -> AWS Edge -> Kafka -> AWS Hindsight w/ HTTP Output -> GCP Edge We continue to write the Hindsight PubSub topic from Phase 1, but we move the HTTP output to the AWS side. This will help us better empirically determine performance and various other implications of cross-cloud requests without potentially affecting production AWS processing. We can still use the GCP PubSub topic for stage testing, but it won't necessarily be actively used and the production GCP Edge will be receiving its data directly from AWS via HTTP POST.","title":"Phase 2"},{"location":"architecture/edge_migration_plan/#phase-3","text":"Timeline: 2019 Data Producers -> AWS Tee -> AWS Edge \\ `-> GCP Edge This is how we did the last major migration from Heka to Hindsight . This architecture introduces risk of data loss, so should be considered more dangerous than previous phases. This is why we are not planning on doing this until we're reasonably confident in the efficacy of the GCP infrastructure. In active tee mode, the client will be affected by GCP edge processing, particularly in request processing time and potentially by its status code. Depending on how we configure the tee, the AWS data ingestion infrastructure is susceptible to data duplication or loss due to client retry behavior. We should ensure that we are sufficiently confident in the behavior, performance, and stability of the GCP Edge before we move to this phase to ensure things don't go south. The previous phases are safer and should let us discover any major issues before we proceed to this phase. This does the \"last mile\" testing of most of the major missing pieces from earlier phases, in addition to prepping for the eventual fourth phase.","title":"Phase 3"},{"location":"architecture/edge_migration_plan/#phase-3-alternative","text":"Data Producers -> Weighted DNS -> AWS Edge \\ `-> GCP edge This alternative does not make use of a Tee to duplicate traffic. Depending on the results of Phase 2, an alternative strategy to teeing would be weighted DNS and running two GCP edges. In this configuration we could e.g. tee 1% of data directly to one of the GCP edges, while having the other 99% be processed by the other edge as per earlier phases. We would then need to do the reverse of Phase 1 and write that 1% back to AWS + Kafka for the AWS ingestion stack to process. This strategy can be just as dangerous as using the OpenResty tee because data loss on either side may result in partial data to both, and also requires writing code to convert the GCP representation back to the AWS representation. The risk can be mitigated by using the DNS weights to adjust the amount of data being sent directly to GCP, which is an advantage. This is an interesting variation more similar to a standard DNS cut-over, if required.","title":"Phase 3 (alternative)"},{"location":"architecture/edge_migration_plan/#phase-4","text":"Timeline: To Be Determined Data Producers -> GCP Edge This will happen after we are confident that there is no risk of data loss by switching the endpoint to the GCP stack. It will also depend on the logistics and timing of sunsetting systems and components in AWS.","title":"Phase 4"},{"location":"architecture/edge_service_specification/","text":"Edge Service Specification This document specifies the behavior of the server that accepts submissions from HTTP clients e.g. Firefox telemetry. General Data Flow HTTP submissions come in from the wild, hit a load balancer, then optionally an nginx proxy, then the HTTP edge server described in this document. Data is accepted via a POST/PUT request from clients, which the server will wrap in a PubSub message and forward to Google Cloud PubSub, where any further processing, analysis, and storage will be handled. Namespaces Namespaces are used to control the processing of data from different types of clients, from the metadata that is collected to the destinations where the data is written, processed and accessible. Data sent to a namespace that is not specifically configured is assumed to be in the non-Telemetry JSON format described here . To request a new namespace configuration file a bug against the Data Platform Team with a short description of what the namespace will be used for and the desired configuration options. Forwarding to the pipeline The message is written to PubSub. If the message cannot be written to PubSub it is written to a disk queue that will periodically retry writing to PubSub. PubSub Message Schema required string data // base64 encoded body required group attributes { required string submission_timestamp // server time, ISO 8601 with microseconds and timezone \"Z\", example: \"2018-03-12T21:02:18.123456Z\" required string uri // example: \"/submit/telemetry/6c49ec73-4350-45a0-9c8a-6c8f5aded0cf/main/Firefox/58.0.2/release/20180206200532\" required string protocol // example: \"HTTP/1.1\" required string method // example: \"POST\" optional string args // query parameters, example: \"v=4\" // Headers optional string remote_addr // usually a load balancer, example: \"172.31.32.5\" optional string content_length // example: \"4722\" optional string date // example: \"Mon, 12 Mar 2018 21:02:18 GMT\" optional string dnt // example: \"1\" optional string host // example: \"incoming.telemetry.mozilla.org\" optional string user_agent // example: \"pingsender/1.0\" optional string x_forwarded_for // example: \"10.98.132.74, 103.3.237.12\" optional string x_pingsender_version // example: \"1.0\" optional string x_debug_id // example: \"my_debug_session_1\" optional string x_pipeline_proxy // time that the AWS->GCP tee received the message, example: \"2018-03-12T21:02:18.123456Z\" } Server Request/Response GET Request Endpoint Description /__heartbeat__ check if service is healthy, and can reach PubSub or has space to store requests on disk /__lbheartbeat__ check if service is running /__version__ return Dockerflow version object GET Response codes 200 - ok, check succeeded 204 - ok, check succeeded, no response body 404 - not found, check doesn't exist 500 - all is not well 507 - insufficient storage, should occur at some configurable limit before disk is full POST/PUT Request Treat POST and PUT the same. Accept POST or PUT to URLs of the form ^/submit/namespace/[/dimensions]$ Example Telemetry format: /submit/telemetry/docId/docType/appName/appVersion/appUpdateChannel/appBuildID Specific Telemetry example: /submit/telemetry/ce39b608-f595-4c69-b6a6-f7a436604648/main/Firefox/61.0a1/nightly/20180328030202 Example non-Telemetry format: /submit/namespace/docType/docVersion/docId Specific non-Telemetry example: /submit/eng-workflow/hgpush/1/2c3a0767-d84a-4d02-8a92-fa54a3376049 Note that docId above is a unique document ID, which is used for de-duping submissions. This is not intended to be the clientId field from Telemetry. docId is required and must be a UUID . Legacy Systems Accept TLS Error Reports as POST or PUT to /submit/sslreports with no docType , docVersion , or docId . Accept Stub Installer pings as GET to /stub/[docVersion]/[dimensions] , with no docType or docId , and over both HTTP and HTTPS. Use POST/PUT Response codes , even though this endpoint is for GET requests. POST/PUT Response codes 200 - ok, request accepted into the pipeline 400 - bad request, for example an unencoded space in the URL 404 - not found, for example using a telemetry format URL in a non-telemetry namespace or vice-versa 411 - missing content-length header 413 - request body too large (note that if we have badly-behaved clients that retry on 4XX , we should send back 202 on body/path too long). 414 - request path too long (see above) 500 - internal error 507 - insufficient storage, request failed because disk is full Other Response codes 405 - wrong request type (anything other than GET|POST|PUT) Other Considerations Compression It is not desirable to do decompression on the edge node. We want to pass along messages from the HTTP Edge node without \"cracking the egg\" of the payload. We may also receive badly formed payloads, and we will want to track the incidence of such things. Bad Messages Since the actual message is not examined by the edge server the only failures that occur are defined by the response status codes above. Messages are only forwarded to the pipeline when a response code of 200 is returned to the client. PubSub Topics All messages that sent a response code of 200 are forwarded to a single PubSub topic for decoding and landfill. GeoIP Lookups No GeoIP lookup is performed by the edge server. If a client IP is available then the PubSub consumer performs the lookup and then discards the IP before the message is forwarded to a decoded PubSub topic. Data Retention The edge server only stores data when PubSub cannot be reached, and removes data after it is successfully written to PubSub. Down scaling will be disabled for the Kubernetes pod and cluster when data is being stored, so that data is not lost. Submission Timestamp Format submission_timestamp is formatted as ISO 8601 with microseconds and timezone, because it is compatible with BigQuery's Timestamp Type , so that the field doesn't need transformation.","title":"Edge Server Specification"},{"location":"architecture/edge_service_specification/#edge-service-specification","text":"This document specifies the behavior of the server that accepts submissions from HTTP clients e.g. Firefox telemetry.","title":"Edge Service Specification"},{"location":"architecture/edge_service_specification/#general-data-flow","text":"HTTP submissions come in from the wild, hit a load balancer, then optionally an nginx proxy, then the HTTP edge server described in this document. Data is accepted via a POST/PUT request from clients, which the server will wrap in a PubSub message and forward to Google Cloud PubSub, where any further processing, analysis, and storage will be handled.","title":"General Data Flow"},{"location":"architecture/edge_service_specification/#namespaces","text":"Namespaces are used to control the processing of data from different types of clients, from the metadata that is collected to the destinations where the data is written, processed and accessible. Data sent to a namespace that is not specifically configured is assumed to be in the non-Telemetry JSON format described here . To request a new namespace configuration file a bug against the Data Platform Team with a short description of what the namespace will be used for and the desired configuration options.","title":"Namespaces"},{"location":"architecture/edge_service_specification/#forwarding-to-the-pipeline","text":"The message is written to PubSub. If the message cannot be written to PubSub it is written to a disk queue that will periodically retry writing to PubSub.","title":"Forwarding to the pipeline"},{"location":"architecture/edge_service_specification/#pubsub-message-schema","text":"required string data // base64 encoded body required group attributes { required string submission_timestamp // server time, ISO 8601 with microseconds and timezone \"Z\", example: \"2018-03-12T21:02:18.123456Z\" required string uri // example: \"/submit/telemetry/6c49ec73-4350-45a0-9c8a-6c8f5aded0cf/main/Firefox/58.0.2/release/20180206200532\" required string protocol // example: \"HTTP/1.1\" required string method // example: \"POST\" optional string args // query parameters, example: \"v=4\" // Headers optional string remote_addr // usually a load balancer, example: \"172.31.32.5\" optional string content_length // example: \"4722\" optional string date // example: \"Mon, 12 Mar 2018 21:02:18 GMT\" optional string dnt // example: \"1\" optional string host // example: \"incoming.telemetry.mozilla.org\" optional string user_agent // example: \"pingsender/1.0\" optional string x_forwarded_for // example: \"10.98.132.74, 103.3.237.12\" optional string x_pingsender_version // example: \"1.0\" optional string x_debug_id // example: \"my_debug_session_1\" optional string x_pipeline_proxy // time that the AWS->GCP tee received the message, example: \"2018-03-12T21:02:18.123456Z\" }","title":"PubSub Message Schema"},{"location":"architecture/edge_service_specification/#server-requestresponse","text":"","title":"Server Request/Response"},{"location":"architecture/edge_service_specification/#get-request","text":"Endpoint Description /__heartbeat__ check if service is healthy, and can reach PubSub or has space to store requests on disk /__lbheartbeat__ check if service is running /__version__ return Dockerflow version object","title":"GET Request"},{"location":"architecture/edge_service_specification/#get-response-codes","text":"200 - ok, check succeeded 204 - ok, check succeeded, no response body 404 - not found, check doesn't exist 500 - all is not well 507 - insufficient storage, should occur at some configurable limit before disk is full","title":"GET Response codes"},{"location":"architecture/edge_service_specification/#postput-request","text":"Treat POST and PUT the same. Accept POST or PUT to URLs of the form ^/submit/namespace/[/dimensions]$ Example Telemetry format: /submit/telemetry/docId/docType/appName/appVersion/appUpdateChannel/appBuildID Specific Telemetry example: /submit/telemetry/ce39b608-f595-4c69-b6a6-f7a436604648/main/Firefox/61.0a1/nightly/20180328030202 Example non-Telemetry format: /submit/namespace/docType/docVersion/docId Specific non-Telemetry example: /submit/eng-workflow/hgpush/1/2c3a0767-d84a-4d02-8a92-fa54a3376049 Note that docId above is a unique document ID, which is used for de-duping submissions. This is not intended to be the clientId field from Telemetry. docId is required and must be a UUID .","title":"POST/PUT Request"},{"location":"architecture/edge_service_specification/#legacy-systems","text":"Accept TLS Error Reports as POST or PUT to /submit/sslreports with no docType , docVersion , or docId . Accept Stub Installer pings as GET to /stub/[docVersion]/[dimensions] , with no docType or docId , and over both HTTP and HTTPS. Use POST/PUT Response codes , even though this endpoint is for GET requests.","title":"Legacy Systems"},{"location":"architecture/edge_service_specification/#postput-response-codes","text":"200 - ok, request accepted into the pipeline 400 - bad request, for example an unencoded space in the URL 404 - not found, for example using a telemetry format URL in a non-telemetry namespace or vice-versa 411 - missing content-length header 413 - request body too large (note that if we have badly-behaved clients that retry on 4XX , we should send back 202 on body/path too long). 414 - request path too long (see above) 500 - internal error 507 - insufficient storage, request failed because disk is full","title":"POST/PUT Response codes"},{"location":"architecture/edge_service_specification/#other-response-codes","text":"405 - wrong request type (anything other than GET|POST|PUT)","title":"Other Response codes"},{"location":"architecture/edge_service_specification/#other-considerations","text":"","title":"Other Considerations"},{"location":"architecture/edge_service_specification/#compression","text":"It is not desirable to do decompression on the edge node. We want to pass along messages from the HTTP Edge node without \"cracking the egg\" of the payload. We may also receive badly formed payloads, and we will want to track the incidence of such things.","title":"Compression"},{"location":"architecture/edge_service_specification/#bad-messages","text":"Since the actual message is not examined by the edge server the only failures that occur are defined by the response status codes above. Messages are only forwarded to the pipeline when a response code of 200 is returned to the client.","title":"Bad Messages"},{"location":"architecture/edge_service_specification/#pubsub-topics","text":"All messages that sent a response code of 200 are forwarded to a single PubSub topic for decoding and landfill.","title":"PubSub Topics"},{"location":"architecture/edge_service_specification/#geoip-lookups","text":"No GeoIP lookup is performed by the edge server. If a client IP is available then the PubSub consumer performs the lookup and then discards the IP before the message is forwarded to a decoded PubSub topic.","title":"GeoIP Lookups"},{"location":"architecture/edge_service_specification/#data-retention","text":"The edge server only stores data when PubSub cannot be reached, and removes data after it is successfully written to PubSub. Down scaling will be disabled for the Kubernetes pod and cluster when data is being stored, so that data is not lost.","title":"Data Retention"},{"location":"architecture/edge_service_specification/#submission-timestamp-format","text":"submission_timestamp is formatted as ISO 8601 with microseconds and timezone, because it is compatible with BigQuery's Timestamp Type , so that the field doesn't need transformation.","title":"Submission Timestamp Format"},{"location":"architecture/landfill_service_specification/","text":"Raw Sink Service Specification This document specifies the behavior of the service that batches raw messages into long term storage. Data Flow Consume messages from a Google Cloud PubSub topic and write in batches to BigQuery. Split batches based by time windows based on when they were retrieved from PubSub. Additionally, split batches when they reach a certain size, if possible. Implementation Execute this as a custom Java application running on GKE. Latency Accept a configuration for batch window size. Deliver batches to BigQuery within 5 minutes of the batch window closing. Other Considerations Message Acks Only acknowledge messages in the PubSub topic subscription after delivery to BigQuery.","title":"Landfill Service Specification"},{"location":"architecture/landfill_service_specification/#raw-sink-service-specification","text":"This document specifies the behavior of the service that batches raw messages into long term storage.","title":"Raw Sink Service Specification"},{"location":"architecture/landfill_service_specification/#data-flow","text":"Consume messages from a Google Cloud PubSub topic and write in batches to BigQuery. Split batches based by time windows based on when they were retrieved from PubSub. Additionally, split batches when they reach a certain size, if possible.","title":"Data Flow"},{"location":"architecture/landfill_service_specification/#implementation","text":"Execute this as a custom Java application running on GKE.","title":"Implementation"},{"location":"architecture/landfill_service_specification/#latency","text":"Accept a configuration for batch window size. Deliver batches to BigQuery within 5 minutes of the batch window closing.","title":"Latency"},{"location":"architecture/landfill_service_specification/#other-considerations","text":"","title":"Other Considerations"},{"location":"architecture/landfill_service_specification/#message-acks","text":"Only acknowledge messages in the PubSub topic subscription after delivery to BigQuery.","title":"Message Acks"},{"location":"architecture/overview/","text":"GCP Ingestion Architecture This document specifies the architecture for GCP Ingestion as a whole. Architecture Diagram The Kubernetes Ingestion Edge sends messages from Producers (e.g. Firefox) to a set of PubSub Raw Topics , routing messages based on uri Raw Topics are the first layer of a \"pipeline family\"; the diagram shows only the \"structured\" pipeline family, but there are also deployments for \"telemetry\", \"stub-installer\", and \"pioneer\" The Raw Sink job copies messages from a PubSub Raw Topic to BigQuery The Dataflow Decoder job decodes messages from the PubSub Raw Topic to the PubSub Decoded Topic Also checks for existence of document_id s in Cloud Memorystore in order to deduplicate messages The Dataflow AET Decoder job provides all the functionality of the Decoder with additional decryption handling for Account Ecosystem Telemetry pings The Dataflow Republisher job reads messages from the PubSub Decoded Topic , marks them as seen in Cloud Memorystore and republishes them to various lower volume derived topics including Monitoring Sample Topics and Per DocType Topics The Kubernetes Decoded Sink job copies messages from the PubSub Decoded Topic to BigQuery with the payload encoded as JSON The Kubernetes Live Sink job copies messages from the PubSub Decoded Topic to BigQuery with the payload structure parsed out to individual fields Architecture Components Ingestion Edge Must send messages from producers to PubSub topics Must store messages on disk when PubSub is unavailable Must attempt to deliver all new messages to PubSub before storing on disk Must not be scaled down when there are messages on disk Must respond server error if PubSub and disk are both unavailable Must use a 5XX error error code Must accept configuration mapping uri to PubSub Topic Expected initial topics are Structured Ingestion, Telemetry, and Pioneer Must accept configuration defining HTTP headers to capture Raw Sink Must copy messages from PubSub topics to BigQuery This copy may be for backfill, recovery, or testing Must not ack messages read from PubSub until they are delivered Must accept configuration mapping PubSub topics to BigQuery tables Should retry transient Cloud Storage errors indefinitely Should use exponential back-off to determine retry timing Decoder Must decode messages from PubSub topics to PubSub topics Must not ack messages read from PubSub until they are delivered Must apply the following transforms in order ( implementations here ): Parse x_pipeline_proxy attribute; if present with a valid value in the edge submission timestamp format , archive the value of submission_timestamp to proxy_timestamp and replace with the x_pipeline_proxy value Resolve GeoIP from remote_addr or x_forwarded_for attribute into geo_* attributes Parse uri attribute into multiple attributes Gzip decompress payload if gzip compressed Validate payload using a JSON Schema determined by attributes Parse agent attribute into user_agent_* attributes Produce normalized_ variants of select attributes Inject normalized_ attributes at the top level and other select attributes into a nested metadata top level key in payload Should deduplicate messages based on the document_id attribute using Cloud MemoryStore Must ensure at least once delivery, so deduplication is only \"best effort\" Should delay deduplication to the latest possible stage of the pipeline to minimize the time window between an ID being marked as seen in Republisher and it being checked in Decoder Must send messages rejected by transforms to a configurable error destination Must allow error destination in BigQuery AET Decoder The AET (Account Ecosystem Telemetry) Decoder is a modified version of the Decoder with the following properties: The raw topic that feeds the AET Decoder must not be sent anywhere else; the AET Decoder needs to either successfully decrypt or sanitize all AET identifiers Must load private keys from an encrypted blob in GCS Must call Cloud KMS at startup to decrypt keys and store these only in memory Must remove or redact all AET ecosystem_anon_id values from the payload before passing to any durable output, including errors Must have access restricted to a limited set of operators to avoid exposing private keys Encrypted fields must be JOSE JWE objects in Compact Serialization form Republisher Must copy messages from PubSub topics to PubSub topics Must attempt to publish the document_id of each consumed message to Cloud MemoryStore ID publishing should be \"best effort\" but must not prevent the message proceeding to further steps in case of errors reaching Cloud MemoryStore Must ack messages read from PubSub after they are delivered to all matching destinations Must not ack messages read from PubSub before they are delivered to all matching destinations Must accept configuration enabling republishing of messages to a debug topic if they contain an x_debug_id attribute Must accept a compile-time parameter enabling or disabling debug republishing Must accept a runtime parameter defining the destination topic Must accept configuration enabling republishing of a random sample of the input stream Must accept a compile-time parameter setting the sample ratio Must accept a runtime parameter defining the destination topic Must accept configuration mapping document_type s to PubSub topics Must accept a compile-time parameter defining a topic pattern string (may be promoted to runtime if Dataflow adds support for PubSub topic names defined via NestedValueProvider ) Must accept a compile-time parameter defining which document_type s to republish Must only deliver messages with configured destinations Must accept configuration mapping document_namespace s to PubSub topics Must accept a compile-time parameter defining a map from document namespaces to topics Must only deliver messages with configured destinations Must accept optional configuration for sampling telemetry data Must accept a compile-time parameter defining a topic pattern string (may be promoted to runtime if Dataflow adds support for PubSub topic names defined via NestedValueProvider ) Must accept compile-time parameters defining the sampling ratio for each channel (nightly, beta, and release) Live Sink Must copy messages from PubSub topics to BigQuery Must not ack messages read from PubSub until they are delivered Must accept configuration mapping PubSub topics to BigQuery tables Must accept configuration for using streaming or batch loads Must transform all field names to lowercase with underscores ( snake_case ) and perform other field name cleaning to match the transformations expected by the jsonschema-transpiler Must set ignoreUnknownValues to true Should retry transient BigQuery errors indefinitely Should use exponential back-off to determine retry timing Must send messages rejected by BigQuery to a configurable error destination Must allow error destinations in BigQuery Decoded Sink Must copy messages from PubSub topics to BigQuery May be used to backfill BigQuery columns previously unspecified in the table schema May be used by BigQuery, Spark, and Dataflow to access columns missing from BigQuery Tables Must not ack messages read from PubSub until they are delivered Must accept configuration mapping PubSub topics to BigQuery tables Should retry transient BigQuery errors indefinitely Should use exponential back-off to determine retry timing Notes PubSub stores unacknowledged messages for 7 days. Any PubSub subscription more than 7 days behind requires a backfill. Dataflow will extend ack deadlines indefinitely when consuming messages, and will not ack messages until they are processed by an output or GroupByKey transform. Dataflow jobs achieve at least once delivery by not using GroupByKey transforms and not falling more than 7 days behind in processing. Design Decisions Kubernetes Engine and PubSub Kubernetes Engine is a scalable, managed service based on an industry standard. PubSub is a simple, scalable, managed service. By comparison a compute instance group instead of Kubernetes Engine and Kafka instead of PubSub would require more operational overhead and engineering effort for maintenance. Different topics for \"raw\" and \"validated\" data We don't want to have to repeat the validation logic in the case where we have multiple consumers of the data. Raw data can be sent to a single topic to simplify the edge service and then validated data can be sent to topics split by docType and other attributes, in order to allow consumers for specific sets of data. BigQuery BigQuery provides a simple, scalable, managed service for executing SQL queries over arbitrarily large or small amounts of data, with built-in schema validation, hyperloglog functions, UDF support, and destination tables (sometimes called materialized views) for minimizing cost and latency of derived tables. Alternatives (such as Presto) would have more operational overhead and engineering effort for maintenance, while generally being less featureful. Archive messages from each stage of the pipeline as JSON payloads in BigQuery One of the primary challenges of building a real-world data pipeline is anticipating and adapting to changes in the schemas of messages flowing through the system. Strong schemas and structured data give us many usability and performance benefits, but changes to the schema at one point in the pipeline can lead to processing errors or dropped data further down the pipeline. Saving JSON messages as compressed bytes fields in BigQuery tables allows use to gracefully handle new fields added upstream without needing to specify those fields completely before they are stored. New columns can be added to a table's schema and then restored via a backfill operation. Use destination tables For complex queries that are calculated over time-based windows of data, using destination tables allows us to save time and cost by only querying each new window of data once. Use views for user-facing data Views we create in BigQuery can be a stable interface for users while we potentially change versions or implementations of a pipeline behind the scenes. If we wanted to rewrite a materialized view, for example, we might run the new and old definitions in parallel, writing to separate tables; when we\u2019re comfortable that the new implementation is stable, we could cut users over to the new implementation by simply changing the definition of the user-facing view. Limits The maximum Content-Length accepted at the edge is 1 MB; larger payloads will be dropped and the request will return a 413 response code The maximum payload size after being decompressed in the Decoder is 8 MB; larger payloads will trigger a PayloadTooLarge exception and be sent to error output Hard limit of 10,000 columns per table in BigQuery (see Load job limits ) Max of 1,000,000 streaming inserts per second per BigQuery table, lower if we populate insertId (see Streaming insert limits ) A PubSub topic without any subscriptions drops all messages until a subscription is created API Rate Limit: 20 req/sec Further Reading Differences from AWS","title":"Overview"},{"location":"architecture/overview/#gcp-ingestion-architecture","text":"This document specifies the architecture for GCP Ingestion as a whole.","title":"GCP Ingestion Architecture"},{"location":"architecture/overview/#architecture-diagram","text":"The Kubernetes Ingestion Edge sends messages from Producers (e.g. Firefox) to a set of PubSub Raw Topics , routing messages based on uri Raw Topics are the first layer of a \"pipeline family\"; the diagram shows only the \"structured\" pipeline family, but there are also deployments for \"telemetry\", \"stub-installer\", and \"pioneer\" The Raw Sink job copies messages from a PubSub Raw Topic to BigQuery The Dataflow Decoder job decodes messages from the PubSub Raw Topic to the PubSub Decoded Topic Also checks for existence of document_id s in Cloud Memorystore in order to deduplicate messages The Dataflow AET Decoder job provides all the functionality of the Decoder with additional decryption handling for Account Ecosystem Telemetry pings The Dataflow Republisher job reads messages from the PubSub Decoded Topic , marks them as seen in Cloud Memorystore and republishes them to various lower volume derived topics including Monitoring Sample Topics and Per DocType Topics The Kubernetes Decoded Sink job copies messages from the PubSub Decoded Topic to BigQuery with the payload encoded as JSON The Kubernetes Live Sink job copies messages from the PubSub Decoded Topic to BigQuery with the payload structure parsed out to individual fields","title":"Architecture Diagram"},{"location":"architecture/overview/#architecture-components","text":"","title":"Architecture Components"},{"location":"architecture/overview/#ingestion-edge","text":"Must send messages from producers to PubSub topics Must store messages on disk when PubSub is unavailable Must attempt to deliver all new messages to PubSub before storing on disk Must not be scaled down when there are messages on disk Must respond server error if PubSub and disk are both unavailable Must use a 5XX error error code Must accept configuration mapping uri to PubSub Topic Expected initial topics are Structured Ingestion, Telemetry, and Pioneer Must accept configuration defining HTTP headers to capture","title":"Ingestion Edge"},{"location":"architecture/overview/#raw-sink","text":"Must copy messages from PubSub topics to BigQuery This copy may be for backfill, recovery, or testing Must not ack messages read from PubSub until they are delivered Must accept configuration mapping PubSub topics to BigQuery tables Should retry transient Cloud Storage errors indefinitely Should use exponential back-off to determine retry timing","title":"Raw Sink"},{"location":"architecture/overview/#decoder","text":"Must decode messages from PubSub topics to PubSub topics Must not ack messages read from PubSub until they are delivered Must apply the following transforms in order ( implementations here ): Parse x_pipeline_proxy attribute; if present with a valid value in the edge submission timestamp format , archive the value of submission_timestamp to proxy_timestamp and replace with the x_pipeline_proxy value Resolve GeoIP from remote_addr or x_forwarded_for attribute into geo_* attributes Parse uri attribute into multiple attributes Gzip decompress payload if gzip compressed Validate payload using a JSON Schema determined by attributes Parse agent attribute into user_agent_* attributes Produce normalized_ variants of select attributes Inject normalized_ attributes at the top level and other select attributes into a nested metadata top level key in payload Should deduplicate messages based on the document_id attribute using Cloud MemoryStore Must ensure at least once delivery, so deduplication is only \"best effort\" Should delay deduplication to the latest possible stage of the pipeline to minimize the time window between an ID being marked as seen in Republisher and it being checked in Decoder Must send messages rejected by transforms to a configurable error destination Must allow error destination in BigQuery","title":"Decoder"},{"location":"architecture/overview/#aet-decoder","text":"The AET (Account Ecosystem Telemetry) Decoder is a modified version of the Decoder with the following properties: The raw topic that feeds the AET Decoder must not be sent anywhere else; the AET Decoder needs to either successfully decrypt or sanitize all AET identifiers Must load private keys from an encrypted blob in GCS Must call Cloud KMS at startup to decrypt keys and store these only in memory Must remove or redact all AET ecosystem_anon_id values from the payload before passing to any durable output, including errors Must have access restricted to a limited set of operators to avoid exposing private keys Encrypted fields must be JOSE JWE objects in Compact Serialization form","title":"AET Decoder"},{"location":"architecture/overview/#republisher","text":"Must copy messages from PubSub topics to PubSub topics Must attempt to publish the document_id of each consumed message to Cloud MemoryStore ID publishing should be \"best effort\" but must not prevent the message proceeding to further steps in case of errors reaching Cloud MemoryStore Must ack messages read from PubSub after they are delivered to all matching destinations Must not ack messages read from PubSub before they are delivered to all matching destinations Must accept configuration enabling republishing of messages to a debug topic if they contain an x_debug_id attribute Must accept a compile-time parameter enabling or disabling debug republishing Must accept a runtime parameter defining the destination topic Must accept configuration enabling republishing of a random sample of the input stream Must accept a compile-time parameter setting the sample ratio Must accept a runtime parameter defining the destination topic Must accept configuration mapping document_type s to PubSub topics Must accept a compile-time parameter defining a topic pattern string (may be promoted to runtime if Dataflow adds support for PubSub topic names defined via NestedValueProvider ) Must accept a compile-time parameter defining which document_type s to republish Must only deliver messages with configured destinations Must accept configuration mapping document_namespace s to PubSub topics Must accept a compile-time parameter defining a map from document namespaces to topics Must only deliver messages with configured destinations Must accept optional configuration for sampling telemetry data Must accept a compile-time parameter defining a topic pattern string (may be promoted to runtime if Dataflow adds support for PubSub topic names defined via NestedValueProvider ) Must accept compile-time parameters defining the sampling ratio for each channel (nightly, beta, and release)","title":"Republisher"},{"location":"architecture/overview/#live-sink","text":"Must copy messages from PubSub topics to BigQuery Must not ack messages read from PubSub until they are delivered Must accept configuration mapping PubSub topics to BigQuery tables Must accept configuration for using streaming or batch loads Must transform all field names to lowercase with underscores ( snake_case ) and perform other field name cleaning to match the transformations expected by the jsonschema-transpiler Must set ignoreUnknownValues to true Should retry transient BigQuery errors indefinitely Should use exponential back-off to determine retry timing Must send messages rejected by BigQuery to a configurable error destination Must allow error destinations in BigQuery","title":"Live Sink"},{"location":"architecture/overview/#decoded-sink","text":"Must copy messages from PubSub topics to BigQuery May be used to backfill BigQuery columns previously unspecified in the table schema May be used by BigQuery, Spark, and Dataflow to access columns missing from BigQuery Tables Must not ack messages read from PubSub until they are delivered Must accept configuration mapping PubSub topics to BigQuery tables Should retry transient BigQuery errors indefinitely Should use exponential back-off to determine retry timing","title":"Decoded Sink"},{"location":"architecture/overview/#notes","text":"PubSub stores unacknowledged messages for 7 days. Any PubSub subscription more than 7 days behind requires a backfill. Dataflow will extend ack deadlines indefinitely when consuming messages, and will not ack messages until they are processed by an output or GroupByKey transform. Dataflow jobs achieve at least once delivery by not using GroupByKey transforms and not falling more than 7 days behind in processing.","title":"Notes"},{"location":"architecture/overview/#design-decisions","text":"","title":"Design Decisions"},{"location":"architecture/overview/#kubernetes-engine-and-pubsub","text":"Kubernetes Engine is a scalable, managed service based on an industry standard. PubSub is a simple, scalable, managed service. By comparison a compute instance group instead of Kubernetes Engine and Kafka instead of PubSub would require more operational overhead and engineering effort for maintenance.","title":"Kubernetes Engine and PubSub"},{"location":"architecture/overview/#different-topics-for-raw-and-validated-data","text":"We don't want to have to repeat the validation logic in the case where we have multiple consumers of the data. Raw data can be sent to a single topic to simplify the edge service and then validated data can be sent to topics split by docType and other attributes, in order to allow consumers for specific sets of data.","title":"Different topics for \"raw\" and \"validated\" data"},{"location":"architecture/overview/#bigquery","text":"BigQuery provides a simple, scalable, managed service for executing SQL queries over arbitrarily large or small amounts of data, with built-in schema validation, hyperloglog functions, UDF support, and destination tables (sometimes called materialized views) for minimizing cost and latency of derived tables. Alternatives (such as Presto) would have more operational overhead and engineering effort for maintenance, while generally being less featureful.","title":"BigQuery"},{"location":"architecture/overview/#archive-messages-from-each-stage-of-the-pipeline-as-json-payloads-in-bigquery","text":"One of the primary challenges of building a real-world data pipeline is anticipating and adapting to changes in the schemas of messages flowing through the system. Strong schemas and structured data give us many usability and performance benefits, but changes to the schema at one point in the pipeline can lead to processing errors or dropped data further down the pipeline. Saving JSON messages as compressed bytes fields in BigQuery tables allows use to gracefully handle new fields added upstream without needing to specify those fields completely before they are stored. New columns can be added to a table's schema and then restored via a backfill operation.","title":"Archive messages from each stage of the pipeline as JSON payloads in BigQuery"},{"location":"architecture/overview/#use-destination-tables","text":"For complex queries that are calculated over time-based windows of data, using destination tables allows us to save time and cost by only querying each new window of data once.","title":"Use destination tables"},{"location":"architecture/overview/#use-views-for-user-facing-data","text":"Views we create in BigQuery can be a stable interface for users while we potentially change versions or implementations of a pipeline behind the scenes. If we wanted to rewrite a materialized view, for example, we might run the new and old definitions in parallel, writing to separate tables; when we\u2019re comfortable that the new implementation is stable, we could cut users over to the new implementation by simply changing the definition of the user-facing view.","title":"Use views for user-facing data"},{"location":"architecture/overview/#limits","text":"The maximum Content-Length accepted at the edge is 1 MB; larger payloads will be dropped and the request will return a 413 response code The maximum payload size after being decompressed in the Decoder is 8 MB; larger payloads will trigger a PayloadTooLarge exception and be sent to error output Hard limit of 10,000 columns per table in BigQuery (see Load job limits ) Max of 1,000,000 streaming inserts per second per BigQuery table, lower if we populate insertId (see Streaming insert limits ) A PubSub topic without any subscriptions drops all messages until a subscription is created API Rate Limit: 20 req/sec","title":"Limits"},{"location":"architecture/overview/#further-reading","text":"Differences from AWS","title":"Further Reading"},{"location":"architecture/pain_points/","text":"Pain points A running list of things that are suboptimal in GCP. App Engine For network-bound applications it can be prohibitively expensive. A PubSub push subscription application that decodes protobuf and forwards messages to the ingestion-edge used ~300 instances at $0.06 per instance hour to handle ~5krps , which is ~$13K/mo . Dataflow Replaces certain components with custom behavior that is not part of the open source Beam API, making it so they can't be extended (e.g. to expose a stream of messages that have been delivered to PubSub). BigQueryIO.Write Requires decoding PubsubMessage.payload from JSON to a TableRow , which gets encoded as JSON to be sent to BigQuery. Crashes the pipeline when the destination table does not exist. FileIO.Write Acknowledges messages in PubSub before they are written to accumulate data across multiple bundles and produce reasonably sized files. Possible workaround being investigated in #380 . This also effects BigQueryIO.Write in batch mode. PubsubIO.Write Does not support dynamic destinations. Does not support [ NestedValueProvider ] for destinations in streaming mode on Dataflow, which is needed to create templates that accept a mapping of document type to a predetermined number of destinations. This is because Dataflow moves the implementation into the shuffler to improve performance. Current workaround is to specify mapping at template creation time. Does not use standard client library. Does not expose an output of delivered messages, which is needed for at least once delivery with deduplication. Current workaround is to get delivered messages from a subscription to the output PubSub topic. Uses HTTPS JSON API, which increases message payload size vs protobuf by 25% for base64 encoding and causes some messages to exceed the 10MB request size limit that otherwise would not. Templates Does not support repeated parameters via ValueProvider<List<...>> , as described in [Dataflow Java SDK #632] PubSub Can be prohibitively expensive. It costs ~$51K/mo to use PubSub with a 70MiB/s stream published or consumed 7 times (Edge to raw topic, raw topic to Cloud Storage, raw topic to Decoder, Decoder to decoded topic, decoded topic to Decoder for deduplication, decoded topic to Cloud Storage, decoded topic to BigQuery). Push Subscriptions are limited to min(10MB, 1000 messages) in flight, making the theoretical maximum parallel latency per message ~ 62ms to achieve 16krps .","title":"Pain Points"},{"location":"architecture/pain_points/#pain-points","text":"A running list of things that are suboptimal in GCP.","title":"Pain points"},{"location":"architecture/pain_points/#app-engine","text":"For network-bound applications it can be prohibitively expensive. A PubSub push subscription application that decodes protobuf and forwards messages to the ingestion-edge used ~300 instances at $0.06 per instance hour to handle ~5krps , which is ~$13K/mo .","title":"App Engine"},{"location":"architecture/pain_points/#dataflow","text":"Replaces certain components with custom behavior that is not part of the open source Beam API, making it so they can't be extended (e.g. to expose a stream of messages that have been delivered to PubSub).","title":"Dataflow"},{"location":"architecture/pain_points/#bigqueryiowrite","text":"Requires decoding PubsubMessage.payload from JSON to a TableRow , which gets encoded as JSON to be sent to BigQuery. Crashes the pipeline when the destination table does not exist.","title":"BigQueryIO.Write"},{"location":"architecture/pain_points/#fileiowrite","text":"Acknowledges messages in PubSub before they are written to accumulate data across multiple bundles and produce reasonably sized files. Possible workaround being investigated in #380 . This also effects BigQueryIO.Write in batch mode.","title":"FileIO.Write"},{"location":"architecture/pain_points/#pubsubiowrite","text":"Does not support dynamic destinations. Does not support [ NestedValueProvider ] for destinations in streaming mode on Dataflow, which is needed to create templates that accept a mapping of document type to a predetermined number of destinations. This is because Dataflow moves the implementation into the shuffler to improve performance. Current workaround is to specify mapping at template creation time. Does not use standard client library. Does not expose an output of delivered messages, which is needed for at least once delivery with deduplication. Current workaround is to get delivered messages from a subscription to the output PubSub topic. Uses HTTPS JSON API, which increases message payload size vs protobuf by 25% for base64 encoding and causes some messages to exceed the 10MB request size limit that otherwise would not.","title":"PubsubIO.Write"},{"location":"architecture/pain_points/#templates","text":"Does not support repeated parameters via ValueProvider<List<...>> , as described in [Dataflow Java SDK #632]","title":"Templates"},{"location":"architecture/pain_points/#pubsub","text":"Can be prohibitively expensive. It costs ~$51K/mo to use PubSub with a 70MiB/s stream published or consumed 7 times (Edge to raw topic, raw topic to Cloud Storage, raw topic to Decoder, Decoder to decoded topic, decoded topic to Decoder for deduplication, decoded topic to Cloud Storage, decoded topic to BigQuery). Push Subscriptions are limited to min(10MB, 1000 messages) in flight, making the theoretical maximum parallel latency per message ~ 62ms to achieve 16krps .","title":"PubSub"},{"location":"architecture/reliability/","text":"Reliability In production the ingestion product aims to provide a Biweekly Uptime Percentage determined by the Reliability Target below. If a component does not meet that then a Stability Work Period should be assigned to each software engineer supporting the component. Disclaimer and Purpose This document is intended solely for those directly running, writing, and managing GCP Ingestion. It is not an agreement, implicit or otherwise, with any other parties. This document is a prototype that should be treated as a goal that will require adjustments. The purpose of this document is first and foremost to encourage behavior that reduces Downtime. The secondary purpose is to establish clear expectations for how software engineers respond to Downtime. Reliability Target Component Biweekly Uptime Percentage ingestion-edge 99.99% ingestion-beam 99.5% Definitions \"Downtime\" on ingestion-edge means for at least 0.1% of requests a status code lower than 500 was not successfully returned. On ingestion-beam it means oldest_unacked_message_age on a PubSub input exceeds 1 hour for a batch sink job or 1 minute for a decoder or streaming sink job. \"Downtime Period\" means a period of 60 consecutive seconds of Downtime. Intermittent Downtime for a period of less than 60 consecutive seconds will not be counted towards any Downtime Periods. \"Biweekly Uptime Percentage\" means total number of minutes in a rolling two week window, minus the number of minutes of Downtime suffered from all Downtime Periods in the window, divided by the total number of minutes in the window. \"Stability Work\" means work on Downtime prevention and reduction. Only code changes resulting from that work may be deployed to production. All other features and work on the component are suspended. \"Stability Work Period\" means a continuous period of time after recovery and postmortem, where each engineer is assigned Stability Work. The length of the Stability Work Period is determined below. Component Biweekly Uptime Percentage Length of Stability Work Period ingestion-edge 99.9% to < 99.99% 2 weeks ingestion-edge 99% to < 99.9% 4 weeks ingestion-edge < 99% 12 weeks ingestion-beam 99% to < 99.5% 2 weeks ingestion-beam 95% to < 99% 4 weeks ingestion-beam < 95% 12 weeks Exclusions The Reliability Target does not apply to any Downtime in ingestion caused by Downtime on a Google Cloud Platform Service, other than Downtime on ingestion-edge caused by PubSub. Additional Information Biweekly Uptime Percentage Downtime per Two Weeks 99.99% 2 minutes 99.9% 20 minutes 99.5% 1 hour 40 minutes 99% 3 hours 21 minutes 95% 16 hours 48 minutes","title":"Reliability"},{"location":"architecture/reliability/#reliability","text":"In production the ingestion product aims to provide a Biweekly Uptime Percentage determined by the Reliability Target below. If a component does not meet that then a Stability Work Period should be assigned to each software engineer supporting the component.","title":"Reliability"},{"location":"architecture/reliability/#disclaimer-and-purpose","text":"This document is intended solely for those directly running, writing, and managing GCP Ingestion. It is not an agreement, implicit or otherwise, with any other parties. This document is a prototype that should be treated as a goal that will require adjustments. The purpose of this document is first and foremost to encourage behavior that reduces Downtime. The secondary purpose is to establish clear expectations for how software engineers respond to Downtime.","title":"Disclaimer and Purpose"},{"location":"architecture/reliability/#reliability-target","text":"Component Biweekly Uptime Percentage ingestion-edge 99.99% ingestion-beam 99.5%","title":"Reliability Target"},{"location":"architecture/reliability/#definitions","text":"\"Downtime\" on ingestion-edge means for at least 0.1% of requests a status code lower than 500 was not successfully returned. On ingestion-beam it means oldest_unacked_message_age on a PubSub input exceeds 1 hour for a batch sink job or 1 minute for a decoder or streaming sink job. \"Downtime Period\" means a period of 60 consecutive seconds of Downtime. Intermittent Downtime for a period of less than 60 consecutive seconds will not be counted towards any Downtime Periods. \"Biweekly Uptime Percentage\" means total number of minutes in a rolling two week window, minus the number of minutes of Downtime suffered from all Downtime Periods in the window, divided by the total number of minutes in the window. \"Stability Work\" means work on Downtime prevention and reduction. Only code changes resulting from that work may be deployed to production. All other features and work on the component are suspended. \"Stability Work Period\" means a continuous period of time after recovery and postmortem, where each engineer is assigned Stability Work. The length of the Stability Work Period is determined below. Component Biweekly Uptime Percentage Length of Stability Work Period ingestion-edge 99.9% to < 99.99% 2 weeks ingestion-edge 99% to < 99.9% 4 weeks ingestion-edge < 99% 12 weeks ingestion-beam 99% to < 99.5% 2 weeks ingestion-beam 95% to < 99% 4 weeks ingestion-beam < 95% 12 weeks","title":"Definitions"},{"location":"architecture/reliability/#exclusions","text":"The Reliability Target does not apply to any Downtime in ingestion caused by Downtime on a Google Cloud Platform Service, other than Downtime on ingestion-edge caused by PubSub.","title":"Exclusions"},{"location":"architecture/reliability/#additional-information","text":"Biweekly Uptime Percentage Downtime per Two Weeks 99.99% 2 minutes 99.9% 20 minutes 99.5% 1 hour 40 minutes 99% 3 hours 21 minutes 95% 16 hours 48 minutes","title":"Additional Information"},{"location":"architecture/test_requirements/","text":"Test Requirements This document specifies the testing required for GCP Ingestion components. Exceptions Code that does not comply with this standard before it is deployed to production must include a document explaining why that decision was made. Test Phases Continuous Integration Must run Unit Tests and Integration Tests Must run against every merge to master and every pull request Must block merging to master Pre-Production Must run Unit Tests and Load Tests Must run against every version deployed to production Must mimic production configuration as closely as possible Should block deployment to production Async Must run Slow Load Tests Must run against every version deployed to production Must mimic production configuration as closely as possible Should not block deployment to production Must notify someone to investigate failures Test Categories Tests must be both thorough and fast enough to not block development. They are split into categories with increasing run time and decreasing coverage. Unit Tests Must cover 100% of code behavior Should run as fast as possible Integration Tests Must cover all expected production behavior Must run fast enough to allow frequent merging to master Load Tests Must cover performance at scale with and without external services down Must run fast enough to allow multiple production deployments per day Slow Load Tests Must cover performance at scale with extended downtime May take a very long time Unit Tests Must run in under 5 seconds in CI, not including build time May use CI parallelism and run each group of tests in under 5 seconds in CI Should validate code as quickly as possible Must run completely in memory Must safely run in parallel May mock anything that would not run in memory Should cover 100% of branches and statements Should cover all variations of statements having variable behavior without branching Should cover all production configuration variations Must have Integration Tests to cover exceptions Must cover all input and output variations Must cover all response codes Must cover all valid URL patterns Must cover gzipped and not gzipped Must cover present, absent, and invalid required attributes Must cover present, absent, and invalid optional attributes Must cover present, absent, invalid, and wrong type payload JSON Must cover present, absent, invalid, and wrong type payload fields Must cover document_id duplicates Must cover all error types Must cover PubSub returning OK but rejecting messages in a batch Must cover External service returns server error Must cover External service timeout Must cover External resource missing Must cover Insufficient permissions Must cover behavior when disk becomes full Must cover behavior with disk already full Must cover behavior when ingestion-edge disk is no longer full Integration Tests Must run in under 5 minutes in CI, not including build time Must be configurable to run locally and in CI Must be configurable to run against a deployment in GCP May require a proxy to modify cloud service behavior May require specific configuration May skip coverage that cannot be forcibly produced in Dataflow Must clean up created resources when complete Must cover everything unit tests should but do not Must cover a configuration that mimics production as closely as possible Must cover all code with input from or output to external services The ingestion-edge disk queue is an external service Must cover all production configuration options Must cover all API endpoints with invalid traffic Must cover all API endpoints with healthy external services Must assert disk queue is not used in ingestion-edge Must assert inputs and outputs preserve schema in ingestion-beam Must cover behavior with errors from external services Must cover PubSub returning OK but rejecting messages in a batch Must cover external service DNS does not resolve Must cover external service cannot TCP connect Must cover external service hangs after TCP connect Must cover external service returns 500 Must cover resource does not exist in external service Must cover insufficient permissions in external service Must cover behavior when disk becomes full Must cover behavior with disk already full Must assert automatic recovery unless manual intervention is required Must cover behavior when ingestion-edge disk is no longer full Load Tests Must run in parallel in under 1 hour May require a separate deployment for each test Must record how much it cost to run Must not require manual intervention to evaluate Must check performance against a tunable threshold Must sustain at least 1.5x expected peak production traffic volume As of 2018-08-01 peak production traffic volume is about 11K req/s Must cover behavior before, during, and after a period of: Service downtime, except ingestion-edge 100%, 50%, and 5% invalid traffic External services cannot TCP connect 100% and 10% of external service requests time out 100% and 10% external service requests return 500 Must cover ingestion-edge behavior with PubSub returning 500 and auto scaling due to disk filling up Must assert ingestion-edge only writes to disk queue when PubSub is unhealthy Must assert full recovery from downtime does not take longer than the downtime period Slow Load Tests May run for longer than 1 hour Should not block deployment to production Must cover behavior with sustained load and a backlog equivalent to: For ingestion-edge: 1.5x the longest PubSub outage in the last year As of 2018-08-01 the longest PubSub outage was about 4.5 hours For everything else: 4 days (1 holiday weekend) Should reach full recovery in under 1 day Must cover behavior of ingestion-beam BigQuery output with sustained load and BigQuery returning 500 for at least 1.5x the longest BigQuery outage in the last year As of 2018-08-01 the longest BigQuery outage was about 2.3 hours","title":"Test requirements"},{"location":"architecture/test_requirements/#test-requirements","text":"This document specifies the testing required for GCP Ingestion components.","title":"Test Requirements"},{"location":"architecture/test_requirements/#exceptions","text":"Code that does not comply with this standard before it is deployed to production must include a document explaining why that decision was made.","title":"Exceptions"},{"location":"architecture/test_requirements/#test-phases","text":"Continuous Integration Must run Unit Tests and Integration Tests Must run against every merge to master and every pull request Must block merging to master Pre-Production Must run Unit Tests and Load Tests Must run against every version deployed to production Must mimic production configuration as closely as possible Should block deployment to production Async Must run Slow Load Tests Must run against every version deployed to production Must mimic production configuration as closely as possible Should not block deployment to production Must notify someone to investigate failures","title":"Test Phases"},{"location":"architecture/test_requirements/#test-categories","text":"Tests must be both thorough and fast enough to not block development. They are split into categories with increasing run time and decreasing coverage. Unit Tests Must cover 100% of code behavior Should run as fast as possible Integration Tests Must cover all expected production behavior Must run fast enough to allow frequent merging to master Load Tests Must cover performance at scale with and without external services down Must run fast enough to allow multiple production deployments per day Slow Load Tests Must cover performance at scale with extended downtime May take a very long time","title":"Test Categories"},{"location":"architecture/test_requirements/#unit-tests","text":"Must run in under 5 seconds in CI, not including build time May use CI parallelism and run each group of tests in under 5 seconds in CI Should validate code as quickly as possible Must run completely in memory Must safely run in parallel May mock anything that would not run in memory Should cover 100% of branches and statements Should cover all variations of statements having variable behavior without branching Should cover all production configuration variations Must have Integration Tests to cover exceptions Must cover all input and output variations Must cover all response codes Must cover all valid URL patterns Must cover gzipped and not gzipped Must cover present, absent, and invalid required attributes Must cover present, absent, and invalid optional attributes Must cover present, absent, invalid, and wrong type payload JSON Must cover present, absent, invalid, and wrong type payload fields Must cover document_id duplicates Must cover all error types Must cover PubSub returning OK but rejecting messages in a batch Must cover External service returns server error Must cover External service timeout Must cover External resource missing Must cover Insufficient permissions Must cover behavior when disk becomes full Must cover behavior with disk already full Must cover behavior when ingestion-edge disk is no longer full","title":"Unit Tests"},{"location":"architecture/test_requirements/#integration-tests","text":"Must run in under 5 minutes in CI, not including build time Must be configurable to run locally and in CI Must be configurable to run against a deployment in GCP May require a proxy to modify cloud service behavior May require specific configuration May skip coverage that cannot be forcibly produced in Dataflow Must clean up created resources when complete Must cover everything unit tests should but do not Must cover a configuration that mimics production as closely as possible Must cover all code with input from or output to external services The ingestion-edge disk queue is an external service Must cover all production configuration options Must cover all API endpoints with invalid traffic Must cover all API endpoints with healthy external services Must assert disk queue is not used in ingestion-edge Must assert inputs and outputs preserve schema in ingestion-beam Must cover behavior with errors from external services Must cover PubSub returning OK but rejecting messages in a batch Must cover external service DNS does not resolve Must cover external service cannot TCP connect Must cover external service hangs after TCP connect Must cover external service returns 500 Must cover resource does not exist in external service Must cover insufficient permissions in external service Must cover behavior when disk becomes full Must cover behavior with disk already full Must assert automatic recovery unless manual intervention is required Must cover behavior when ingestion-edge disk is no longer full","title":"Integration Tests"},{"location":"architecture/test_requirements/#load-tests","text":"Must run in parallel in under 1 hour May require a separate deployment for each test Must record how much it cost to run Must not require manual intervention to evaluate Must check performance against a tunable threshold Must sustain at least 1.5x expected peak production traffic volume As of 2018-08-01 peak production traffic volume is about 11K req/s Must cover behavior before, during, and after a period of: Service downtime, except ingestion-edge 100%, 50%, and 5% invalid traffic External services cannot TCP connect 100% and 10% of external service requests time out 100% and 10% external service requests return 500 Must cover ingestion-edge behavior with PubSub returning 500 and auto scaling due to disk filling up Must assert ingestion-edge only writes to disk queue when PubSub is unhealthy Must assert full recovery from downtime does not take longer than the downtime period","title":"Load Tests"},{"location":"architecture/test_requirements/#slow-load-tests","text":"May run for longer than 1 hour Should not block deployment to production Must cover behavior with sustained load and a backlog equivalent to: For ingestion-edge: 1.5x the longest PubSub outage in the last year As of 2018-08-01 the longest PubSub outage was about 4.5 hours For everything else: 4 days (1 holiday weekend) Should reach full recovery in under 1 day Must cover behavior of ingestion-beam BigQuery output with sustained load and BigQuery returning 500 for at least 1.5x the longest BigQuery outage in the last year As of 2018-08-01 the longest BigQuery outage was about 2.3 hours","title":"Slow Load Tests"},{"location":"ingestion-beam/","text":"Apache Beam Jobs for Ingestion This ingestion-beam java module contains our Apache Beam jobs for use in Ingestion. Google Cloud Dataflow is a Google Cloud Platform service that natively runs Apache Beam jobs. The source code lives in the ingestion-beam subdirectory of the gcp-ingestion repository. There are currently three jobs defined, please see the respective sections on them in the documentation: Sink job : A job for delivering messages between Google Cloud services Decoder job : A job for normalizing ingestion messages Republisher job : A job for republishing subsets of decoded messages to new destinations Building Move to the ingestion-beam subdirectory of your gcp-ingestion checkout and run: ./bin/mvn clean compile See the details below under each job for details on how to run what you've produced. Testing Before anything else, be sure to download the test data: ./bin/download-cities15000 ./bin/download-geolite2 ./bin/download-schemas Run tests locally with CircleCI Local CLI (cd .. && circleci build --job ingestion-beam) To make more targeted test invocations, you can install Java and maven locally or use the bin/mvn executable to run maven in docker: ./bin/mvn clean test If you wish to just run a single test class or a single test case, try something like this: # Run all tests in a single class ./bin/mvn test -Dtest=com.mozilla.telemetry.util.SnakeCaseTest # Run only a single test case ./bin/mvn test -Dtest='com.mozilla.telemetry.util.SnakeCaseTest#testSnakeCaseFormat' To run the project in a sandbox against production data, see this document on configuring an integration testing workflow . Code Formatting Use spotless to automatically reformat code: mvn spotless:apply or just check what changes it requires: mvn spotless:check","title":"Overview"},{"location":"ingestion-beam/#apache-beam-jobs-for-ingestion","text":"This ingestion-beam java module contains our Apache Beam jobs for use in Ingestion. Google Cloud Dataflow is a Google Cloud Platform service that natively runs Apache Beam jobs. The source code lives in the ingestion-beam subdirectory of the gcp-ingestion repository. There are currently three jobs defined, please see the respective sections on them in the documentation: Sink job : A job for delivering messages between Google Cloud services Decoder job : A job for normalizing ingestion messages Republisher job : A job for republishing subsets of decoded messages to new destinations","title":"Apache Beam Jobs for Ingestion"},{"location":"ingestion-beam/#building","text":"Move to the ingestion-beam subdirectory of your gcp-ingestion checkout and run: ./bin/mvn clean compile See the details below under each job for details on how to run what you've produced.","title":"Building"},{"location":"ingestion-beam/#testing","text":"Before anything else, be sure to download the test data: ./bin/download-cities15000 ./bin/download-geolite2 ./bin/download-schemas Run tests locally with CircleCI Local CLI (cd .. && circleci build --job ingestion-beam) To make more targeted test invocations, you can install Java and maven locally or use the bin/mvn executable to run maven in docker: ./bin/mvn clean test If you wish to just run a single test class or a single test case, try something like this: # Run all tests in a single class ./bin/mvn test -Dtest=com.mozilla.telemetry.util.SnakeCaseTest # Run only a single test case ./bin/mvn test -Dtest='com.mozilla.telemetry.util.SnakeCaseTest#testSnakeCaseFormat' To run the project in a sandbox against production data, see this document on configuring an integration testing workflow .","title":"Testing"},{"location":"ingestion-beam/#code-formatting","text":"Use spotless to automatically reformat code: mvn spotless:apply or just check what changes it requires: mvn spotless:check","title":"Code Formatting"},{"location":"ingestion-beam/decoder-job/","text":"Decoder Job A job for normalizing ingestion messages. Defined in the com.mozilla.telemetry.Decoder class ( source ). Transforms These transforms are currently executed against each message in order. GeoIP Lookup Extract ip from the x_forwarded_for attribute when the x_pipeline_proxy attribute is not present, use the second-to-last value (since the last value is a forwarding rule IP added by Google load balancer) when the x_pipeline_proxy attribute is present, use the third-to-last value (since the tee introduces an additional proxy IP) fall back to the remote_addr attribute, then to an empty string Execute the following steps until one fails and ignore the exception Parse ip using InetAddress.getByName Lookup ip in the configured GeoIP2City.mmdb Extract country.iso_code as geo_country Extract city.name as geo_city if cities15000.txt is not configured or city.geo_name_id is in the configured cities15000.txt Extract subdivisions[0].iso_code as geo_subdivision1 Extract subdivisions[1].iso_code as geo_subdivision2 Remove the x_forwarded_for and remote_addr attributes Remove any null values added to attributes Parse URI Attempt to extract attributes from uri , on failure send messages to the configured error output. Decompress Attempt to decompress payload with gzip, on failure pass the message through unmodified. Parse Payload Parse the message body as a UTF-8 encoded JSON payload Drop specific fields or entire messages that match a specific set of signatures for toxic data that we want to make sure we do not store Maintain counter metrics for each type of dropped message Validate the payload structure based on the JSON schema for the specified document type Invalid messages are routed to error output Extract some additional attributes such as client_id and os_name based on the payload contents Parse User Agent Attempt to extract browser, browser version, and os from the user_agent attribute, drop any nulls, and remove user_agent from attributes. Write Metadata Into the Payload Add a nested metadata field and several normalized_* attributes into the payload body. Executing Decoder jobs are executed the same way as sink jobs but with a few extra flags: -Dexec.mainClass=com.mozilla.telemetry.Decoder --geoCityDatabase=/path/to/GeoIP2-City.mmdb --geoCityFilter=/path/to/cities15000.txt (optional) To download the GeoLite2 database , you need to register for a MaxMind account to obtain a license key. After generating a new license key, set MM_LICENSE_KEY to your license key. Example: # create a test input file mkdir -p tmp/ echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"remote_addr\":\"63.245.208.195\"}}' > tmp/input.json # Download `cities15000.txt`, `GeoLite2-City.mmdb`, and `schemas.tar.gz` ./bin/download-cities15000 ./bin/download-schemas export MM_LICENSE_KEY=\"Your MaxMind License Key\" ./bin/download-geolite2 # do geo lookup on messages to stdout ./bin/mvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.Decoder -Dexec.args=\"\\ --geoCityDatabase=GeoLite2-City.mmdb \\ --geoCityFilter=cities15000.txt \\ --schemasLocation=schemas.tar.gz \\ --inputType=file \\ --input=tmp/input.json \\ --outputType=stdout \\ --errorOutputType=stderr \\ \" # check the DecoderOptions help page for options specific to Decoder ./bin/mvn compile exec:java -Dexec.args=--help=DecoderOptions \"","title":"Decoder Job"},{"location":"ingestion-beam/decoder-job/#decoder-job","text":"A job for normalizing ingestion messages. Defined in the com.mozilla.telemetry.Decoder class ( source ).","title":"Decoder Job"},{"location":"ingestion-beam/decoder-job/#transforms","text":"These transforms are currently executed against each message in order.","title":"Transforms"},{"location":"ingestion-beam/decoder-job/#geoip-lookup","text":"Extract ip from the x_forwarded_for attribute when the x_pipeline_proxy attribute is not present, use the second-to-last value (since the last value is a forwarding rule IP added by Google load balancer) when the x_pipeline_proxy attribute is present, use the third-to-last value (since the tee introduces an additional proxy IP) fall back to the remote_addr attribute, then to an empty string Execute the following steps until one fails and ignore the exception Parse ip using InetAddress.getByName Lookup ip in the configured GeoIP2City.mmdb Extract country.iso_code as geo_country Extract city.name as geo_city if cities15000.txt is not configured or city.geo_name_id is in the configured cities15000.txt Extract subdivisions[0].iso_code as geo_subdivision1 Extract subdivisions[1].iso_code as geo_subdivision2 Remove the x_forwarded_for and remote_addr attributes Remove any null values added to attributes","title":"GeoIP Lookup"},{"location":"ingestion-beam/decoder-job/#parse-uri","text":"Attempt to extract attributes from uri , on failure send messages to the configured error output.","title":"Parse URI"},{"location":"ingestion-beam/decoder-job/#decompress","text":"Attempt to decompress payload with gzip, on failure pass the message through unmodified.","title":"Decompress"},{"location":"ingestion-beam/decoder-job/#parse-payload","text":"Parse the message body as a UTF-8 encoded JSON payload Drop specific fields or entire messages that match a specific set of signatures for toxic data that we want to make sure we do not store Maintain counter metrics for each type of dropped message Validate the payload structure based on the JSON schema for the specified document type Invalid messages are routed to error output Extract some additional attributes such as client_id and os_name based on the payload contents","title":"Parse Payload"},{"location":"ingestion-beam/decoder-job/#parse-user-agent","text":"Attempt to extract browser, browser version, and os from the user_agent attribute, drop any nulls, and remove user_agent from attributes.","title":"Parse User Agent"},{"location":"ingestion-beam/decoder-job/#write-metadata-into-the-payload","text":"Add a nested metadata field and several normalized_* attributes into the payload body.","title":"Write Metadata Into the Payload"},{"location":"ingestion-beam/decoder-job/#executing","text":"Decoder jobs are executed the same way as sink jobs but with a few extra flags: -Dexec.mainClass=com.mozilla.telemetry.Decoder --geoCityDatabase=/path/to/GeoIP2-City.mmdb --geoCityFilter=/path/to/cities15000.txt (optional) To download the GeoLite2 database , you need to register for a MaxMind account to obtain a license key. After generating a new license key, set MM_LICENSE_KEY to your license key. Example: # create a test input file mkdir -p tmp/ echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"remote_addr\":\"63.245.208.195\"}}' > tmp/input.json # Download `cities15000.txt`, `GeoLite2-City.mmdb`, and `schemas.tar.gz` ./bin/download-cities15000 ./bin/download-schemas export MM_LICENSE_KEY=\"Your MaxMind License Key\" ./bin/download-geolite2 # do geo lookup on messages to stdout ./bin/mvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.Decoder -Dexec.args=\"\\ --geoCityDatabase=GeoLite2-City.mmdb \\ --geoCityFilter=cities15000.txt \\ --schemasLocation=schemas.tar.gz \\ --inputType=file \\ --input=tmp/input.json \\ --outputType=stdout \\ --errorOutputType=stderr \\ \" # check the DecoderOptions help page for options specific to Decoder ./bin/mvn compile exec:java -Dexec.args=--help=DecoderOptions \"","title":"Executing"},{"location":"ingestion-beam/ingestion_testing_workflow/","text":"Ingestion Testing Workflow The ingestion-beam handles data flow of documents from the edge into various sinks. You may be interested in standing up a small testing instance to validate the integration of the various components. Figure : An overview of the various components necessary to query BigQuery against data from a PubSub subscription. Setting up the GCS project Read through whd/gcp-quickstart for details about the sandbox environment that is provided by data operations. Install the Google Cloud SDK Navigate to the Google Cloud Console Create a new project under firefox.gcp.mozilla.com/dataops/sandbox gcloud config set project <PROJECT> Create a PubSub subscription (see gcp-quickstart/pubsub.sh ) Create a GCS bucket gsutil mb gs://<PROJECT> Enable the Dataflow API Create a service account and store the key locally Bootstrapping schemas from mozilla-pipeline-schemas Download the latest schemas from mozilla-pipeline-schemas using bin/download-schemas . This script may also inject testing resources into the resulting archive. A schemas.tar.gz will appear at the project root. Copy generated BigQuery schemas using bin/copy-bq-schemas . Schemas will be written to bq-schemas/ with a .bq extension Schemas can be generated directly from JSON Schema using bin/generate-bq-schemas bq-schemas/ \u251c\u2500\u2500 activity-stream.impression-stats.1.bq \u251c\u2500\u2500 coverage.coverage.1.bq \u251c\u2500\u2500 edge-validator.error-report.1.bq \u251c\u2500\u2500 eng-workflow.bmobugs.1.bq .... Update the BigQuery table in the current project using bin/update-bq-table . This may take several minutes. Read the script for usage information. Each namespace will be given its own dataset and each document type its own table. Verify that tables have been updated by viewing the BigQuery console. Download a copy of sampled documents using bin/download-document-sample Upload this to your project's data bucket e.g. gs://$PROJECT/data/ Building the project Follow the instructions of the project readme. Here is a quick-reference for a running a job from a set of files in GCS. # this must be an absolute path export GOOGLE_APPLICATION_CREDENTIALS=keys.json PROJECT=$(gcloud config get-value project) BUCKET=\"gs://$PROJECT\" path=\"$BUCKET/data/*.ndjson\" # use local maven instead of the docker container in bin/mvn, otherwise make sure to mount # credentials into the proper location in the container mvn compile exec:java -Dexec.args=\"\\ --runner=Dataflow \\ --project=$PROJECT \\ --autoscalingAlgorithm=NONE \\ --workerMachineType=n1-standard-1 \\ --numWorkers=1 \\ --gcpTempLocation=$BUCKET/tmp \\ --inputFileFormat=json \\ --inputType=file \\ --input=$path\\ --outputType=bigquery \\ --output=$PROJECT:test_ingestion.\\${document_namespace}__\\${document_type}_v\\${document_version} \\ --bqWriteMethod=file_loads \\ --tempLocation=$BUCKET/temp/bq-loads \\ --errorOutputType=file \\ --errorOutput=$BUCKET/error/ \\ \"","title":"Ingestion testing workflow"},{"location":"ingestion-beam/ingestion_testing_workflow/#ingestion-testing-workflow","text":"The ingestion-beam handles data flow of documents from the edge into various sinks. You may be interested in standing up a small testing instance to validate the integration of the various components. Figure : An overview of the various components necessary to query BigQuery against data from a PubSub subscription.","title":"Ingestion Testing Workflow"},{"location":"ingestion-beam/ingestion_testing_workflow/#setting-up-the-gcs-project","text":"Read through whd/gcp-quickstart for details about the sandbox environment that is provided by data operations. Install the Google Cloud SDK Navigate to the Google Cloud Console Create a new project under firefox.gcp.mozilla.com/dataops/sandbox gcloud config set project <PROJECT> Create a PubSub subscription (see gcp-quickstart/pubsub.sh ) Create a GCS bucket gsutil mb gs://<PROJECT> Enable the Dataflow API Create a service account and store the key locally","title":"Setting up the GCS project"},{"location":"ingestion-beam/ingestion_testing_workflow/#bootstrapping-schemas-from-mozilla-pipeline-schemas","text":"Download the latest schemas from mozilla-pipeline-schemas using bin/download-schemas . This script may also inject testing resources into the resulting archive. A schemas.tar.gz will appear at the project root. Copy generated BigQuery schemas using bin/copy-bq-schemas . Schemas will be written to bq-schemas/ with a .bq extension Schemas can be generated directly from JSON Schema using bin/generate-bq-schemas bq-schemas/ \u251c\u2500\u2500 activity-stream.impression-stats.1.bq \u251c\u2500\u2500 coverage.coverage.1.bq \u251c\u2500\u2500 edge-validator.error-report.1.bq \u251c\u2500\u2500 eng-workflow.bmobugs.1.bq .... Update the BigQuery table in the current project using bin/update-bq-table . This may take several minutes. Read the script for usage information. Each namespace will be given its own dataset and each document type its own table. Verify that tables have been updated by viewing the BigQuery console. Download a copy of sampled documents using bin/download-document-sample Upload this to your project's data bucket e.g. gs://$PROJECT/data/","title":"Bootstrapping schemas from mozilla-pipeline-schemas"},{"location":"ingestion-beam/ingestion_testing_workflow/#building-the-project","text":"Follow the instructions of the project readme. Here is a quick-reference for a running a job from a set of files in GCS. # this must be an absolute path export GOOGLE_APPLICATION_CREDENTIALS=keys.json PROJECT=$(gcloud config get-value project) BUCKET=\"gs://$PROJECT\" path=\"$BUCKET/data/*.ndjson\" # use local maven instead of the docker container in bin/mvn, otherwise make sure to mount # credentials into the proper location in the container mvn compile exec:java -Dexec.args=\"\\ --runner=Dataflow \\ --project=$PROJECT \\ --autoscalingAlgorithm=NONE \\ --workerMachineType=n1-standard-1 \\ --numWorkers=1 \\ --gcpTempLocation=$BUCKET/tmp \\ --inputFileFormat=json \\ --inputType=file \\ --input=$path\\ --outputType=bigquery \\ --output=$PROJECT:test_ingestion.\\${document_namespace}__\\${document_type}_v\\${document_version} \\ --bqWriteMethod=file_loads \\ --tempLocation=$BUCKET/temp/bq-loads \\ --errorOutputType=file \\ --errorOutput=$BUCKET/error/ \\ \"","title":"Building the project"},{"location":"ingestion-beam/republisher-job/","text":"Republisher Job A job for republishing subsets of decoded messages to new destinations. Defined in the com.mozilla.telemetry.Republisher class ( source ). The primary intention is to produce smaller derived Pub/Sub topics so that consumers that only need a specific subset of messages don't incur the cost of reading the entire stream of decoded messages. The Republisher has the additional responsibility of marking messages as seen in Cloud MemoryStore for deduplication purposes. That functionality exists here to avoid the expense of an additional separate consumer of the full decoded topic. Capabilities Marking Messages As Seen The job needs to connect to Redis in order to mark document_id s of consumed messages as seen. The Decoder is able to use that information to drop duplicate messages flowing through the pipeline. Debug Republishing If --enableDebugDestination is set, messages containing an x_debug_id attribute will be republished to a destination that's configurable at runtime. This is currently expected to be a feature specific to structured ingestion, so should not be set for telemetry-decoded input. Per- docType Republishing If --perDocTypeEnabledList is provided, a separate producer will be created for each docType specified in the given comma-separated list. See the --help output for details on format. Per-Channel Sampled Republishing If --perChannelSampleRatios is provided, a separate producer will be created for each specified release channel. The messages will be randomly sampled according to the ratios provided per channel. This is currently intended as a feature only for telemetry data, so should not be set for structured-decoded input. See the --help output for details on format. Executing Republisher jobs are executed the same way as sink jobs but with a few differences in flags. You'll need to set the mainClass : -Dexec.mainClass=com.mozilla.telemetry.Republisher The --outputType flag is still required as in the sink, but the --output configuration is ignored for the Republisher. Instead, there is a separate destination configuration flag for each of the three republishing types. For each type, there is an compile-time option that affects what publishers are generated in the graph for the Dataflow job along with a runtime option that determines the specific location (usually a topic name) for each publisher. To enable debug republishing: --enableDebugDestination (compile-time) --debugDestination=/some/pubsub/topic/path To enable per- docType republishing: --perDocTypeDestination='{\"/some/pubsub/topic/path/per-doctype-name\":[\"activity-stream/impression-stats\"]}' (compile-time) To enable per-channel sampled republishing: --perChannelSampleRatios='{\"nightly\":1.0,\"beta\":0.1,\"release\":0.01}' (compile-time) --perChannelDestination=/some/pubsub/topic/path/per-channel-${channel} (compile-time) Example: # create a test input file mkdir -p tmp/ echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"x_debug_id\":\"mysession\"}}' > tmp/input.json # Republish only messages with x_debug_id attribute to stdout. ./bin/mvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.Republisher -Dexec.args=\"\\ --inputType=file \\ --input=tmp/input.json \\ --outputType=stdout \\ --errorOutputType=stderr \\ --enableDebugDestination \" # check the RepublisherOptions help page for options specific to Republisher ./bin/mvn compile exec:java -Dexec.args=--help=RepublisherOptions \"","title":"Republisher Job"},{"location":"ingestion-beam/republisher-job/#republisher-job","text":"A job for republishing subsets of decoded messages to new destinations. Defined in the com.mozilla.telemetry.Republisher class ( source ). The primary intention is to produce smaller derived Pub/Sub topics so that consumers that only need a specific subset of messages don't incur the cost of reading the entire stream of decoded messages. The Republisher has the additional responsibility of marking messages as seen in Cloud MemoryStore for deduplication purposes. That functionality exists here to avoid the expense of an additional separate consumer of the full decoded topic.","title":"Republisher Job"},{"location":"ingestion-beam/republisher-job/#capabilities","text":"","title":"Capabilities"},{"location":"ingestion-beam/republisher-job/#marking-messages-as-seen","text":"The job needs to connect to Redis in order to mark document_id s of consumed messages as seen. The Decoder is able to use that information to drop duplicate messages flowing through the pipeline.","title":"Marking Messages As Seen"},{"location":"ingestion-beam/republisher-job/#debug-republishing","text":"If --enableDebugDestination is set, messages containing an x_debug_id attribute will be republished to a destination that's configurable at runtime. This is currently expected to be a feature specific to structured ingestion, so should not be set for telemetry-decoded input.","title":"Debug Republishing"},{"location":"ingestion-beam/republisher-job/#per-doctype-republishing","text":"If --perDocTypeEnabledList is provided, a separate producer will be created for each docType specified in the given comma-separated list. See the --help output for details on format.","title":"Per-docType Republishing"},{"location":"ingestion-beam/republisher-job/#per-channel-sampled-republishing","text":"If --perChannelSampleRatios is provided, a separate producer will be created for each specified release channel. The messages will be randomly sampled according to the ratios provided per channel. This is currently intended as a feature only for telemetry data, so should not be set for structured-decoded input. See the --help output for details on format.","title":"Per-Channel Sampled Republishing"},{"location":"ingestion-beam/republisher-job/#executing","text":"Republisher jobs are executed the same way as sink jobs but with a few differences in flags. You'll need to set the mainClass : -Dexec.mainClass=com.mozilla.telemetry.Republisher The --outputType flag is still required as in the sink, but the --output configuration is ignored for the Republisher. Instead, there is a separate destination configuration flag for each of the three republishing types. For each type, there is an compile-time option that affects what publishers are generated in the graph for the Dataflow job along with a runtime option that determines the specific location (usually a topic name) for each publisher. To enable debug republishing: --enableDebugDestination (compile-time) --debugDestination=/some/pubsub/topic/path To enable per- docType republishing: --perDocTypeDestination='{\"/some/pubsub/topic/path/per-doctype-name\":[\"activity-stream/impression-stats\"]}' (compile-time) To enable per-channel sampled republishing: --perChannelSampleRatios='{\"nightly\":1.0,\"beta\":0.1,\"release\":0.01}' (compile-time) --perChannelDestination=/some/pubsub/topic/path/per-channel-${channel} (compile-time) Example: # create a test input file mkdir -p tmp/ echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"x_debug_id\":\"mysession\"}}' > tmp/input.json # Republish only messages with x_debug_id attribute to stdout. ./bin/mvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.Republisher -Dexec.args=\"\\ --inputType=file \\ --input=tmp/input.json \\ --outputType=stdout \\ --errorOutputType=stderr \\ --enableDebugDestination \" # check the RepublisherOptions help page for options specific to Republisher ./bin/mvn compile exec:java -Dexec.args=--help=RepublisherOptions \"","title":"Executing"},{"location":"ingestion-beam/sink-job/","text":"Sink Job A job for delivering messages between Google Cloud services. Defined in the com.mozilla.telemetry.Sink class ( source ). Supported Input and Outputs Supported inputs: Google Cloud PubSub Google Cloud Storage Supported outputs: Google Cloud PubSub Google Cloud Storage Google Cloud BigQuery stdout stderr Supported error outputs, must include attributes and must not validate messages: Google Cloud PubSub Google Cloud Storage with JSON encoding stdout with JSON encoding stderr with JSON encoding Encoding Internally messages are stored and transported as PubsubMessage . Supported file formats for Cloud Storage are json or text . The json file format stores newline delimited JSON, encoding the field payload as a base64 string, and attributeMap as an optional object with string keys and values. The text file format stores newline delimited strings, encoding the field payload as UTF-8 . We'll construct example inputs based on the following two values and their base64 encodings: $ echo -en \"test\" | base64 dGVzdA== $ echo -en \"test\\n\" | base64 dGVzdAo= Example json file: {\"payload\":\"dGVzdA==\",\"attributeMap\":{\"meta\":\"data\"}} {\"payload\":\"dGVzdAo=\",\"attributeMap\":null} {\"payload\":\"dGVzdA==\"} The above file when stored in the text format: test test test Note that the newline embedded at the end of the second JSON message results in two text messages, one of which is blank. Output Path Specification Depending on the specified output type, the --output path that you provide controls several aspects of the behavior. BigQuery When --outputType=bigquery , --output is a tableSpec of form dataset.tablename or the more verbose projectId:dataset.tablename . The values can contain attribute placeholders of form ${attribute_name} . To set dataset to the document namespace and table name to the document type, specify: --output='${document_namespace}.${document_type}' All - characters in the attributes will be converted to _ per BigQuery naming restrictions. Additionally, document namespace and type values will be processed to ensure they are in snake case format ( untrustedModules becomes untrusted_modules ). Defaults for the placeholders using ${attribute_name:-default_value} are supported, but likely don't make much sense since it's unlikely that there is a default table whose schema is compatible with all potential payloads. Instead, records missing an attribute required by a placeholder will be redirected to error output if no default is provided. Protocol When --outputType=file , --output may be prefixed by a protocol specifier to determine the target data store. Without a protocol prefix, the output path is assumed to be a relative or absolute path on the filesystem. To write to Google Cloud Storage, use a gs:// path like: --output=gs://mybucket/somdir/myfileprefix Attribute placeholders We support FileIO 's \"Dynamic destinations\" feature ( FileIO.writeDynamic ) where it's possible to route individual messages to different output locations based on properties of the message. In our case, we allow routing messages based on the PubsubMessage attribute map. Routing is accomplished by adding placeholders of form ${attribute_name:-default_value} to the path. For example, to route based on a document_type attribute, your path might look like: --output=gs://mybucket/mydocs/${document_type:-UNSPECIFIED}/myfileprefix Messages with document_type of \"main\" would be grouped together and end up in the following directory: gs://mybucket/mydocs/main/ Messages with document_type set to null or missing that attribute completely would be grouped together and end up in directory: gs://mybucket/mydocs/UNSPECIFIED/ Note that placeholders must specify a default value so that a poorly formatted message doesn't cause a pipeline exception. A placeholder without a default will result in an IllegalArgumentException on pipeline startup. File-based outputs support the additional derived attributes \"submission_date\" and \"submission_hour\" which will be parsed from the value of the submission_timestamp attribute if it exists. These can be useful for making sure your output specification buckets messages into hourly directories. The templating and default syntax used here is based on the Apache commons-text StringSubstitutor , which in turn bases its syntax on common practice in bash and other Unix/Linux shells. Beware the need for proper escaping on the command line (use \\$ in place of $ ), as your shell may try to substitute in values for your placeholders before they're passed to Sink . Google's PubsubMessage format allows arbitrary strings for attribute names and values. We place the following restrictions on attribute names and default values used in placeholders: attribute names may not contain the string :- attribute names may not contain curly braces ( { or } ) default values may not contain curly braces ( { or } ) File prefix Individual files are named by replacing : with - in the default format discussed in the \"File naming\" section of Beam's FileIO Javadoc : $prefix-$start-$end-$pane-$shard-of-$numShards$suffix$compressionSuffix In our case, $prefix is determined from the last / -delimited piece of the --output path. If you specify a path ending in / , you'll end up with an empty prefix and your file names will begin with - . This is probably not what you want, so it's recommended to end your output path with a non-empty file prefix. We replace : with - because Hadoop can't handle : in file names . For example, given: --output=/tmp/output/out An output file might be: /tmp/output/out--290308-12-21T20-00-00.000Z--290308-12-21T20-10-00.000Z-00000-of-00001.ndjson Executing Note: -Dexec.args does not handle newlines gracefully, but bash will remove \\ escaped newlines in \" s. Locally If you install Java and maven, you can invoke mvn in the following commands instead of using ./bin/mvn ; be aware, though, that Java 8 is the target JVM and some reflection warnings may be thrown on newer versions, though these are generally harmless. The provided bin/mvn script downloads and runs maven via docker so that less setup is needed on the local machine. For prolonged development performance is likely to be significantly better, especially in MacOS, if mvn is installed and run natively without docker. # create a test input file mkdir -p tmp/ echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' > tmp/input.json # consume messages from the test file, decode and re-encode them, and write to a directory ./bin/mvn compile exec:java -Dexec.args=\"\\ --inputFileFormat=json \\ --inputType=file \\ --input=tmp/input.json \\ --outputFileFormat=json \\ --outputType=file \\ --output=tmp/output/out \\ --errorOutputType=file \\ --errorOutput=tmp/error \\ \" # check that the message was delivered cat tmp/output/* # write message payload straight to stdout ./bin/mvn compile exec:java -Dexec.args=\"\\ --inputFileFormat=json \\ --inputType=file \\ --input=tmp/input.json \\ --outputFileFormat=text \\ --outputType=stdout \\ --errorOutputType=stderr \\ \" # check the help page to see types of options ./bin/mvn compile exec:java -Dexec.args=--help # check the SinkOptions help page for options specific to Sink ./bin/mvn compile exec:java -Dexec.args=--help=SinkOptions On Dataflow # Pick a bucket to store files in BUCKET=\"gs://$(gcloud config get-value project)\" # create a test input file echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' | gsutil cp - $BUCKET/input.json # Set credentials; beam is not able to use gcloud credentials export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/creds.json\" # consume messages from the test file, decode and re-encode them, and write to a bucket ./bin/mvn compile exec:java -Dexec.args=\"\\ --runner=Dataflow \\ --inputFileFormat=json \\ --inputType=file \\ --input=$BUCKET/input.json \\ --outputFileFormat=json \\ --outputType=file \\ --output=$BUCKET/output \\ --errorOutputType=file \\ --errorOutput=$BUCKET/error \\ \" # wait for the job to finish gcloud dataflow jobs list # check that the message was delivered gsutil cat $BUCKET/output/* On Dataflow with templates Dataflow templates make a distinction between runtime parameters that implement the ValueProvider interface and compile-time parameters which do not. All options can be specified at template compile time by passing command line flags, but runtime parameters can also be overridden when executing the template via the --parameters flag. In the output of --help=SinkOptions , runtime parameters are those with type ValueProvider . # Pick a bucket to store files in BUCKET=\"gs://$(gcloud config get-value project)\" # Set credentials; beam is not able to use gcloud credentials export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/creds.json\" # create a template ./bin/mvn compile exec:java -Dexec.args=\"\\ --runner=Dataflow \\ --project=$(gcloud config get-value project) \\ --inputFileFormat=json \\ --inputType=file \\ --outputFileFormat=json \\ --outputType=file \\ --errorOutputType=file \\ --templateLocation=$BUCKET/sink/templates/JsonFileToJsonFile \\ --stagingLocation=$BUCKET/sink/staging \\ \" # create a test input file echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' | gsutil cp - $BUCKET/input.json # run the dataflow template with gcloud JOBNAME=FileToFile1 gcloud dataflow jobs run $JOBNAME --gcs-location=$BUCKET/sink/templates/JsonFileToJsonFile --parameters \"input=$BUCKET/input.json,output=$BUCKET/output/,errorOutput=$BUCKET/error\" # get the job id JOB_ID=\"$(gcloud dataflow jobs list --filter name=fileToStdout1 | tail -1 | cut -d' ' -f1)\" # wait for the job to finish gcloud dataflow jobs show \"$JOB_ID\" # check that the message was delivered gsutil cat $BUCKET/output/* In streaming mode If --inputType=pubsub , Beam will execute in streaming mode, requiring some extra configuration for file-based outputs. You will need to specify sharding like: --outputNumShards=10 --errorOutputNumShards=10 As discussed in the Beam documentation for FileIO.Write#withNumShards , batch mode is most efficient when the runner is left to determine sharding, so numShards options should normally be left to their default of 0 , but streaming mode can't perform the same optimizations thus an exception will be thrown during pipeline construction if sharding is not specified. As codified in apache/beam/pull/1952 , the Dataflow runner suggests a reasonable starting point numShards is 2 * maxWorkers or 10 if --maxWorkers is unspecified.","title":"Sink Job"},{"location":"ingestion-beam/sink-job/#sink-job","text":"A job for delivering messages between Google Cloud services. Defined in the com.mozilla.telemetry.Sink class ( source ).","title":"Sink Job"},{"location":"ingestion-beam/sink-job/#supported-input-and-outputs","text":"Supported inputs: Google Cloud PubSub Google Cloud Storage Supported outputs: Google Cloud PubSub Google Cloud Storage Google Cloud BigQuery stdout stderr Supported error outputs, must include attributes and must not validate messages: Google Cloud PubSub Google Cloud Storage with JSON encoding stdout with JSON encoding stderr with JSON encoding","title":"Supported Input and Outputs"},{"location":"ingestion-beam/sink-job/#encoding","text":"Internally messages are stored and transported as PubsubMessage . Supported file formats for Cloud Storage are json or text . The json file format stores newline delimited JSON, encoding the field payload as a base64 string, and attributeMap as an optional object with string keys and values. The text file format stores newline delimited strings, encoding the field payload as UTF-8 . We'll construct example inputs based on the following two values and their base64 encodings: $ echo -en \"test\" | base64 dGVzdA== $ echo -en \"test\\n\" | base64 dGVzdAo= Example json file: {\"payload\":\"dGVzdA==\",\"attributeMap\":{\"meta\":\"data\"}} {\"payload\":\"dGVzdAo=\",\"attributeMap\":null} {\"payload\":\"dGVzdA==\"} The above file when stored in the text format: test test test Note that the newline embedded at the end of the second JSON message results in two text messages, one of which is blank.","title":"Encoding"},{"location":"ingestion-beam/sink-job/#output-path-specification","text":"Depending on the specified output type, the --output path that you provide controls several aspects of the behavior.","title":"Output Path Specification"},{"location":"ingestion-beam/sink-job/#bigquery","text":"When --outputType=bigquery , --output is a tableSpec of form dataset.tablename or the more verbose projectId:dataset.tablename . The values can contain attribute placeholders of form ${attribute_name} . To set dataset to the document namespace and table name to the document type, specify: --output='${document_namespace}.${document_type}' All - characters in the attributes will be converted to _ per BigQuery naming restrictions. Additionally, document namespace and type values will be processed to ensure they are in snake case format ( untrustedModules becomes untrusted_modules ). Defaults for the placeholders using ${attribute_name:-default_value} are supported, but likely don't make much sense since it's unlikely that there is a default table whose schema is compatible with all potential payloads. Instead, records missing an attribute required by a placeholder will be redirected to error output if no default is provided.","title":"BigQuery"},{"location":"ingestion-beam/sink-job/#protocol","text":"When --outputType=file , --output may be prefixed by a protocol specifier to determine the target data store. Without a protocol prefix, the output path is assumed to be a relative or absolute path on the filesystem. To write to Google Cloud Storage, use a gs:// path like: --output=gs://mybucket/somdir/myfileprefix","title":"Protocol"},{"location":"ingestion-beam/sink-job/#attribute-placeholders","text":"We support FileIO 's \"Dynamic destinations\" feature ( FileIO.writeDynamic ) where it's possible to route individual messages to different output locations based on properties of the message. In our case, we allow routing messages based on the PubsubMessage attribute map. Routing is accomplished by adding placeholders of form ${attribute_name:-default_value} to the path. For example, to route based on a document_type attribute, your path might look like: --output=gs://mybucket/mydocs/${document_type:-UNSPECIFIED}/myfileprefix Messages with document_type of \"main\" would be grouped together and end up in the following directory: gs://mybucket/mydocs/main/ Messages with document_type set to null or missing that attribute completely would be grouped together and end up in directory: gs://mybucket/mydocs/UNSPECIFIED/ Note that placeholders must specify a default value so that a poorly formatted message doesn't cause a pipeline exception. A placeholder without a default will result in an IllegalArgumentException on pipeline startup. File-based outputs support the additional derived attributes \"submission_date\" and \"submission_hour\" which will be parsed from the value of the submission_timestamp attribute if it exists. These can be useful for making sure your output specification buckets messages into hourly directories. The templating and default syntax used here is based on the Apache commons-text StringSubstitutor , which in turn bases its syntax on common practice in bash and other Unix/Linux shells. Beware the need for proper escaping on the command line (use \\$ in place of $ ), as your shell may try to substitute in values for your placeholders before they're passed to Sink . Google's PubsubMessage format allows arbitrary strings for attribute names and values. We place the following restrictions on attribute names and default values used in placeholders: attribute names may not contain the string :- attribute names may not contain curly braces ( { or } ) default values may not contain curly braces ( { or } )","title":"Attribute placeholders"},{"location":"ingestion-beam/sink-job/#file-prefix","text":"Individual files are named by replacing : with - in the default format discussed in the \"File naming\" section of Beam's FileIO Javadoc : $prefix-$start-$end-$pane-$shard-of-$numShards$suffix$compressionSuffix In our case, $prefix is determined from the last / -delimited piece of the --output path. If you specify a path ending in / , you'll end up with an empty prefix and your file names will begin with - . This is probably not what you want, so it's recommended to end your output path with a non-empty file prefix. We replace : with - because Hadoop can't handle : in file names . For example, given: --output=/tmp/output/out An output file might be: /tmp/output/out--290308-12-21T20-00-00.000Z--290308-12-21T20-10-00.000Z-00000-of-00001.ndjson","title":"File prefix"},{"location":"ingestion-beam/sink-job/#executing","text":"Note: -Dexec.args does not handle newlines gracefully, but bash will remove \\ escaped newlines in \" s.","title":"Executing"},{"location":"ingestion-beam/sink-job/#locally","text":"If you install Java and maven, you can invoke mvn in the following commands instead of using ./bin/mvn ; be aware, though, that Java 8 is the target JVM and some reflection warnings may be thrown on newer versions, though these are generally harmless. The provided bin/mvn script downloads and runs maven via docker so that less setup is needed on the local machine. For prolonged development performance is likely to be significantly better, especially in MacOS, if mvn is installed and run natively without docker. # create a test input file mkdir -p tmp/ echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' > tmp/input.json # consume messages from the test file, decode and re-encode them, and write to a directory ./bin/mvn compile exec:java -Dexec.args=\"\\ --inputFileFormat=json \\ --inputType=file \\ --input=tmp/input.json \\ --outputFileFormat=json \\ --outputType=file \\ --output=tmp/output/out \\ --errorOutputType=file \\ --errorOutput=tmp/error \\ \" # check that the message was delivered cat tmp/output/* # write message payload straight to stdout ./bin/mvn compile exec:java -Dexec.args=\"\\ --inputFileFormat=json \\ --inputType=file \\ --input=tmp/input.json \\ --outputFileFormat=text \\ --outputType=stdout \\ --errorOutputType=stderr \\ \" # check the help page to see types of options ./bin/mvn compile exec:java -Dexec.args=--help # check the SinkOptions help page for options specific to Sink ./bin/mvn compile exec:java -Dexec.args=--help=SinkOptions","title":"Locally"},{"location":"ingestion-beam/sink-job/#on-dataflow","text":"# Pick a bucket to store files in BUCKET=\"gs://$(gcloud config get-value project)\" # create a test input file echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' | gsutil cp - $BUCKET/input.json # Set credentials; beam is not able to use gcloud credentials export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/creds.json\" # consume messages from the test file, decode and re-encode them, and write to a bucket ./bin/mvn compile exec:java -Dexec.args=\"\\ --runner=Dataflow \\ --inputFileFormat=json \\ --inputType=file \\ --input=$BUCKET/input.json \\ --outputFileFormat=json \\ --outputType=file \\ --output=$BUCKET/output \\ --errorOutputType=file \\ --errorOutput=$BUCKET/error \\ \" # wait for the job to finish gcloud dataflow jobs list # check that the message was delivered gsutil cat $BUCKET/output/*","title":"On Dataflow"},{"location":"ingestion-beam/sink-job/#on-dataflow-with-templates","text":"Dataflow templates make a distinction between runtime parameters that implement the ValueProvider interface and compile-time parameters which do not. All options can be specified at template compile time by passing command line flags, but runtime parameters can also be overridden when executing the template via the --parameters flag. In the output of --help=SinkOptions , runtime parameters are those with type ValueProvider . # Pick a bucket to store files in BUCKET=\"gs://$(gcloud config get-value project)\" # Set credentials; beam is not able to use gcloud credentials export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/creds.json\" # create a template ./bin/mvn compile exec:java -Dexec.args=\"\\ --runner=Dataflow \\ --project=$(gcloud config get-value project) \\ --inputFileFormat=json \\ --inputType=file \\ --outputFileFormat=json \\ --outputType=file \\ --errorOutputType=file \\ --templateLocation=$BUCKET/sink/templates/JsonFileToJsonFile \\ --stagingLocation=$BUCKET/sink/staging \\ \" # create a test input file echo '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' | gsutil cp - $BUCKET/input.json # run the dataflow template with gcloud JOBNAME=FileToFile1 gcloud dataflow jobs run $JOBNAME --gcs-location=$BUCKET/sink/templates/JsonFileToJsonFile --parameters \"input=$BUCKET/input.json,output=$BUCKET/output/,errorOutput=$BUCKET/error\" # get the job id JOB_ID=\"$(gcloud dataflow jobs list --filter name=fileToStdout1 | tail -1 | cut -d' ' -f1)\" # wait for the job to finish gcloud dataflow jobs show \"$JOB_ID\" # check that the message was delivered gsutil cat $BUCKET/output/*","title":"On Dataflow with templates"},{"location":"ingestion-beam/sink-job/#in-streaming-mode","text":"If --inputType=pubsub , Beam will execute in streaming mode, requiring some extra configuration for file-based outputs. You will need to specify sharding like: --outputNumShards=10 --errorOutputNumShards=10 As discussed in the Beam documentation for FileIO.Write#withNumShards , batch mode is most efficient when the runner is left to determine sharding, so numShards options should normally be left to their default of 0 , but streaming mode can't perform the same optimizations thus an exception will be thrown during pipeline construction if sharding is not specified. As codified in apache/beam/pull/1952 , the Dataflow runner suggests a reasonable starting point numShards is 2 * maxWorkers or 10 if --maxWorkers is unspecified.","title":"In streaming mode"},{"location":"ingestion-edge/","text":"Ingestion Edge Service A simple service for delivering HTTP messages to Google Cloud PubSub The source code lives in the ingestion-edge subdirectory of the gcp-ingestion repository. Building We assume that you have docker-compose installed. From inside the ingestion-edge subdirectory: # docker-compose docker-compose build # pytest bin/build Running Use docker-compose to run a local development server that auto-detects changes: # run the web server and PubSub emulator docker-compose up --detach web # manually check the server curl http://localhost:8000/__version__ curl http://localhost:8000/__heartbeat__ curl http://localhost:8000/__lbheartbeat__ curl http://localhost:8000/submit/test -d \"test\" # check web logs docker-compose logs web # clean up docker-compose environment docker-compose down --timeout 0 Configuration The ingestion-edge docker container accepts these configuration options from environment variables: ROUTE_TABLE : a JSON list of mappings from uri to PubSub topic, uri matches are detected in order, defaults to [] , each mapping is a list and may include an optional third element that specifies a list of allowed methods instead of the default [\"POST\",\"PUT\"] QUEUE_PATH : a filesystem path to a directory where a SQLite database will be created to store requests for when PubSub is unavailable, paths may be relative to the docker container WORKDIR , defaults to queue MINIMUM_DISK_FREE_BYTES : an integer indicating the threshold of free bytes on the filesystem where QUEUE_PATH is mounted below which /__heartbeat__ will fail, defaults to 0 which disables the check METADATA_HEADERS : a JSON list of headers to preserve as PubSub message attributes, defaults to [\"Content-Length\", \"Date\", \"DNT\", \"User-Agent\", \"X-Forwarded-For\", \"X-Pingsender-Version\", \"X-Pipeline-Proxy\", \"X-Debug-ID\"] ; the message attribute name will be the header name in lowercase and with - converted to _ PUBLISH_TIMEOUT_SECONDS : a float indicating the maximum number of seconds to wait for the PubSub client to complete a publish operation, defaults to 1 second and may require tuning FLUSH_CONCURRENT_MESSAGES : an integer indicating the number of messages per worker that may be read from the queue before waiting on publish results, defaults to 1000 messages based on publish request limits and may require tuning FLUSH_CONCURRENT_BYTES : an integer indicating the number of bytes per worker that may be read from the queue before waiting on publish results, which may be exceeded by one message and measures data bytes rather than serialized message size, defaults to 10MB based on publish request limits and may require tuning FLUSH_SLEEP_SECONDS : a float indicating the number of seconds waited between flush attempts, defaults to 1 second and may require tuning Testing Run tests with CircleCI Local CLI , docker-compose , or pytest wrappers # circleci (cd .. && circleci build --job ingestion-edge) # docker-compose docker-compose run --rm test # pytest wrapper (pytest-all calls lint and pytest) ./bin/pytest-all The pytest wrappers add these options via the environment: CLEAN_RELOCATES controls whether bin/lint and bin/pytest will remove .pyc files not in venv/ that do not contain $PWD to prevent errors when switching between running in and out of docker, defaults to true VENV controls whether to use a python venv in venv/$(uname) in bin/lint and bin/pytest , and in bin/build to create and use that venv , defaults to false in Dockerfile and true otherwise Style Checks Run style checks # docker-compose docker-compose run --rm test bin/lint # pytest wrapper ./bin/lint Unit Tests Run unit tests # docker-compose docker-compose run --rm test bin/pytest tests/unit # pytest wrapper ./bin/pytest tests/unit Integration Tests Run integration tests locally # docker-compose docker-compose run --rm test bin/pytest tests/integration # pytest wrapper ./bin/pytest tests/integration Test a remote server (requires credentials to read PubSub) # define the same ROUTE_TABLE as your edge server export ROUTE_TABLE='[[\"/submit/telemetry/<suffix:path>\",\"projects/PROJECT/topics/TOPIC\"]]' # docker using latest image and no git checkout docker run --rm --tty --interactive --env ROUTE_TABLE mozilla/ingestion-edge:latest bin/pytest tests/integration --server https://myedgeserver.example.com # docker-compose docker-compose run --rm -e ROUTE_TABLE test bin/pytest tests/integration --server https://myedgeserver.example.com # pytest wrapper ./bin/pytest tests/integration --server https://myedgeserver.example.com Load Tests Run a load test (defaults to a single GKE cluster and a PubSub emulator) # docker using latest image and no git checkout docker run --rm --tty --interactive mozilla/ingestion-edge:latest bin/pytest tests/load # docker-compose docker-compose run --rm test bin/pytest tests/load # pytest ./bin/pytest tests/load Load test options (from ./bin/test -h ) --min-success-rate=MIN_SUCCESS_RATE Minimum 200 responses per non-200 response to require during --test-period, default is 1000 (0.1% errors) --min-throughput=MIN_THROUGHPUT Minimum 200 responses per second to require during --test-period, default is 15000 --test-period=TEST_PERIOD Number of seconds to evaluate after warmup, default is 1800 (30 minutes) --warmup-threshold=WARMUP_THRESHOLD Minimum 200 responses per second that indicate warmup is complete, default is 15000 --warmup-timeout=WARMUP_TIMEOUT Maximum number of seconds to wait for warmup to complete, default is 600 (10 minutes) --cluster=CLUSTER Name of GKE cluster to create for test resources, default is 'load-test', ignored when --load-balancer and --no-traffic-generator are both specified --location=LOCATION Location to use for --cluster, default is us-west1 --preemptible Use preemptible instances for --cluster, default is False --project=PROJECT Project to use for --cluster, default is from credentials --load-balancer=LOAD_BALANCER Load Balancing url map to monitor, implies --no- generator when --server-uri is not specified, ignores --image and --no-emulator --server-uri=SERVER_URI Server uri like 'https://edge.stage.domain.com/submit/ telemetry/suffix', ignored when --no-generator is specified or --load-balancer is missing --image=IMAGE Docker image for server deployment, default is 'mozilla/ingestion-edge:latest', ignored when --load- balancer is specified --no-emulator Don't use a PubSub emulator, ignored when --load- balancer is specified --topic=TOPIC PubSub topic name, default is 'topic', ignored when --load-balancer is specified --no-generator Don't deploy a traffic generator, ignore --script --script=SCRIPT Lua script to use for traffic generator deployment, default is 'tests/load/wrk/telemetry.lua', ignored when --no-generator is specified","title":"ingestion-edge"},{"location":"ingestion-edge/#ingestion-edge-service","text":"A simple service for delivering HTTP messages to Google Cloud PubSub The source code lives in the ingestion-edge subdirectory of the gcp-ingestion repository.","title":"Ingestion Edge Service"},{"location":"ingestion-edge/#building","text":"We assume that you have docker-compose installed. From inside the ingestion-edge subdirectory: # docker-compose docker-compose build # pytest bin/build","title":"Building"},{"location":"ingestion-edge/#running","text":"Use docker-compose to run a local development server that auto-detects changes: # run the web server and PubSub emulator docker-compose up --detach web # manually check the server curl http://localhost:8000/__version__ curl http://localhost:8000/__heartbeat__ curl http://localhost:8000/__lbheartbeat__ curl http://localhost:8000/submit/test -d \"test\" # check web logs docker-compose logs web # clean up docker-compose environment docker-compose down --timeout 0","title":"Running"},{"location":"ingestion-edge/#configuration","text":"The ingestion-edge docker container accepts these configuration options from environment variables: ROUTE_TABLE : a JSON list of mappings from uri to PubSub topic, uri matches are detected in order, defaults to [] , each mapping is a list and may include an optional third element that specifies a list of allowed methods instead of the default [\"POST\",\"PUT\"] QUEUE_PATH : a filesystem path to a directory where a SQLite database will be created to store requests for when PubSub is unavailable, paths may be relative to the docker container WORKDIR , defaults to queue MINIMUM_DISK_FREE_BYTES : an integer indicating the threshold of free bytes on the filesystem where QUEUE_PATH is mounted below which /__heartbeat__ will fail, defaults to 0 which disables the check METADATA_HEADERS : a JSON list of headers to preserve as PubSub message attributes, defaults to [\"Content-Length\", \"Date\", \"DNT\", \"User-Agent\", \"X-Forwarded-For\", \"X-Pingsender-Version\", \"X-Pipeline-Proxy\", \"X-Debug-ID\"] ; the message attribute name will be the header name in lowercase and with - converted to _ PUBLISH_TIMEOUT_SECONDS : a float indicating the maximum number of seconds to wait for the PubSub client to complete a publish operation, defaults to 1 second and may require tuning FLUSH_CONCURRENT_MESSAGES : an integer indicating the number of messages per worker that may be read from the queue before waiting on publish results, defaults to 1000 messages based on publish request limits and may require tuning FLUSH_CONCURRENT_BYTES : an integer indicating the number of bytes per worker that may be read from the queue before waiting on publish results, which may be exceeded by one message and measures data bytes rather than serialized message size, defaults to 10MB based on publish request limits and may require tuning FLUSH_SLEEP_SECONDS : a float indicating the number of seconds waited between flush attempts, defaults to 1 second and may require tuning","title":"Configuration"},{"location":"ingestion-edge/#testing","text":"Run tests with CircleCI Local CLI , docker-compose , or pytest wrappers # circleci (cd .. && circleci build --job ingestion-edge) # docker-compose docker-compose run --rm test # pytest wrapper (pytest-all calls lint and pytest) ./bin/pytest-all The pytest wrappers add these options via the environment: CLEAN_RELOCATES controls whether bin/lint and bin/pytest will remove .pyc files not in venv/ that do not contain $PWD to prevent errors when switching between running in and out of docker, defaults to true VENV controls whether to use a python venv in venv/$(uname) in bin/lint and bin/pytest , and in bin/build to create and use that venv , defaults to false in Dockerfile and true otherwise","title":"Testing"},{"location":"ingestion-edge/#style-checks","text":"Run style checks # docker-compose docker-compose run --rm test bin/lint # pytest wrapper ./bin/lint","title":"Style Checks"},{"location":"ingestion-edge/#unit-tests","text":"Run unit tests # docker-compose docker-compose run --rm test bin/pytest tests/unit # pytest wrapper ./bin/pytest tests/unit","title":"Unit Tests"},{"location":"ingestion-edge/#integration-tests","text":"Run integration tests locally # docker-compose docker-compose run --rm test bin/pytest tests/integration # pytest wrapper ./bin/pytest tests/integration Test a remote server (requires credentials to read PubSub) # define the same ROUTE_TABLE as your edge server export ROUTE_TABLE='[[\"/submit/telemetry/<suffix:path>\",\"projects/PROJECT/topics/TOPIC\"]]' # docker using latest image and no git checkout docker run --rm --tty --interactive --env ROUTE_TABLE mozilla/ingestion-edge:latest bin/pytest tests/integration --server https://myedgeserver.example.com # docker-compose docker-compose run --rm -e ROUTE_TABLE test bin/pytest tests/integration --server https://myedgeserver.example.com # pytest wrapper ./bin/pytest tests/integration --server https://myedgeserver.example.com","title":"Integration Tests"},{"location":"ingestion-edge/#load-tests","text":"Run a load test (defaults to a single GKE cluster and a PubSub emulator) # docker using latest image and no git checkout docker run --rm --tty --interactive mozilla/ingestion-edge:latest bin/pytest tests/load # docker-compose docker-compose run --rm test bin/pytest tests/load # pytest ./bin/pytest tests/load Load test options (from ./bin/test -h ) --min-success-rate=MIN_SUCCESS_RATE Minimum 200 responses per non-200 response to require during --test-period, default is 1000 (0.1% errors) --min-throughput=MIN_THROUGHPUT Minimum 200 responses per second to require during --test-period, default is 15000 --test-period=TEST_PERIOD Number of seconds to evaluate after warmup, default is 1800 (30 minutes) --warmup-threshold=WARMUP_THRESHOLD Minimum 200 responses per second that indicate warmup is complete, default is 15000 --warmup-timeout=WARMUP_TIMEOUT Maximum number of seconds to wait for warmup to complete, default is 600 (10 minutes) --cluster=CLUSTER Name of GKE cluster to create for test resources, default is 'load-test', ignored when --load-balancer and --no-traffic-generator are both specified --location=LOCATION Location to use for --cluster, default is us-west1 --preemptible Use preemptible instances for --cluster, default is False --project=PROJECT Project to use for --cluster, default is from credentials --load-balancer=LOAD_BALANCER Load Balancing url map to monitor, implies --no- generator when --server-uri is not specified, ignores --image and --no-emulator --server-uri=SERVER_URI Server uri like 'https://edge.stage.domain.com/submit/ telemetry/suffix', ignored when --no-generator is specified or --load-balancer is missing --image=IMAGE Docker image for server deployment, default is 'mozilla/ingestion-edge:latest', ignored when --load- balancer is specified --no-emulator Don't use a PubSub emulator, ignored when --load- balancer is specified --topic=TOPIC PubSub topic name, default is 'topic', ignored when --load-balancer is specified --no-generator Don't deploy a traffic generator, ignore --script --script=SCRIPT Lua script to use for traffic generator deployment, default is 'tests/load/wrk/telemetry.lua', ignored when --no-generator is specified","title":"Load Tests"}]}