{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GCP Ingestion","text":"<p>GCP Ingestion is a monorepo for documentation and implementation of the Mozilla telemetry ingestion system deployed to Google Cloud Platform (GCP).</p> <p>The components are:</p> <ul> <li>ingestion-edge: a simple Python service for accepting HTTP   messages and delivering to Google Cloud Pub/Sub   (deployment docs \ud83d\udd12)</li> <li>ingestion-beam: a Java module defining   Apache Beam jobs for streaming and batch   transformations of ingested messages   (deployment docs \ud83d\udd12)</li> <li>ingestion-sink: a Java application that runs   in Kubernetes, reading input from Google Cloud Pub/Sub and emitting   records to outputs like GCS or BigQuery   (deployment docs \ud83d\udd12)</li> </ul> <p>The design behind the system along with various trade offs are documented in the architecture section.</p> <p>This project requires Java 11. To manage multiple local JDKs, consider jenv and the <code>jenv enable-plugin maven</code> command. Also consider reading through Apache Beam's wiki article on IntelliJ IDEA setup for some ideas on configuring an IDE environment.</p> <p>Feel free to ask us in <code>#data-help</code> on Slack or <code>#telemetry</code> on <code>chat.mozilla.org</code> if you have specific questions.</p>"},{"location":"architecture/bigquery_sink_specification/","title":"Live Sink Service Specification","text":"<p>This document specifies the behavior of the service that delivers decoded messages into BigQuery.</p>"},{"location":"architecture/bigquery_sink_specification/#data-flow","title":"Data Flow","text":"<p>Consume messages from a PubSub topic or Cloud Storage location or BigQuery table and insert them into BigQuery. Send errors to another configurable location.</p>"},{"location":"architecture/bigquery_sink_specification/#implementation","title":"Implementation","text":"<p>Execute this as an Apache Beam job. Note: As of February 2020, we are transitioning this sink to a custom Java application running on GKE.</p>"},{"location":"architecture/bigquery_sink_specification/#configuration","title":"Configuration","text":"<p>Require configuration for:</p> <ul> <li>The input PubSub topic, Cloud Storage location, or BigQuery table</li> <li>The route map from PubSub message attributes to output BigQuery table</li> <li>The error output PubSub topic, Cloud Storage location, or BigQuery table</li> </ul> <p>Accept optional configuration for:</p> <ul> <li>The fallback output PubSub topic for messages with no route</li> <li>The output mode for BigQuery, default to <code>mixed</code></li> <li>List of document types to opt-in for streaming when running in <code>mixed</code>   output mode</li> <li>The triggering frequency for writing to BigQuery, when output mode is   <code>file_loads</code></li> </ul>"},{"location":"architecture/bigquery_sink_specification/#coerce-types","title":"Coerce Types","text":"<p>Reprocess the JSON payload in each message to match the schema of the destination table found in BigQuery as codified by the <code>jsonschema-transpiler</code>.</p> <p>Support the following logical transformations:</p> <ul> <li>Transform key names to replace <code>-</code> and <code>.</code> with <code>_</code></li> <li>Transform key names beginning with a number by prefixing with <code>_</code></li> <li>Transform map types to arrays of key/value maps when the destination   field is a repeated <code>STRUCT&lt;key, value&gt;</code></li> <li>Transform complex types to JSON strings when the destination field   in BigQuery expects a string</li> </ul>"},{"location":"architecture/bigquery_sink_specification/#accumulate-unknown-values-as-additional_properties","title":"Accumulate Unknown Values As <code>additional_properties</code>","text":"<p>Accumulate values that are not present in the destination BigQuery table schema and inject as a JSON string into the payload as <code>additional_properties</code>. This should make it possible to backfill a new column by using JSON operators in the case that a new field was added to a ping in the client before being added to the relevant JSON schema.</p> <p>Unexpected fields should never cause the message to fail insertion.</p>"},{"location":"architecture/bigquery_sink_specification/#errors","title":"Errors","text":"<p>Send all messages that trigger an error described below to the error output.</p> <p>Handle any exceptions when routing and decoding messages by returning them in a separate <code>PCollection</code>. We detect messages that are too large to send to BigQuery and route them to error output by raising a <code>PayloadTooLarge</code> exception.</p> <p>Errors when writing to BigQuery via streaming inserts are returned as a <code>PCollection</code> via the <code>getFailedInserts</code> method. Use <code>InsertRetryPolicy.retryTransientErrors</code> when writing to BigQuery so that retries are handled automatically and all errors returned are non-transient.</p>"},{"location":"architecture/bigquery_sink_specification/#error-message-schema","title":"Error Message Schema","text":"<p>Always include the error attributes specified in the Decoded Error Message Schema.</p> <p>Encode errors received as type <code>TableRow</code> as JSON in the payload of a <code>PubsubMessage</code>, and add error attributes.</p> <p>Do not modify errors received as type <code>PubsubMessage</code> except to add error attributes.</p>"},{"location":"architecture/bigquery_sink_specification/#other-considerations","title":"Other Considerations","text":""},{"location":"architecture/bigquery_sink_specification/#message-acks","title":"Message Acks","text":"<p>Acknowledge messages in the PubSub topic subscription only after successful delivery to an output. Only deliver messages to a single output.</p>"},{"location":"architecture/decoder_service_specification/","title":"Decoder Service Specification","text":"<p>This document specifies the behavior of the service that decodes messages in the Structured Ingestion pipeline.</p>"},{"location":"architecture/decoder_service_specification/#data-flow","title":"Data Flow","text":"<ol> <li>Consume messages from Google Cloud PubSub raw topic</li> <li>Deduplicate message by <code>uri</code> (which generally contains <code>docId</code>)</li> <li>Disabled for stub-installer, which does not include a UUID in the URI</li> <li>Perform GeoIP lookup and drop <code>x_forwarded_for</code> and <code>remote_addr</code> and    optionally <code>geo_city</code> based on population</li> <li>Parse the <code>uri</code> attribute to determine document type, etc.</li> <li>Decode the body from base64, optionally decompress, and parse as JSON</li> <li>Scrub the message, checking the content against a list of known signatures    that should cause the message to be dropped as toxic, sent to error output,    or to have specific fields redacted</li> <li>Validate the schema of the body</li> <li>Extract user agent information and drop <code>user_agent</code></li> <li>Sanitize metadata based on per-document type configuration</li> <li>Add metadata fields to message</li> <li>Write message to PubSub decoded topic based on <code>namespace</code> and <code>docType</code></li> </ol>"},{"location":"architecture/decoder_service_specification/#implementation","title":"Implementation","text":"<p>The above steps will be executed as a single Apache Beam job that can accept either a streaming input from PubSub or a batch input from Cloud Storage.</p>"},{"location":"architecture/decoder_service_specification/#decoding-errors","title":"Decoding Errors","text":"<p>All messages that are rejected at any step of the Data Flow above will be forwarded to a PubSub error topic for backfill and monitoring purposes. If we determine that a message has already been successfully processed based on <code>docId</code>, we drop the duplicated body and publish just the metadata to the error topic.</p>"},{"location":"architecture/decoder_service_specification/#error-message-schema","title":"Error message schema","text":"<p>The message that failed decoding, with several additional attributes:</p> <pre><code>...\nrequired group attributes {\n  ...\n  required string error_type    // example: \"schema\"\n  required string error_message // example: \"message did not match json schema for &lt;namespace&gt;/&lt;docVersion&gt;/&lt;docType&gt;\"\n  required string exception_class // example: \"java.lang.RuntimeException\"\n  required string stack_trace\n  optional string stack_trace_cause_1\n  optional string stack_trace_cause_2\n  optional string stack_trace_cause_3\n  optional string stack_trace_cause_4\n  optional string stack_trace_cause_5\n}\n</code></pre>"},{"location":"architecture/decoder_service_specification/#raw-message-schema","title":"Raw message schema","text":"<p>See Edge Service PubSub Message Schema.</p>"},{"location":"architecture/decoder_service_specification/#decoded-message-metadata-schema","title":"Decoded message metadata schema","text":"<p>Decoded messages published to Pub/Sub will contain the following attributes:</p> <pre><code>required group attributes {\n  ...\n  required string document_version           // from uri for non-Telemetry, from message for Telemetry\n  required string document_id                // from uri\n  required string document_namespace         // from uri\n  required string document_type              // from uri\n  optional string app_name                   // from uri for Telemetry\n  optional string app_version                // from uri for Telemetry\n  optional string app_update_channel         // from uri for Telemetry\n  optional string app_build_id               // from uri for Telemetry\n  optional string geo_country                // from geoip lookup\n  optional string geo_subdivision1           // from geoip lookup\n  optional string geo_subdivision2           // from geoip lookup\n  optional string geo_city                   // from geoip lookup\n  required string submission_timestamp       // from edge metadata\n  optional string date                       // header from client\n  optional string dnt                        // header from client\n  optional string x_pingsender_version       // header from client\n  optional string x_debug_id                 // header from client\n  optional string x_foxsec_ip_reputation     // header from iprepd\n  optional string x_lb_tags                  // header from load balancer\n  optional string x_source_tags              // header from client\n  optional string x_telemetry_agent          // header from client\n  optional string user_agent_browser         // from user_agent\n  optional string user_agent_browser_version // from user_agent\n  optional string user_agent_os              // from user_agent\n  optional string user_agent_os_version      // from user_agent\n  optional string normalized_app_name        // based on parsed json payload\n  optional string normalized_channel         // based on parsed json payload or URI\n  required string normalized_country_code    // from geoip lookup\n  optional string normalized_os              // based on parsed json payload\n  optional string normalized_os_version      // based on parsed json payload\n  optional string sample_id                  // based on parsed json payload\n}\n</code></pre> <p>Many of these fields are also injected into the JSON payload either at the top level or nested inside a <code>metadata</code> object. The schema for injected metadata is maintained under the <code>metadata</code> namespace in <code>mozilla-pipeline-schemas</code>.</p>"},{"location":"architecture/decoder_service_specification/#other-considerations","title":"Other Considerations","text":""},{"location":"architecture/decoder_service_specification/#message-acks","title":"Message Acks","text":"<p>Messages should only be acknowledged in the PubSub raw topic subscription after delivery to either a decoded topic or the error topic.</p> <p>If this is not possible then any time a message is not successfully delivered to PubSub it should by treated as lost data and the appropriate time window will be backfilled from Cloud Storage in batch mode, and appropriate steps will be taken downstream to handle the backfill.</p> <p>Deployments should always terminate functional pipelines using the <code>drain</code> method, to ensure ack'd messages are fully delivered.</p>"},{"location":"architecture/decoder_service_specification/#deduplication","title":"Deduplication","text":"<p>Each <code>uri</code> will be allowed through \"at least once\", and only be rejected as a duplicate if we have completed delivery of a message with the same <code>uri</code>. We assume that each <code>uri</code> contains a UUID that uniquely identifies the document. \"Exactly once\" semantics can be applied to derived data sets using SQL in BigQuery, and GroupByKey in Beam and Spark.</p> <p>Note that deduplication is only provided with a \"best effort\" quality of service using a 10 minute window.</p>"},{"location":"architecture/differences_from_aws/","title":"Differences from AWS","text":"<p>This document explains how GCP Ingestion differs from the AWS Data Platform Architecture.</p>"},{"location":"architecture/differences_from_aws/#replace-heka-framed-protobuf-with-newline-delimited-json","title":"Replace Heka Framed Protobuf with newline delimited JSON","text":"<p>Heka framed protobuf requires special code to read and write. Newline delimited JSON is readable by BigQuery, Dataflow, and Spark using standard libraries. JSON doesn't enforce a schema, so it can be used to store data with an incomplete schema and be used to backfill missing columns.</p>"},{"location":"architecture/differences_from_aws/#replace-ec2-edge-with-kubernetes-edge","title":"Replace EC2 Edge with Kubernetes Edge","text":"<p>The AWS data platform uses EC2 instances running an NGinX module to encode HTTP requests as Heka messages and then write them to Kafka using <code>librdkafka</code> and directly to files on disk. <code>librdkafka</code> handles buffering and batching when writing to Kafka. Files on disk are rotated with cron and uploaded to S3. On shutdown files are forcefully rotated and uploaded. The sizing of the EC2 instance cluster is effectively static, but is configured to scale up if needed.</p> <p>The EC2 instances have been replaced with a Kubernetes cluster. This decision was made by the Cloud Operations team to simplify operational support for them.</p> <p>The NGinX module has been replaced by an HTTP service running in Docker. A number of factors informed the decision to rewrite the edge, including:</p> <ul> <li>The PubSub equivalent of <code>librdkafka</code> is the google client   libraries,   which do not have a C implementation</li> <li>We can simplify the edge by uploading to landfill after PubSub while   remaining resilient to Dataflow and PubSub failures, because PubSub durably   stores unacknowledged messages for 7 days</li> <li>We can simplify disaster recovery by Ensuring that all data eventually flows   through PubSub</li> <li>In the AWS edge data only flows to at least one of Kafka or landfill</li> <li>We can allow Kubernetes to auto scale when PubSub is available by only   queuing requests to disk only when they cannot be delivered to PubSub</li> <li>We can ensure that data is not lost on shutdown by disabling auto scaling   down when there are requests on disk</li> </ul>"},{"location":"architecture/differences_from_aws/#replace-kafka-with-pubsub","title":"Replace Kafka with PubSub","text":"<p>Comparison:</p> Kafka in AWS Data Pipeline PubSub Managed by Ops Google Access control Security groups, all-or-nothing Cloud IAM, per-topic Scaling Manual Automatic Cost Per EC2 instance Per GB, min charge 1 KB/req Data Storage Configured in GB per EC2 instance 7 days for unacknowledged Cross region no yes"},{"location":"architecture/differences_from_aws/#replace-hindsight-data-warehouse-loaders-with-dataflow","title":"Replace Hindsight Data Warehouse Loaders with Dataflow","text":"<p>Dataflow advantages:</p> <ul> <li>Connectors for PubSub, Cloud Storage, and BigQuery built-in</li> <li>Seamlessly supports streaming and batch sources and sinks</li> <li>Runs on managed service and has simple local runner for testing and   development</li> <li>Auto scales on input volume</li> </ul>"},{"location":"architecture/differences_from_aws/#replace-s3-with-cloud-storage","title":"Replace S3 with Cloud Storage","text":"<p>They are equivalent products for the different cloud vendors.</p>"},{"location":"architecture/differences_from_aws/#messages-always-delivered-to-message-queue","title":"Messages Always Delivered to Message Queue","text":"<p>In AWS the edge aimed to ensure messages were always delivered to either Kafka or landfill, and in the case of an outage one could be backfilled from the other. On GCP the Kubernetes edge aims to ensure messages are always delivered to PubSub. This ensures that consumers from PubSub never miss data unless they fall too far behind. It also allows landfill to be handled downstream from PubSub (see below).</p>"},{"location":"architecture/differences_from_aws/#landfill-is-downstream-from-message-queue","title":"Landfill is Downstream from Message Queue","text":"<p>In AWS, the failsafe data store was upstream of the message queue (Kafka). On GCP, the failsafe data store is downstream from the message queue (PubSub).</p> <p>This makes the edge and Dataflow landfill loader simpler. The edge doesn\u2019t have to ensure that pending messages are safely offloaded on shutdown, because messages are only left pending when PubSub is unavailable, and scaling down is disabled while messages are pending. The Dataflow landfill loader doesn\u2019t have to ensure pending messages are safely offloaded because it only acks messages after they are uploaded, ensuring at least once delivery.</p> <p>This design is possible because of two changes compared to our AWS implementation. First, the Kubernetes edge eventually delivers all messages to PubSub. In AWS if Kafka were down then messages would only be delivered directly to landfill, and would never flow through Kafka. This change ensures that if all messages are consumed from PubSub then no messages have been skipped. Second, PubSub stores unacknowledged messages for 7 days. In AWS Kafka stores messages for 2-3 days, depending on topic and total message volume. This change ensures that we have sufficient time to reliably consume all messages before they are dropped from the queue, even if total message volume changes dramatically or consumers are not highly available and suffer an outage over a holiday weekend.</p>"},{"location":"architecture/edge_migration_plan/","title":"Plans for migrating edge traffic to GCP Ingestion","text":"<p>This document outlines plans to migrate edge traffic from AWS to GCP using the code in this repository.</p>"},{"location":"architecture/edge_migration_plan/#current-state","title":"Current state","text":"<p>Today, data producers send data to the ingestion stack on AWS as described here.</p>"},{"location":"architecture/edge_migration_plan/#phase-1","title":"Phase 1","text":"<p>Timeline: Q4 2018</p> <pre><code>Data Producers -&gt; AWS Edge -&gt; Kafka -&gt; AWS Hindsight -&gt; PubSub -&gt; GCP Hindsight w/ HTTP Output -&gt; GCP Edge\n</code></pre> <p>In this configuration we get 100% of \"expected\" data going to the GCP pipeline without any risk of affecting production AWS processing.</p> <p>This \"expected\" data does not include recording and replaying traffic that we currently throw away at the edge (e.g. data from Firefox versions prior to unified telemetry). There may be some other subtle differences from the original incoming data from producers, such as missing some headers that are not currently being stored in Landfill. On the whole, this is a good approximation of 100% of the data we actually care about. We also need to test operation of the new system while processing data we don't care about; see more detail in Phase 3 below.</p> <p>During this phase, we will collect logging info to determine exactly what data is being ignored by limiting to this \"expected\" data.</p> <p>Why have a GCP PubSub topic instead of running an HTTP output from the AWS consumer directly? The main reason is that we want 100% of data in a PubSub topic anyway, for staging purposes. This way we can have a production GCP Edge ingestion stack writing to production GCP resources, while being able to e.g. simulate load and various outage conditions using the same data in a staging environment. We could in theory do the stage testing using the prod GCP edge output topic, assuming the edge has no issues. This will be the eventual end state of the system when there's no more AWS, but we are currently reusing most of the hindsight tooling for stage testing since it's already written and working in production.</p>"},{"location":"architecture/edge_migration_plan/#phase-2","title":"Phase 2","text":"<p>Timeline: possibly Q4 2018</p> <pre><code>Data Producers -&gt; AWS Edge -&gt; Kafka -&gt; AWS Hindsight w/ HTTP Output -&gt; GCP Edge\n</code></pre> <p>We continue to write the Hindsight PubSub topic from Phase 1, but we move the HTTP output to the AWS side. This will help us better empirically determine performance and various other implications of cross-cloud requests without potentially affecting production AWS processing. We can still use the GCP PubSub topic for stage testing, but it won't necessarily be actively used and the production GCP Edge will be receiving its data directly from AWS via HTTP POST.</p>"},{"location":"architecture/edge_migration_plan/#phase-3","title":"Phase 3","text":"<p>Timeline: 2019</p> <pre><code>Data Producers -&gt; AWS Tee -&gt; AWS Edge\n                          \\\n                           `-&gt; GCP Edge\n</code></pre> <p>This is how we did the last major migration from <code>Heka</code> to <code>Hindsight</code>.</p> <p>This architecture introduces risk of data loss, so should be considered more dangerous than previous phases. This is why we are not planning on doing this until we're reasonably confident in the efficacy of the GCP infrastructure. In active tee mode, the client will be affected by GCP edge processing, particularly in request processing time and potentially by its status code. Depending on how we configure the tee, the AWS data ingestion infrastructure is susceptible to data duplication or loss due to client retry behavior.</p> <p>We should ensure that we are sufficiently confident in the behavior, performance, and stability of the GCP Edge before we move to this phase to ensure things don't go south. The previous phases are safer and should let us discover any major issues before we proceed to this phase.</p> <p>This does the \"last mile\" testing of most of the major missing pieces from earlier phases, in addition to prepping for the eventual fourth phase.</p>"},{"location":"architecture/edge_migration_plan/#phase-3-alternative","title":"Phase 3 (alternative)","text":"<pre><code>Data Producers -&gt; Weighted DNS -&gt; AWS Edge\n                               \\\n                                `-&gt; GCP edge\n</code></pre> <p>This alternative does not make use of a Tee to duplicate traffic.</p> <p>Depending on the results of Phase 2, an alternative strategy to teeing would be weighted DNS and running two GCP edges. In this configuration we could e.g. tee 1% of data directly to one of the GCP edges, while having the other 99% be processed by the other edge as per earlier phases. We would then need to do the reverse of Phase 1 and write that 1% back to AWS + Kafka for the AWS ingestion stack to process. This strategy can be just as dangerous as using the <code>OpenResty</code> tee because data loss on either side may result in partial data to both, and also requires writing code to convert the GCP representation back to the AWS representation. The risk can be mitigated by using the DNS weights to adjust the amount of data being sent directly to GCP, which is an advantage. This is an interesting variation more similar to a standard DNS cut-over, if required.</p>"},{"location":"architecture/edge_migration_plan/#phase-4","title":"Phase 4","text":"<p>Timeline: To Be Determined</p> <pre><code>Data Producers -&gt; GCP Edge\n</code></pre> <p>This will happen after we are confident that there is no risk of data loss by switching the endpoint to the GCP stack. It will also depend on the logistics and timing of sunsetting systems and components in AWS.</p>"},{"location":"architecture/edge_service_specification/","title":"Edge Service Specification","text":"<p>This document specifies the behavior of the server that accepts submissions from HTTP clients e.g. Firefox telemetry.</p>"},{"location":"architecture/edge_service_specification/#general-data-flow","title":"General Data Flow","text":"<p>HTTP submissions come in from the wild, hit a load balancer, then optionally an nginx proxy, then the HTTP edge server described in this document. Data is accepted via a POST/PUT request from clients, which the server will wrap in a PubSub message and forward to Google Cloud PubSub, where any further processing, analysis, and storage will be handled.</p>"},{"location":"architecture/edge_service_specification/#namespaces","title":"Namespaces","text":"<p>Namespaces are used to control the processing of data from different types of clients, from the metadata that is collected to the destinations where the data is written, processed and accessible. Data sent to a namespace that is not specifically configured is assumed to be in the non-Telemetry JSON format described here. To request a new namespace configuration file a bug against the Data Platform Team with a short description of what the namespace will be used for and the desired configuration options.</p>"},{"location":"architecture/edge_service_specification/#forwarding-to-the-pipeline","title":"Forwarding to the pipeline","text":"<p>The message is written to PubSub. If the message cannot be written to PubSub it is written to a disk queue that will periodically retry writing to PubSub.</p>"},{"location":"architecture/edge_service_specification/#pubsub-message-schema","title":"PubSub Message Schema","text":"<pre><code>required string data                   // base64 encoded body\nrequired group attributes {\n  required string submission_timestamp // server time, ISO 8601 with microseconds and timezone \"Z\", example: \"2018-03-12T21:02:18.123456Z\"\n  required string uri                  // example: \"/submit/telemetry/6c49ec73-4350-45a0-9c8a-6c8f5aded0cf/main/Firefox/58.0.2/release/20180206200532\"\n  required string protocol             // example: \"HTTP/1.1\"\n  required string method               // example: \"POST\"\n  optional string args                 // query parameters, example: \"v=4\"\n  // Headers\n  optional string remote_addr          // usually a load balancer, example: \"172.31.32.5\"\n  optional string content_length       // example: \"4722\"\n  optional string date                 // example: \"Mon, 12 Mar 2018 21:02:18 GMT\"\n  optional string dnt                  // example: \"1\"\n  optional string host                 // example: \"incoming.telemetry.mozilla.org\"\n  optional string user_agent           // example: \"pingsender/1.0\"\n  optional string x_forwarded_for      // example: \"10.98.132.74, 103.3.237.12\"\n  optional string x_pingsender_version // example: \"1.0\"\n  optional string x_debug_id           // example: \"my_debug_session_1\"\n  optional string x_pipeline_proxy     // time that the AWS-&gt;GCP tee received the message, ignored, example: \"2018-03-12T21:02:18.123456Z\"\n  optional string x_telemetry_agent    // example: \"Glean/0.40.0 (Kotlin on Android)\"\n  optional string x_source_tags        // example: \"automation, other\"\n  optional string x_foxsec_ip_reputation // example: \"95\"\n  optional string x_lb_tags            // example: \"TLSv1.3, 009C\"\n}\n</code></pre>"},{"location":"architecture/edge_service_specification/#server-requestresponse","title":"Server Request/Response","text":""},{"location":"architecture/edge_service_specification/#get-request","title":"GET Request","text":"Endpoint Description <code>/__heartbeat__</code> check if service is healthy, and can reach PubSub or has space to store requests on disk <code>/__lbheartbeat__</code> check if service is running <code>/__version__</code> return Dockerflow version object"},{"location":"architecture/edge_service_specification/#get-response-codes","title":"GET Response codes","text":"<ul> <li>200 - ok, check succeeded</li> <li>204 - ok, check succeeded, no response body</li> <li>404 - not found, check doesn't exist</li> <li>500 - all is not well</li> <li>507 - insufficient storage, should occur at some configurable limit before disk is full</li> </ul>"},{"location":"architecture/edge_service_specification/#postput-request","title":"POST/PUT Request","text":"<p>Treat POST and PUT the same. Accept POST or PUT to URLs of the form</p> <p><code>^/submit/namespace/[/dimensions]$</code></p> <p>Example Telemetry format:</p> <p><code>/submit/telemetry/docId/docType/appName/appVersion/appUpdateChannel/appBuildID</code></p> <p>Specific Telemetry example:</p> <p><code>/submit/telemetry/ce39b608-f595-4c69-b6a6-f7a436604648/main/Firefox/61.0a1/nightly/20180328030202</code></p> <p>Example non-Telemetry format:</p> <p><code>/submit/namespace/docType/docVersion/docId</code></p> <p>Specific non-Telemetry example:</p> <p><code>/submit/eng-workflow/hgpush/1/2c3a0767-d84a-4d02-8a92-fa54a3376049</code></p> <p>Note that <code>docId</code> above is a unique document ID, which is used for de-duping submissions. This is not intended to be the <code>clientId</code> field from Telemetry. <code>docId</code> is required and must be a UUID.</p>"},{"location":"architecture/edge_service_specification/#legacy-systems","title":"Legacy Systems","text":"<p>Accept TLS Error Reports as POST or PUT to <code>/submit/sslreports</code> with no <code>docType</code>, <code>docVersion</code>, or <code>docId</code>.</p> <p>Accept Stub Installer pings as GET to <code>/stub/[docVersion]/[dimensions]</code>, with no <code>docType</code> or <code>docId</code>, and over both HTTP and HTTPS. Use POST/PUT Response codes, even though this endpoint is for GET requests.</p>"},{"location":"architecture/edge_service_specification/#postput-response-codes","title":"POST/PUT Response codes","text":"<ul> <li>200 - ok, request accepted into the pipeline</li> <li>400 - bad request, for example an unencoded space in the URL</li> <li>404 - not found, for example using a telemetry format URL in a non-telemetry namespace or vice-versa</li> <li>411 - missing content-length header</li> <li>413 - request body too large (note that if we have badly-behaved clients that retry on <code>4XX</code>, we should send back 202 on body/path too long).</li> <li>414 - request path too long (see above)</li> <li>500 - internal error</li> <li>507 - insufficient storage, request failed because disk is full</li> </ul>"},{"location":"architecture/edge_service_specification/#other-response-codes","title":"Other Response codes","text":"<ul> <li>405 - wrong request type (anything other than GET|POST|PUT)</li> </ul>"},{"location":"architecture/edge_service_specification/#other-considerations","title":"Other Considerations","text":""},{"location":"architecture/edge_service_specification/#compression","title":"Compression","text":"<p>It is not desirable to do decompression on the edge node. We want to pass along messages from the HTTP Edge node without \"cracking the egg\" of the payload.</p> <p>We may also receive badly formed payloads, and we will want to track the incidence of such things.</p>"},{"location":"architecture/edge_service_specification/#bad-messages","title":"Bad Messages","text":"<p>Since the actual message is not examined by the edge server the only failures that occur are defined by the response status codes above. Messages are only forwarded to the pipeline when a response code of 200 is returned to the client.</p>"},{"location":"architecture/edge_service_specification/#pubsub-topics","title":"PubSub Topics","text":"<p>All messages that sent a response code of 200 are forwarded to a single PubSub topic for decoding and landfill.</p>"},{"location":"architecture/edge_service_specification/#geoip-lookups","title":"GeoIP Lookups","text":"<p>No GeoIP lookup is performed by the edge server. If a client IP is available then the PubSub consumer performs the lookup and then discards the IP before the message is forwarded to a decoded PubSub topic.</p>"},{"location":"architecture/edge_service_specification/#data-retention","title":"Data Retention","text":"<p>The edge server only stores data when PubSub cannot be reached, and removes data after it is successfully written to PubSub. Down scaling will be disabled for the Kubernetes pod and cluster when data is being stored, so that data is not lost.</p>"},{"location":"architecture/edge_service_specification/#submission-timestamp-format","title":"Submission Timestamp Format","text":"<p><code>submission_timestamp</code> is formatted as ISO 8601 with microseconds and timezone, because it is compatible with BigQuery's Timestamp Type, so that the field doesn't need transformation.</p>"},{"location":"architecture/landfill_service_specification/","title":"Raw Sink Service Specification","text":"<p>This document specifies the behavior of the service that batches raw messages into long term storage.</p>"},{"location":"architecture/landfill_service_specification/#data-flow","title":"Data Flow","text":"<p>Consume messages from a Google Cloud PubSub topic and write in batches to BigQuery. Split batches based by time windows based on when they were retrieved from PubSub. Additionally, split batches when they reach a certain size, if possible.</p>"},{"location":"architecture/landfill_service_specification/#implementation","title":"Implementation","text":"<p>Execute this as a custom Java application running on GKE.</p>"},{"location":"architecture/landfill_service_specification/#latency","title":"Latency","text":"<p>Accept a configuration for batch window size. Deliver batches to BigQuery within 5 minutes of the batch window closing.</p>"},{"location":"architecture/landfill_service_specification/#other-considerations","title":"Other Considerations","text":""},{"location":"architecture/landfill_service_specification/#message-acks","title":"Message Acks","text":"<p>Only acknowledge messages in the PubSub topic subscription after delivery to BigQuery.</p>"},{"location":"architecture/overview/","title":"GCP Ingestion Architecture","text":"<p>This document specifies the architecture for GCP Ingestion as a whole.</p>"},{"location":"architecture/overview/#architecture-diagram","title":"Architecture Diagram","text":"<ul> <li>The Kubernetes <code>Ingestion Edge</code> sends messages from <code>Producers</code> (e.g.   Firefox) to a set of PubSub <code>Raw Topics</code>, routing messages based on <code>uri</code></li> <li><code>Raw Topics</code> are the first layer of a \"pipeline family\"; the diagram shows   only the \"structured\" pipeline family, but there are also deployments   for \"telemetry\", \"stub-installer\", and \"pioneer\"</li> <li>The <code>Raw Sink</code> job copies messages from a PubSub <code>Raw Topic</code> to   <code>BigQuery</code></li> <li>The Dataflow <code>Decoder</code> job decodes messages from the PubSub <code>Raw Topic</code> to   the PubSub <code>Decoded Topic</code></li> <li>The Dataflow <code>Republisher</code> job reads messages from the PubSub <code>Decoded Topic</code>   and republishes them to various   lower volume derived topics including <code>Monitoring Sample Topics</code> and   <code>Per DocType Topics</code></li> <li>The Kubernetes <code>Decoded Sink</code> job copies messages from the PubSub <code>Decoded Topic</code>   to <code>BigQuery</code> with the payload encoded as JSON</li> <li>The Kubernetes <code>Live Sink</code> job copies messages from the PubSub <code>Decoded Topic</code>   to <code>BigQuery</code> with the payload structure parsed out to individual fields</li> </ul>"},{"location":"architecture/overview/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/overview/#ingestion-edge","title":"Ingestion Edge","text":"<ul> <li>Must send messages from producers to PubSub topics</li> <li>Must store messages on disk when PubSub is unavailable</li> <li>Must attempt to deliver all new messages to PubSub before storing on disk</li> <li>Must not be scaled down when there are messages on disk</li> <li>Must respond server error if PubSub and disk are both unavailable</li> <li>Must use a 5XX error error code</li> <li>Must accept configuration mapping <code>uri</code> to PubSub Topic</li> <li>Expected initial topics are Structured Ingestion, Telemetry, and Pioneer</li> <li>Must accept configuration defining HTTP headers to capture</li> </ul>"},{"location":"architecture/overview/#raw-sink","title":"Raw Sink","text":"<ul> <li>Must copy messages from PubSub topics to BigQuery</li> <li>This copy may be for backfill, recovery, or testing</li> <li>Must not ack messages read from PubSub until they are delivered</li> <li>Must accept configuration mapping PubSub topics to BigQuery tables</li> <li>Should retry transient Cloud Storage errors indefinitely</li> <li>Should use exponential back-off to determine retry timing</li> </ul>"},{"location":"architecture/overview/#decoder","title":"Decoder","text":"<ul> <li>Must decode messages from PubSub topics to PubSub topics</li> <li>Must not ack messages read from PubSub until they are delivered</li> <li>Must apply the following transforms in order   (implementations here):</li> <li>Resolve GeoIP from <code>remote_addr</code> or <code>x_forwarded_for</code> attribute into      <code>geo_*</code> attributes</li> <li>Parse <code>uri</code> attribute into multiple attributes</li> <li>Gzip decompress <code>payload</code> if gzip compressed</li> <li>Validate <code>payload</code> using a JSON Schema determined by attributes</li> <li>Parse <code>agent</code> attribute into <code>user_agent_*</code> attributes</li> <li>Produce <code>normalized_</code> variants of select attributes</li> <li>Sanitize attributes, configurable per document type via schema metadata</li> <li>Inject <code>normalized_</code> attributes at the top level and other select      attributes into a nested <code>metadata</code> top level key in <code>payload</code></li> <li>Should deduplicate messages based on the <code>uri</code> attribute</li> <li>Must ensure at least once delivery, so deduplication is only \"best effort\"</li> <li>Must send messages rejected by transforms to a configurable error destination</li> <li>Must allow error destination in BigQuery</li> </ul>"},{"location":"architecture/overview/#republisher","title":"Republisher","text":"<ul> <li>Must copy messages from PubSub topics to PubSub topics</li> <li>Must ack messages read from PubSub after they are delivered to all   matching destinations</li> <li>Must not ack messages read from PubSub before they are delivered to all   matching destinations</li> <li>Must accept configuration enabling republishing of messages to a debug   topic if they contain an <code>x_debug_id</code> attribute</li> <li>Must accept configuration enabling or disabling debug republishing</li> <li>Must accept configuration for the destination topic</li> <li>Must accept configuration enabling republishing of a random sample of the   input stream</li> <li>Must accept configuration for the sample ratio</li> <li>Must accept configuration for the destination topic</li> <li>Must accept configuration mapping <code>document_type</code>s to PubSub topics</li> <li>Must accept configuration for the destination topic pattern</li> <li>Must accept configuration for which <code>document_type</code>s to republish</li> <li>Must only deliver messages with configured destinations</li> <li>Must accept configuration mapping <code>document_namespace</code>s to PubSub topics</li> <li>Must accept configuration for a map from <code>document_namespace</code>s to topics</li> <li>Must only deliver messages with configured destinations</li> <li>Must accept optional configuration for sampling telemetry data</li> <li>Must accept configuration for the destination topic pattern</li> <li>Must accept configuration for the sampling ratio for each channel (nightly,     beta, and release)</li> </ul>"},{"location":"architecture/overview/#live-sink","title":"Live Sink","text":"<ul> <li>Must copy messages from PubSub topics to BigQuery</li> <li>Must not ack messages read from PubSub until they are delivered</li> <li>Must accept configuration mapping PubSub topics to BigQuery tables</li> <li>Must accept configuration for using streaming or batch loads</li> <li>Must transform all field names to lowercase with underscores (<code>snake_case</code>)   and perform other field name cleaning to match the transformations   expected by the <code>jsonschema-transpiler</code></li> <li>Must set <code>ignoreUnknownValues</code>   to <code>true</code></li> <li>Should retry transient BigQuery errors indefinitely</li> <li>Should use exponential back-off to determine retry timing</li> <li>Must send messages rejected by BigQuery to a configurable error destination</li> <li>Must allow error destinations in BigQuery</li> </ul>"},{"location":"architecture/overview/#decoded-sink","title":"Decoded Sink","text":"<ul> <li>Must copy messages from PubSub topics to BigQuery</li> <li>May be used to backfill BigQuery columns previously unspecified in the     table schema</li> <li>May be used by BigQuery, Spark, and Dataflow to access columns missing     from BigQuery Tables</li> <li>Must not ack messages read from PubSub until they are delivered</li> <li>Must accept configuration mapping PubSub topics to BigQuery tables</li> <li>Should retry transient BigQuery errors indefinitely</li> <li>Should use exponential back-off to determine retry timing</li> </ul>"},{"location":"architecture/overview/#notes","title":"Notes","text":"<p>PubSub stores unacknowledged messages for 7 days. Any PubSub subscription more than 7 days behind requires a backfill.</p> <p>Dataflow will extend ack deadlines indefinitely when consuming messages, and will not ack messages until they are processed by an output or <code>GroupByKey</code> transform.</p> <p>Dataflow jobs achieve at least once delivery by not using GroupByKey transforms and not falling more than 7 days behind in processing.</p>"},{"location":"architecture/overview/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/overview/#kubernetes-engine-and-pubsub","title":"Kubernetes Engine and PubSub","text":"<p>Kubernetes Engine is a scalable, managed service based on an industry standard. PubSub is a simple, scalable, managed service. By comparison a compute instance group instead of Kubernetes Engine and Kafka instead of PubSub would require more operational overhead and engineering effort for maintenance.</p>"},{"location":"architecture/overview/#different-topics-for-raw-and-validated-data","title":"Different topics for \"raw\" and \"validated\" data","text":"<p>We don't want to have to repeat the validation logic in the case where we have multiple consumers of the data. Raw data can be sent to a single topic to simplify the edge service and then validated data can be sent to topics split by <code>docType</code> and other attributes, in order to allow consumers for specific sets of data.</p>"},{"location":"architecture/overview/#bigquery","title":"BigQuery","text":"<p>BigQuery provides a simple, scalable, managed service for executing SQL queries over arbitrarily large or small amounts of data, with built-in schema validation, hyperloglog functions, UDF support, and destination tables (sometimes called materialized views) for minimizing cost and latency of derived tables. Alternatives (such as Presto) would have more operational overhead and engineering effort for maintenance, while generally being less featureful.</p>"},{"location":"architecture/overview/#archive-messages-from-each-stage-of-the-pipeline-as-json-payloads-in-bigquery","title":"Archive messages from each stage of the pipeline as JSON payloads in BigQuery","text":"<p>One of the primary challenges of building a real-world data pipeline is anticipating and adapting to changes in the schemas of messages flowing through the system. Strong schemas and structured data give us many usability and performance benefits, but changes to the schema at one point in the pipeline can lead to processing errors or dropped data further down the pipeline.</p> <p>Saving JSON messages as compressed bytes fields in BigQuery tables allows use to gracefully handle new fields added upstream without needing to specify those fields completely before they are stored. New columns can be added to a table's schema and then restored via a backfill operation.</p>"},{"location":"architecture/overview/#use-destination-tables","title":"Use destination tables","text":"<p>For complex queries that are calculated over time-based windows of data, using destination tables allows us to save time and cost by only querying each new window of data once.</p>"},{"location":"architecture/overview/#use-views-for-user-facing-data","title":"Use views for user-facing data","text":"<p>Views we create in BigQuery can be a stable interface for users while we potentially change versions or implementations of a pipeline behind the scenes. If we wanted to rewrite a materialized view, for example, we might run the new and old definitions in parallel, writing to separate tables; when we\u2019re comfortable that the new implementation is stable, we could cut users over to the new implementation by simply changing the definition of the user-facing view.</p>"},{"location":"architecture/overview/#limits","title":"Limits","text":"<ul> <li>The maximum <code>Content-Length</code> accepted at the edge is 1 MB; larger payloads   will be dropped and the request will return a 413 response code</li> <li>The maximum payload size after being decompressed in the Decoder is 8 MB;   larger payloads will trigger a <code>PayloadTooLarge</code> exception and be sent to   error output</li> <li>Hard limit of 10,000 columns per table in BigQuery (see Load job limits)</li> <li>Max of 1,000,000 streaming inserts per second per BigQuery table, lower if we populate <code>insertId</code> (see Streaming insert limits)</li> <li>A PubSub topic without any subscriptions drops all messages until a subscription is created</li> <li>API Rate Limit: 20 req/sec</li> </ul>"},{"location":"architecture/overview/#further-reading","title":"Further Reading","text":"<p>Differences from AWS</p>"},{"location":"architecture/pain_points/","title":"Pain points","text":"<p>A running list of things that are suboptimal in GCP.</p>"},{"location":"architecture/pain_points/#app-engine","title":"App Engine","text":"<p>For network-bound applications it can be prohibitively expensive. A PubSub push subscription application that decodes protobuf and forwards messages to the ingestion-edge used <code>~300</code> instances at <code>$0.06</code> per instance hour to handle <code>~5krps</code>, which is <code>~$13K/mo</code>.</p>"},{"location":"architecture/pain_points/#dataflow","title":"Dataflow","text":"<p>Replaces certain components with custom behavior that is not part of the open source Beam API, making it so they can't be extended (e.g. to expose a stream of messages that have been delivered to PubSub).</p>"},{"location":"architecture/pain_points/#bigqueryiowrite","title":"<code>BigQueryIO.Write</code>","text":"<p>Requires decoding <code>PubsubMessage.payload</code> from JSON to a <code>TableRow</code>, which gets encoded as JSON to be sent to BigQuery.</p> <p>Crashes the pipeline when the destination table does not exist.</p>"},{"location":"architecture/pain_points/#fileiowrite","title":"<code>FileIO.Write</code>","text":"<p>Acknowledges messages in PubSub before they are written to accumulate data across multiple bundles and produce reasonably sized files. Possible workaround being investigated in #380. This also effects <code>BigQueryIO.Write</code> in batch mode.</p>"},{"location":"architecture/pain_points/#pubsubiowrite","title":"<code>PubsubIO.Write</code>","text":"<p>Does not support dynamic destinations.</p> <p>Does not use standard client library.</p> <p>Does not expose an output of delivered messages, which is needed for at least once delivery with deduplication. Current workaround is to use the deduplication available via <code>PubsubIO.read()</code>.</p> <p>Uses HTTPS JSON API, which increases message payload size vs protobuf by 25% for base64 encoding and causes some messages to exceed the 10MB request size limit that otherwise would not.</p>"},{"location":"architecture/pain_points/#pubsub","title":"PubSub","text":"<p>Can be prohibitively expensive. It costs <code>~$51K/mo</code> to use PubSub with a <code>70MiB/s</code> stream published or consumed 7 times (Edge to raw topic, raw topic to Cloud Storage, raw topic to Decoder, Decoder to decoded topic, decoded topic to Decoder for deduplication, decoded topic to Cloud Storage, decoded topic to BigQuery).</p> <p>Push Subscriptions are limited to <code>min(10MB, 1000 messages)</code> in flight, making the theoretical maximum parallel latency per message ~<code>62ms</code> to achieve <code>16krps</code>.</p>"},{"location":"architecture/reliability/","title":"Reliability","text":"<p>In production the ingestion product aims to provide a Biweekly Uptime Percentage determined by the Reliability Target below. If a component does not meet that then a Stability Work Period should be assigned to each software engineer supporting the component.</p>"},{"location":"architecture/reliability/#disclaimer-and-purpose","title":"Disclaimer and Purpose","text":"<p>This document is intended solely for those directly running, writing, and managing GCP Ingestion. It is not an agreement, implicit or otherwise, with any other parties. This document is a prototype that should be treated as a goal that will require adjustments.</p> <p>The purpose of this document is first and foremost to encourage behavior that reduces Downtime. The secondary purpose is to establish clear expectations for how software engineers respond to Downtime.</p>"},{"location":"architecture/reliability/#reliability-target","title":"Reliability Target","text":"Component Biweekly Uptime Percentage ingestion-edge 99.99% ingestion-beam 99.5%"},{"location":"architecture/reliability/#definitions","title":"Definitions","text":"<p>\"Downtime\" on ingestion-edge means for at least 0.1% of requests a status code lower than 500 was not successfully returned. On ingestion-beam it means <code>oldest_unacked_message_age</code> on a PubSub input exceeds 1 hour for a batch sink job or 1 minute for a decoder or streaming sink job.</p> <p>\"Downtime Period\" means a period of 60 consecutive seconds of Downtime. Intermittent Downtime for a period of less than 60 consecutive seconds will not be counted towards any Downtime Periods.</p> <p>\"Biweekly Uptime Percentage\" means total number of minutes in a rolling two week window, minus the number of minutes of Downtime suffered from all Downtime Periods in the window, divided by the total number of minutes in the window.</p> <p>\"Stability Work\" means work on Downtime prevention and reduction. Only code changes resulting from that work may be deployed to production. All other features and work on the component are suspended.</p> <p>\"Stability Work Period\" means a continuous period of time after recovery and postmortem, where each engineer is assigned Stability Work. The length of the Stability Work Period is determined below.</p> Component Biweekly Uptime Percentage Length of Stability Work Period ingestion-edge 99.9% to &lt; 99.99% 2 weeks ingestion-edge 99% to &lt; 99.9% 4 weeks ingestion-edge &lt; 99% 12 weeks ingestion-beam 99% to &lt; 99.5% 2 weeks ingestion-beam 95% to &lt; 99% 4 weeks ingestion-beam &lt; 95% 12 weeks"},{"location":"architecture/reliability/#exclusions","title":"Exclusions","text":"<p>The Reliability Target does not apply to any Downtime in ingestion caused by Downtime on a Google Cloud Platform Service, other than Downtime on ingestion-edge caused by PubSub.</p>"},{"location":"architecture/reliability/#additional-information","title":"Additional Information","text":"Biweekly Uptime Percentage Downtime per Two Weeks 99.99% 2 minutes 99.9% 20 minutes 99.5% 1 hour 40 minutes 99% 3 hours 21 minutes 95% 16 hours 48 minutes"},{"location":"architecture/test_requirements/","title":"Test Requirements","text":"<p>This document specifies the testing required for GCP Ingestion components.</p>"},{"location":"architecture/test_requirements/#exceptions","title":"Exceptions","text":"<p>Code that does not comply with this standard before it is deployed to production must include a document explaining why that decision was made.</p>"},{"location":"architecture/test_requirements/#test-phases","title":"Test Phases","text":"<ol> <li>Continuous Integration</li> <li>Must run Unit Tests and Integration Tests</li> <li>Must run against every merge to main and every pull request</li> <li>Must block merging to main</li> <li>Pre-Production</li> <li>Must run Unit Tests and Load Tests</li> <li>Must run against every version deployed to production</li> <li>Must mimic production configuration as closely as possible</li> <li>Should block deployment to production</li> <li>Async</li> <li>Must run Slow Load Tests</li> <li>Must run against every version deployed to production</li> <li>Must mimic production configuration as closely as possible</li> <li>Should not block deployment to production</li> <li>Must notify someone to investigate failures</li> </ol>"},{"location":"architecture/test_requirements/#test-categories","title":"Test Categories","text":"<p>Tests must be both thorough and fast enough to not block development. They are split into categories with increasing run time and decreasing coverage.</p> <ol> <li>Unit Tests</li> <li>Must cover 100% of code behavior</li> <li>Should run as fast as possible</li> <li>Integration Tests</li> <li>Must cover all expected production behavior</li> <li>Must run fast enough to allow frequent merging to main</li> <li>Load Tests</li> <li>Must cover performance at scale with and without external services down</li> <li>Must run fast enough to allow multiple production deployments per day</li> <li>Slow Load Tests</li> <li>Must cover performance at scale with extended downtime</li> <li>May take a very long time</li> </ol>"},{"location":"architecture/test_requirements/#unit-tests","title":"Unit Tests","text":"<ul> <li>Must run in under 5 seconds in CI, not including build time</li> <li>May use CI parallelism and run each group of tests in under 5 seconds in CI</li> <li>Should validate code as quickly as possible</li> <li>Must run completely in memory</li> <li>Must safely run in parallel</li> <li>May mock anything that would not run in memory</li> <li>Should cover 100% of branches and statements</li> <li>Should cover all variations of statements having variable behavior without     branching</li> <li>Should cover all production configuration variations</li> <li>Must have Integration Tests to cover exceptions</li> <li>Must cover all input and output variations</li> <li>Must cover all response codes</li> <li>Must cover all valid URL patterns</li> <li>Must cover gzipped and not gzipped</li> <li>Must cover present, absent, and invalid required attributes</li> <li>Must cover present, absent, and invalid optional attributes</li> <li>Must cover present, absent, invalid, and wrong type payload JSON</li> <li>Must cover present, absent, invalid, and wrong type payload fields</li> <li>Must cover <code>document_id</code> duplicates</li> <li>Must cover all error types</li> <li>Must cover PubSub returning OK but rejecting messages in a batch</li> <li>Must cover External service returns server error</li> <li>Must cover External service timeout</li> <li>Must cover External resource missing</li> <li>Must cover Insufficient permissions</li> <li>Must cover behavior when disk becomes full</li> <li>Must cover behavior with disk already full</li> <li>Must cover behavior when ingestion-edge disk is no longer full</li> </ul>"},{"location":"architecture/test_requirements/#integration-tests","title":"Integration Tests","text":"<ul> <li>Must run in under 5 minutes in CI, not including build time</li> <li>Must be configurable to run locally and in CI</li> <li>Must be configurable to run against a deployment in GCP</li> <li>May require a proxy to modify cloud service behavior</li> <li>May require specific configuration</li> <li>May skip coverage that cannot be forcibly produced in Dataflow</li> <li>Must clean up created resources when complete</li> <li>Must cover everything unit tests should but do not</li> <li>Must cover a configuration that mimics production as closely as possible</li> <li>Must cover all code with input from or output to external services</li> <li>The ingestion-edge disk queue is an external service</li> <li>Must cover all production configuration options</li> <li>Must cover all API endpoints with invalid traffic</li> <li>Must cover all API endpoints with healthy external services</li> <li>Must assert disk queue is not used in ingestion-edge</li> <li>Must assert inputs and outputs preserve schema in ingestion-beam</li> <li>Must cover behavior with errors from external services</li> <li>Must cover PubSub returning OK but rejecting messages in a batch</li> <li>Must cover external service DNS does not resolve</li> <li>Must cover external service cannot TCP connect</li> <li>Must cover external service hangs after TCP connect</li> <li>Must cover external service returns 500</li> <li>Must cover resource does not exist in external service</li> <li>Must cover insufficient permissions in external service</li> <li>Must cover behavior when disk becomes full</li> <li>Must cover behavior with disk already full</li> <li>Must assert automatic recovery unless manual intervention is required</li> <li>Must cover behavior when ingestion-edge disk is no longer full</li> </ul>"},{"location":"architecture/test_requirements/#load-tests","title":"Load Tests","text":"<ul> <li>Must run in parallel in under 1 hour</li> <li>May require a separate deployment for each test</li> <li>Must record how much it cost to run</li> <li>Must not require manual intervention to evaluate</li> <li>Must check performance against a tunable threshold</li> <li>Must sustain at least 1.5x expected peak production traffic volume</li> <li>As of 2018-08-01 peak production traffic volume is about 11K req/s</li> <li>Must cover behavior before, during, and after a period of:</li> <li>Service downtime, except ingestion-edge</li> <li>100%, 50%, and 5% invalid traffic</li> <li>External services cannot TCP connect</li> <li>100% and 10% of external service requests time out</li> <li>100% and 10% external service requests return 500</li> <li>Must cover ingestion-edge behavior with PubSub returning 500 and auto scaling   due to disk filling up</li> <li>Must assert ingestion-edge only writes to disk queue when PubSub is unhealthy</li> <li>Must assert full recovery from downtime does not take longer than the   downtime period</li> </ul>"},{"location":"architecture/test_requirements/#slow-load-tests","title":"Slow Load Tests","text":"<ul> <li>May run for longer than 1 hour</li> <li>Should not block deployment to production</li> <li>Must cover behavior with sustained load and a backlog equivalent to:</li> <li>For ingestion-edge: 1.5x the longest PubSub outage in the last year<ul> <li>As of 2018-08-01 the longest PubSub outage was about 4.5 hours</li> </ul> </li> <li>For everything else: 4 days (1 holiday weekend)<ul> <li>Should reach full recovery in under 1 day</li> </ul> </li> <li>Must cover behavior of ingestion-beam BigQuery output with sustained load and   BigQuery returning 500 for at least 1.5x the longest BigQuery outage in the   last year</li> <li>As of 2018-08-01 the longest BigQuery outage was about 2.3 hours</li> </ul>"},{"location":"ingestion-beam/","title":"Apache Beam Jobs for Ingestion","text":"<p>This ingestion-beam java module contains our Apache Beam jobs for use in Ingestion. Google Cloud Dataflow is a Google Cloud Platform service that natively runs Apache Beam jobs.</p> <p>The source code lives in the ingestion-beam subdirectory of the gcp-ingestion repository.</p> <p>The following are the main Beam classes, please see the respective sections on them in the documentation:</p> <ul> <li>Decoder job: A job for normalizing ingestion messages</li> <li>Republisher job: A job for republishing subsets of decoded messages to new destinations</li> </ul> <p>There are a few additional jobs for special cases listed in the index for this section.</p>"},{"location":"ingestion-beam/#building","title":"Building","text":"<p>Move to the <code>ingestion-beam</code> subdirectory of your gcp-ingestion checkout and run:</p> <pre><code>./bin/mvn clean compile\n</code></pre> <p>See the details below under each job for details on how to run what you've produced.</p>"},{"location":"ingestion-beam/#testing","title":"Testing","text":"<p>Before anything else, be sure to download the test data:</p> <pre><code>./bin/download-cities15000\n./bin/download-geolite2\n./bin/download-schemas\n</code></pre> <p>Run tests locally with CircleCI Local CLI</p> <pre><code>(cd .. &amp;&amp; circleci build --job ingestion-beam)\n</code></pre> <p>To make more targeted test invocations, you can install Java and maven locally or use the <code>bin/mvn</code> executable to run maven in docker:</p> <pre><code>./bin/mvn clean test\n</code></pre> <p>If you wish to just run a single test class or a single test case, try something like this:</p> <pre><code># Run all tests in a single class\n./bin/mvn test -Dtest=com.mozilla.telemetry.util.SnakeCaseTest\n\n# Run only a single test case\n./bin/mvn test -Dtest='com.mozilla.telemetry.util.SnakeCaseTest#testSnakeCaseFormat'\n</code></pre> <p>To run the project in a sandbox against production data, see this document on configuring an integration testing workflow.</p>"},{"location":"ingestion-beam/#code-formatting","title":"Code Formatting","text":"<p>Use spotless to automatically reformat code:</p> <pre><code>mvn spotless:apply\n</code></pre> <p>or just check what changes it requires:</p> <pre><code>mvn spotless:check\n</code></pre>"},{"location":"ingestion-beam/contextual-services-job/","title":"Contextual Services Reporter Job","text":"<p>The contextual services reporter forwards contextual-services click and impression events to an external partner, with a particular eye towards minimizing the scope of contextual metadata that is shared. For more context, see the Data and Firefox Suggest blog post.</p> <p>The code is defined in the <code>com.mozilla.telemetry.ContextualServicesReporter</code> class.</p> <p>The input of this job is all <code>contextual-services</code> namespace messages for desktop Firefox, which includes <code>topsites-impression</code>, <code>topsites-click</code>, <code>quicksuggest-impression</code>, and <code>quicksuggest-click</code>. It also includes <code>topsites-impression</code> pings from various mobile applications.</p>"},{"location":"ingestion-beam/contextual-services-job/#data-flows-for-contextual-services","title":"Data flows for Contextual Services","text":"<p>Note that in addition to this near real-time Dataflow job, we have several batch workflows for preparing Contextual Services data for both internal analytic use and for sending to external partners.</p> <p>Search terms handling starts with log routing configuration in <code>cloudops-infra</code> (Mozilla internal) and then proceeds with a series of nightly scheduled queries defined under <code>search_terms_derived</code> in <code>bigquery-etl</code>. Additional aggregations for clicks and impressions are defined under <code>contextual_services_derived</code> in <code>bigquery-etl</code>.</p> <p>Some daily aggregate data is shared with an external partner via the <code>adm_export</code> DAG defined in <code>telemetry-airflow</code>.</p>"},{"location":"ingestion-beam/contextual-services-job/#beam-pipeline-transforms","title":"Beam Pipeline Transforms","text":"<p>The following diagram shows the steps in the Beam pipeline. This is up to date as of 2021-08-23.</p> <p> Figure: An overview of the execution graph for the <code>ContextualServicesReporter</code>.</p>"},{"location":"ingestion-beam/contextual-services-job/#pubsub-republished-topic","title":"Pub/Sub republished topic","text":"<p>The input to this job is the subset of decoded messages in the <code>contextual-services</code> namespace as well as <code>topsites-impression</code> pings from various namespaces associated with mobile applications sending telemetry via Glean.</p>"},{"location":"ingestion-beam/contextual-services-job/#filterbydoctype","title":"<code>FilterByDoctype</code>","text":"<p>This step filters out document types based on the <code>document_type</code> attribute using the <code>--allowedDocTypes</code> pipeline option. For example, this can be used to allow the job to process only sponsored tiles or only Suggest events, or only clicks or only impressions.</p>"},{"location":"ingestion-beam/contextual-services-job/#verifymetadata","title":"<code>VerifyMetadata</code>","text":"<p>Message contents are validated in this step using a few simple heuristics such as user agent and user agent version. Messages that fail this check are rejected and sent to the error table.</p>"},{"location":"ingestion-beam/contextual-services-job/#decompresspayload","title":"<code>DecompressPayload</code>","text":"<p>This step attempts to decompress a gzip-compressed payload. This transform is shared with other Beam pipelines such as the decoder.</p>"},{"location":"ingestion-beam/contextual-services-job/#parsereportingurl","title":"<code>ParseReportingUrl</code>","text":"<p>This is where the URLs used for reporting events are built. The <code>reporting_url</code> value from the message payload is used as the base URL, then additional query parameters are added based on the message metadata such as the client\u2019s country, region, and OS. The updated <code>reporting_url</code> value is put in the message payload and attributes.</p>"},{"location":"ingestion-beam/contextual-services-job/#labelclickspikes","title":"<code>LabelClickSpikes</code>","text":"<p>This step counts the number of click events per client (using the <code>context_id</code> attribute) in a time interval. This transform uses Beam\u2019s state and timers; a state and a timer is maintained for every client. The state is a list of recent timestamps of clicks from the current client and the timer will clear the state if there are no recent clicks from the client. If the number of elements in the list exceeds the set threshold, any additional clicks will be marked with a <code>click-status</code>. Because a state needs to be maintained per client, the memory required for this step increases with the number of unique clients.</p>"},{"location":"ingestion-beam/contextual-services-job/#aggregateimpressions","title":"<code>AggregateImpressions</code>","text":"<p>This step groups impressions together in a timed window based on the reporting URLs. The purpose of this step is to reduce the number of HTTP requests made to external endpoints. One URL is output for each unique reporting URL in the timed window by counting the number of occurrences of each URL. The output URL is the original reporting URL with the additional query parameters <code>impressions</code>, <code>begin-timestamp</code>, and <code>end-timestamp</code>.</p> <p>A Beam <code>IntervalWindow</code> is used to group by impressions together based on the processing time - the time at which the message enters this transform. The output is generated at the end of the window which means that there is an increased delay between when the impression enters the pipeline and when the corresponding reporting URL is requested. This transform needs to keep track of a window for every unique reporting URL it receives which means memory required increases with the number of unique URLs.</p>"},{"location":"ingestion-beam/contextual-services-job/#sendrequest","title":"<code>SendRequest</code>","text":"<p>This step sends a HTTP GET request to the URL specified by the <code>reporting_url</code> attribute in the message. To keep track of requests sent by the job for debugging purposes, successful requests will throw a <code>RequestContentException</code> which will add them to the BigQuery error table (see below).</p>"},{"location":"ingestion-beam/contextual-services-job/#bigquery-error-table","title":"BigQuery Error Table","text":"<p>The job is configured at certain steps to catch runtime exceptions and write the message contents to BigQuery. The configured table is <code>moz-fx-data-shared-prod.payload_bytes_error.contextual_services</code>. This can be used for debugging or to backfill messages that initially failed to process.</p>"},{"location":"ingestion-beam/contextual-services-job/#working-with-the-beam-job","title":"Working with the Beam Job","text":"<p>Options specific to this job are found in https://github.com/mozilla/gcp-ingestion/blob/main/ingestion-beam/src/main/java/com/mozilla/telemetry/contextualservices/ContextualServicesReporterOptions.java</p>"},{"location":"ingestion-beam/contextual-services-job/#test-deployment","title":"Test Deployment","text":"<p>This job can be deployed in a sandbox project for testing. The <code>contextual-services-dev</code> project is currently used for this purpose.</p> <p>There are a few required components to get a job running:</p> <ul> <li>A PubSub subscription on the republished <code>contextual-services</code> topic</li> <li>A BigQuery table with the <code>payload_bytes_error</code> schema used for error output</li> <li>A URL allowed list stored in GCS</li> <li>The Beam pipeline running on Dataflow, reading from the PubSub subscription, and writing to the BigQuery table</li> </ul> <p>Example script to start the Dataflow job from the ingestion-beam directory:</p> <pre><code>#!/bin/bash\n\nset -ux\n\nPROJECT=\"contextual-services-dev\"\nJOB_NAME=\"contextual-services\"\n\nmvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.ContextualServicesReporter -Dexec.args=\"\\\n   --runner=Dataflow \\\n   --jobName=$JOB_NAME \\\n   --project=$PROJECT  \\\n   --inputType=pubsub \\\n   --input='projects/contextual-services-dev/subscriptions/ctxsvc-input' \\\n   --outputTableRowFormat=payload \\\n   --errorBqWriteMethod=streaming \\\n   --errorOutputType=bigquery \\\n   --errorOutput=$PROJECT:contextual_services.reporting_errors \\\n   --region=us-central1 \\\n   --usePublicIps=true \\\n   --gcsUploadBufferSizeBytes=16777216 \\\n   --urlAllowList=gs://contextual-services-data-dev/urlAllowlist.csv \\\n   --allowedDocTypes=topsites-impression,topsites-click, \\\n   --reportingEnabled=false \\\n   --aggregationWindowDuration=10m \\\n   --clickSpikeWindowDuration=3m \\\n   --clickSpikeThreshold=10 \\\n   --logReportingUrls=true \\\n   --maxNumWorkers=2 \\\n   --numWorkers=1 \\\n   --autoscalingAlgorithm=THROUGHPUT_BASED \\\n\"\n</code></pre>"},{"location":"ingestion-beam/decoder-job/","title":"Decoder Job","text":"<p>A job for normalizing ingestion messages. Defined in the <code>com.mozilla.telemetry.Decoder</code> class (source).</p>"},{"location":"ingestion-beam/decoder-job/#transforms","title":"Transforms","text":"<p>These transforms are currently executed against each message in order.</p>"},{"location":"ingestion-beam/decoder-job/#geoip-lookup","title":"GeoIP Lookup","text":"<ol> <li>Extract <code>ip</code> from the <code>x_forwarded_for</code> attribute</li> <li>use the third-to-last value (since the second-to-last value is a forwarding rule IP added by      Google load balancer, and the last value is a Google load balancer IP added by nginx)</li> <li>Execute the following steps until one fails and ignore the exception</li> <li>Parse <code>ip</code> using <code>InetAddress.getByName</code></li> <li>Lookup <code>ip</code> in the configured <code>GeoIP2City.mmdb</code></li> <li>Extract <code>country.iso_code</code> as <code>geo_country</code></li> <li>Extract <code>city.name</code> as <code>geo_city</code> if <code>cities15000.txt</code> is not configured       or <code>city.geo_name_id</code> is in the configured <code>cities15000.txt</code></li> <li>Extract <code>subdivisions[0].iso_code</code> as <code>geo_subdivision1</code></li> <li>Extract <code>subdivisions[1].iso_code</code> as <code>geo_subdivision2</code></li> <li>Remove the <code>x_forwarded_for</code> and <code>remote_addr</code> attributes</li> <li>Remove any <code>null</code> values added to attributes</li> </ol>"},{"location":"ingestion-beam/decoder-job/#parse-uri","title":"Parse URI","text":"<p>Attempt to extract attributes from <code>uri</code>, on failure send messages to the configured error output.</p>"},{"location":"ingestion-beam/decoder-job/#decompress","title":"Decompress","text":"<p>Attempt to decompress payload with gzip, on failure pass the message through unmodified.</p>"},{"location":"ingestion-beam/decoder-job/#parse-payload","title":"Parse Payload","text":"<ol> <li>Parse the message body as a <code>UTF-8</code> encoded JSON payload</li> <li>Drop specific fields or entire messages that match a specific set of signatures    for toxic data that we want to make sure we do not store</li> <li>Maintain counter metrics for each type of dropped message</li> <li>Validate the payload structure based on the JSON schema for the specified    document type</li> <li>Invalid messages are routed to error output</li> <li>Extract some additional attributes such as <code>client_id</code> and <code>os_name</code>    based on the payload contents</li> </ol>"},{"location":"ingestion-beam/decoder-job/#parse-user-agent","title":"Parse User Agent","text":"<p>Attempt to extract browser, browser version, and os from the <code>user_agent</code> attribute, drop any nulls, and remove <code>user_agent</code> from attributes.</p>"},{"location":"ingestion-beam/decoder-job/#write-metadata-into-the-payload","title":"Write Metadata Into the Payload","text":"<p>Add a nested <code>metadata</code> field and several <code>normalized_*</code> attributes into the payload body.</p>"},{"location":"ingestion-beam/decoder-job/#executing","title":"Executing","text":"<p>Decoder jobs are executed the same way as sink jobs but with a few extra flags:</p> <ul> <li><code>-Dexec.mainClass=com.mozilla.telemetry.Decoder</code></li> <li>For Dataflow Flex Templates, change the <code>docker-compose</code> build argument to     <code>--build-arg FLEX_TEMPLATE_JAVA_MAIN_CLASS=com.mozilla.telemetry.Decoder</code></li> <li><code>--geoCityDatabase=/path/to/GeoIP2-City.mmdb</code></li> <li><code>--geoCityFilter=/path/to/cities15000.txt</code> (optional)</li> </ul> <p>To download the GeoLite2 database, you need to register for a MaxMind account to obtain a license key. After generating a new license key, set <code>MM_LICENSE_KEY</code> to your license key.</p> <p>Example:</p> <pre><code># create a test input file\nmkdir -p tmp/\necho '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"remote_addr\":\"63.245.208.195\"}}' &gt; tmp/input.json\n\n# Download `cities15000.txt`, `GeoLite2-City.mmdb`, and `schemas.tar.gz`\n./bin/download-cities15000\n./bin/download-schemas\n\nexport MM_LICENSE_KEY=\"Your MaxMind License Key\"\n./bin/download-geolite2\n\n\n# do geo lookup on messages to stdout\n./bin/mvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.Decoder -Dexec.args=\"\\\n    --geoCityDatabase=GeoLite2-City.mmdb \\\n    --geoCityFilter=cities15000.txt \\\n    --schemasLocation=schemas.tar.gz \\\n    --inputType=file \\\n    --input=tmp/input.json \\\n    --outputType=stdout \\\n    --errorOutputType=stderr \\\n\"\n\n# check the DecoderOptions help page for options specific to Decoder\n./bin/mvn compile exec:java -Dexec.args=--help=DecoderOptions\n\"\n</code></pre>"},{"location":"ingestion-beam/ingestion_testing_workflow/","title":"Ingestion Testing Workflow","text":"<p>The ingestion-beam handles data flow of documents from the edge into various sinks. You may be interested in standing up a small testing instance to validate the integration of the various components.</p> <p> Figure: An overview of the various components necessary to query BigQuery against data from a PubSub subscription.</p>"},{"location":"ingestion-beam/ingestion_testing_workflow/#setting-up-the-gcs-project","title":"Setting up the GCS project","text":"<p>Read through <code>whd/gcp-quickstart</code> for details about the sandbox environment that is provided by data operations.</p> <ul> <li>Install the Google Cloud SDK</li> <li>Navigate to the Google Cloud Console</li> <li>Create a new project under <code>firefox.gcp.mozilla.com/dataops/sandbox</code></li> <li><code>gcloud config set project &lt;PROJECT&gt;</code></li> <li>Create a PubSub subscription (see <code>gcp-quickstart/pubsub.sh</code>)</li> <li>Create a GCS bucket</li> <li><code>gsutil mb gs://&lt;PROJECT&gt;</code></li> <li>Enable the Dataflow API</li> <li>Create a service account and store the key locally</li> </ul>"},{"location":"ingestion-beam/ingestion_testing_workflow/#bootstrapping-schemas-from-mozilla-pipeline-schemas","title":"Bootstrapping schemas from <code>mozilla-pipeline-schemas</code>","text":"<ul> <li>Download the latest schemas from <code>mozilla-pipeline-schemas</code> using <code>bin/download-schemas</code>.</li> <li>This script may also inject testing resources into the resulting archive.</li> <li>A <code>schemas.tar.gz</code> will appear at the project root.</li> <li>Copy generated BigQuery schemas using <code>bin/copy-bq-schemas</code>.</li> <li>Schemas will be written to <code>bq-schemas/</code> with a <code>.bq</code> extension</li> <li>Schemas can be generated directly from JSON Schema using <code>bin/generate-bq-schemas</code> <code>bq-schemas/   \u251c\u2500\u2500 activity-stream.impression-stats.1.bq   \u251c\u2500\u2500 coverage.coverage.1.bq   \u251c\u2500\u2500 edge-validator.error-report.1.bq   \u251c\u2500\u2500 eng-workflow.bmobugs.1.bq   ....</code></li> <li>Update the BigQuery table in the current project using <code>bin/update-bq-table</code>.</li> <li>This may take several minutes. Read the script for usage information.</li> <li>Each namespace will be given its own dataset and each document type its own table.</li> <li>Verify that tables have been updated by viewing the BigQuery console.</li> <li>Download a copy of sampled documents using <code>bin/download-document-sample</code></li> <li>Upload this to your project's data bucket e.g. <code>gs://$PROJECT/data/</code></li> </ul>"},{"location":"ingestion-beam/ingestion_testing_workflow/#building-the-project","title":"Building the project","text":"<p>Follow the instructions of the project readme. Here is a quick-reference for a running a job from a set of files in GCS.</p> <pre><code># this must be an absolute path\nexport GOOGLE_APPLICATION_CREDENTIALS=keys.json\nPROJECT=$(gcloud config get-value project)\nBUCKET=\"gs://$PROJECT\"\n\npath=\"$BUCKET/data/*.ndjson\"\n\n# use local maven instead of the docker container in bin/mvn, otherwise make sure to mount\n# credentials into the proper location in the container\nmvn compile exec:java -Dexec.args=\"\\\n    --runner=Dataflow \\\n    --project=$PROJECT \\\n    --autoscalingAlgorithm=NONE \\\n    --workerMachineType=n1-standard-1 \\\n    --numWorkers=1 \\\n    --gcpTempLocation=$BUCKET/tmp \\\n    --inputFileFormat=json \\\n    --inputType=file \\\n    --input=$path\\\n    --outputType=bigquery \\\n    --output=$PROJECT:test_ingestion.\\${document_namespace}__\\${document_type}_v\\${document_version} \\\n    --bqWriteMethod=file_loads \\\n    --tempLocation=$BUCKET/temp/bq-loads \\\n    --errorOutputType=file \\\n    --errorOutput=$BUCKET/error/ \\\n\"\n</code></pre>"},{"location":"ingestion-beam/rally-job/","title":"Rally Decoder Job","text":"<p>The Rally decoder job is a variant of the decoder job defined in the <code>com.mozilla.telemetry.decoder.rally</code> package (source). The decoder supports the Rally data donation and sharing platform. More information can be found on the mana page.</p> <p>See bug 1628539 for initial implementation of the <code>pioneer-v2</code> decoder and bug 1697342 for implementation of the Glean.js encrypted pings.</p>"},{"location":"ingestion-beam/rally-job/#overview","title":"Overview","text":"<p>The Rally decoder includes three new options: <code>--pioneerEnabled</code>, <code>--pioneerMetadataLocation</code>, and <code>--pioneerKmsEnabled</code>. And example of running the job is as follows, which is captured in the <code>bin/run-pioneer-benchmark</code> script.</p> <pre><code>mvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.Decoder -Dexec.args=\"\\\n    --runner=Dataflow \\\n    --profilingAgentConfiguration='{\\\"APICurated\\\": true}'\n    --project=$project \\\n    --autoscalingAlgorithm=NONE \\\n    --workerMachineType=n1-standard-1 \\\n    --gcpTempLocation=$bucket/tmp \\\n    --numWorkers=2 \\\n    --region=us-central1 \\\n    --pioneerEnabled=true \\\n    --pioneerMetadataLocation=$bucket/$prefix/metadata/metadata.json \\\n    --pioneerKmsEnabled=false \\\n    --pioneerDecompressPayload=false \\\n    --geoIspDatabase=$bucket/$prefix/metadata/GeoIP2-ISP.mmdb  \\\n    --geoCityDatabase=$bucket/$prefix/metadata/GeoLite2-City.mmdb \\\n    --geoCityFilter=$bucket/$prefix/metadata/cities15000.txt \\\n    --schemasLocation=$bucket/$prefix/metadata/schemas.tar.gz \\\n    --inputType=file \\\n    --input=$bucket/$prefix/input/ciphertext/'part-*' \\\n    --outputType=file \\\n    --output=$bucket/$prefix/output/ciphertext/ \\\n    --errorOutputType=file \\\n    --errorOutput=$bucket/$prefix/error/ciphertext/ \\\n\"\n</code></pre> <p>The <code>--pioneerEnabled</code> flag enables the transform in the decoder pipeline, which comes before schema validation and payload processing. It uses the document specified by the <code>--pioneerMetadataLocation</code> to locate information for the <code>KeyStore</code>. The metadata location takes on the following form validated by this schema:</p> <pre><code>[\n  {\n    \"private_key_id\": \"rally-study-foo\",\n    \"private_key_uri\": \"src/test/resources/jwe/rally-study-foo.private.json\",\n    \"kms_resource_id\": \"projects/DUMMY_PROJECT_ID/locations/global/keyRings/test-ingestion-beam-integration/cryptoKeys/study-foo\"\n  },\n  ....\n]\n</code></pre> <p>The decoder reads the JSON Web Key into memory from the location in the <code>private_key_uri</code>. It can be encrypted using Cloud Key Management Service by specifying <code>kms_resource_id</code> and enabling the <code>--pioneerKmsEnabled</code> flag.</p> <p>The decoder decrypts pings that follow conventions for Rally or Pioneer pings. All encryption and decryption takes place using JSON Web Encryption (JWE). An envelope is a piece of metadata that surrounds the encrypted data. The Rally envelope is an object with a payload field containing a JWE compact object. After decrypting the payload, the ping takes the form of a Glean ping. The document namespace (as per the HTTP Edge Server Specification) is used to fetch the key from memory.</p> <p>The Pioneer ping's envelope uses the legacy mechanism of sending data through the Telemetry pipeline as a <code>telemetry.pioneer-study.4</code> ping. In addition, the envelope explicitly specifies the routing information for the ping. Finally, the decoder constructs a PubSub message that includes the routing information and decrypted message.</p>"},{"location":"ingestion-beam/rally-job/#notable-design-decisions","title":"Notable design decisions","text":"<p>Data SRE allocates each Rally study a JWK pair. The client must encode messages with the key to reach the analysis environment. The client may not always have the key, so there are exceptions for the enrollment and deletion pings. The decoder will ignore the payload and extract the pioneer id for these document types.</p>"},{"location":"ingestion-beam/republisher-job/","title":"Republisher Job","text":"<p>A job for republishing subsets of decoded messages to new destinations. Defined in the <code>com.mozilla.telemetry.Republisher</code> class (source).</p> <p>The primary intention is to produce smaller derived Pub/Sub topics so that consumers that only need a specific subset of messages don't incur the cost of reading the entire stream of decoded messages.</p>"},{"location":"ingestion-beam/republisher-job/#capabilities","title":"Capabilities","text":""},{"location":"ingestion-beam/republisher-job/#debug-republishing","title":"Debug Republishing","text":"<p>If <code>--enableDebugDestination</code> is set, messages containing an <code>x_debug_id</code> attribute will be republished to a destination that's configurable at runtime. This is currently expected to be a feature specific to structured ingestion, so should not be set for <code>telemetry-decoded</code> input.</p>"},{"location":"ingestion-beam/republisher-job/#per-doctype-republishing","title":"Per-<code>docType</code> Republishing","text":"<p>If <code>--perDocTypeEnabledList</code> is provided, a separate producer will be created for each <code>docType</code> specified in the given comma-separated list. See the <code>--help</code> output for details on format.</p>"},{"location":"ingestion-beam/republisher-job/#per-channel-sampled-republishing","title":"Per-Channel Sampled Republishing","text":"<p>If <code>--perChannelSampleRatios</code> is provided, a separate producer will be created for each specified release channel. The messages will be randomly sampled according to the ratios provided per channel. This is currently intended as a feature only for telemetry data, so should not be set for <code>structured-decoded</code> input. See the <code>--help</code> output for details on format.</p>"},{"location":"ingestion-beam/republisher-job/#executing","title":"Executing","text":"<p>Republisher jobs are executed the same way as sink jobs but with a few differences in flags. You'll need to set the <code>mainClass</code>:</p> <ul> <li><code>-Dexec.mainClass=com.mozilla.telemetry.Republisher</code></li> <li>For Dataflow Flex Templates, change the <code>docker-compose</code> build argument to     <code>--build-arg FLEX_TEMPLATE_JAVA_MAIN_CLASS=com.mozilla.telemetry.Republisher</code></li> </ul> <p>The <code>--outputType</code> flag is still required as in the sink, but the <code>--output</code> configuration is ignored for the Republisher. Instead, there is a separate destination configuration flag for each of the three republishing types. For each type, there is an compile-time option that affects what publishers are generated in the graph for the Dataflow job along with a runtime option that determines the specific location (usually a topic name) for each publisher.</p> <p>To enable debug republishing:</p> <ul> <li><code>--enableDebugDestination</code> (compile-time)</li> <li><code>--debugDestination=/some/pubsub/topic/path</code></li> </ul> <p>To enable per-<code>docType</code> republishing:</p> <ul> <li><code>--perDocTypeDestination='{\"/some/pubsub/topic/path/per-doctype-name\":[\"activity-stream/impression-stats\"]}'</code> (compile-time)</li> </ul> <p>To enable per-channel sampled republishing:</p> <ul> <li><code>--perChannelSampleRatios='{\"nightly\":1.0,\"beta\":0.1,\"release\":0.01}'</code> (compile-time)</li> <li><code>--perChannelDestination=/some/pubsub/topic/path/per-channel-${channel}</code> (compile-time)</li> </ul> <p>Example:</p> <pre><code># create a test input file\nmkdir -p tmp/\necho '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"x_debug_id\":\"mysession\"}}' &gt; tmp/input.json\n\n# Republish only messages with x_debug_id attribute to stdout.\n./bin/mvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.Republisher -Dexec.args=\"\\\n    --inputType=file \\\n    --input=tmp/input.json \\\n    --outputType=stdout \\\n    --errorOutputType=stderr \\\n    --enableDebugDestination\n\"\n\n# check the RepublisherOptions help page for options specific to Republisher\n./bin/mvn compile exec:java -Dexec.args=--help=RepublisherOptions\n\"\n</code></pre>"},{"location":"ingestion-beam/sink-job/","title":"Sink Job","text":"<p>A job for delivering messages between Google Cloud services. Defined in the <code>com.mozilla.telemetry.Sink</code> class (source).</p>"},{"location":"ingestion-beam/sink-job/#deprecated","title":"Deprecated","text":"<p>This job has been replaced by ingestion-sink for loading messages from Google Cloud PubSub into BigQuery.</p>"},{"location":"ingestion-beam/sink-job/#supported-input-and-outputs","title":"Supported Input and Outputs","text":"<p>Supported inputs:</p> <ul> <li>Google Cloud PubSub</li> <li>Google Cloud Storage</li> </ul> <p>Supported outputs:</p> <ul> <li>Google Cloud PubSub</li> <li>Google Cloud Storage</li> <li>Google Cloud BigQuery</li> <li>stdout</li> <li>stderr</li> </ul> <p>Supported error outputs, must include attributes and must not validate messages:</p> <ul> <li>Google Cloud PubSub</li> <li>Google Cloud Storage with JSON encoding</li> <li>stdout with JSON encoding</li> <li>stderr with JSON encoding</li> </ul>"},{"location":"ingestion-beam/sink-job/#encoding","title":"Encoding","text":"<p>Internally messages are stored and transported as PubsubMessage.</p> <p>Supported file formats for Cloud Storage are <code>json</code> or <code>text</code>. The <code>json</code> file format stores newline delimited JSON, encoding the field <code>payload</code> as a base64 string, and <code>attributeMap</code> as an optional object with string keys and values. The <code>text</code> file format stores newline delimited strings, encoding the field <code>payload</code> as <code>UTF-8</code>.</p> <p>We'll construct example inputs based on the following two values and their base64 encodings:</p> <pre><code>$ echo -en \"test\" | base64\ndGVzdA==\n\n$ echo -en \"test\\n\" | base64\ndGVzdAo=\n</code></pre> <p>Example <code>json</code> file:</p> <pre><code>{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"meta\":\"data\"}}\n{\"payload\":\"dGVzdAo=\",\"attributeMap\":null}\n{\"payload\":\"dGVzdA==\"}\n</code></pre> <p>The above file when stored in the <code>text</code> format:</p> <pre><code>test\ntest\n\ntest\n</code></pre> <p>Note that the newline embedded at the end of the second JSON message results in two text messages, one of which is blank.</p>"},{"location":"ingestion-beam/sink-job/#output-path-specification","title":"Output Path Specification","text":"<p>Depending on the specified output type, the <code>--output</code> path that you provide controls several aspects of the behavior.</p>"},{"location":"ingestion-beam/sink-job/#bigquery","title":"BigQuery","text":"<p>When <code>--outputType=bigquery</code>, <code>--output</code> is a <code>tableSpec</code> of form <code>dataset.tablename</code> or the more verbose <code>projectId:dataset.tablename</code>. The values can contain attribute placeholders of form <code>${attribute_name}</code>. To set dataset to the document namespace and table name to the document type, specify:</p> <pre><code>--output='${document_namespace}.${document_type}'\n</code></pre> <p>All <code>-</code> characters in the attributes will be converted to <code>_</code> per BigQuery naming restrictions. Additionally, document namespace and type values will be processed to ensure they are in snake case format (<code>untrustedModules</code> becomes <code>untrusted_modules</code>).</p> <p>Defaults for the placeholders using <code>${attribute_name:-default_value}</code> are supported, but likely don't make much sense since it's unlikely that there is a default table whose schema is compatible with all potential payloads. Instead, records missing an attribute required by a placeholder will be redirected to error output if no default is provided.</p>"},{"location":"ingestion-beam/sink-job/#protocol","title":"Protocol","text":"<p>When <code>--outputType=file</code>, <code>--output</code> may be prefixed by a protocol specifier to determine the target data store. Without a protocol prefix, the output path is assumed to be a relative or absolute path on the filesystem. To write to Google Cloud Storage, use a <code>gs://</code> path like:</p> <pre><code>--output=gs://mybucket/somdir/myfileprefix\n</code></pre>"},{"location":"ingestion-beam/sink-job/#attribute-placeholders","title":"Attribute placeholders","text":"<p>We support <code>FileIO</code>'s \"Dynamic destinations\" feature (<code>FileIO.writeDynamic</code>) where it's possible to route individual messages to different output locations based on properties of the message. In our case, we allow routing messages based on the <code>PubsubMessage</code> attribute map. Routing is accomplished by adding placeholders of form <code>${attribute_name:-default_value}</code> to the path.</p> <p>For example, to route based on a <code>document_type</code> attribute, your path might look like:</p> <pre><code>--output=gs://mybucket/mydocs/${document_type:-UNSPECIFIED}/myfileprefix\n</code></pre> <p>Messages with <code>document_type</code> of \"main\" would be grouped together and end up in the following directory:</p> <pre><code>gs://mybucket/mydocs/main/\n</code></pre> <p>Messages with <code>document_type</code> set to <code>null</code> or missing that attribute completely would be grouped together and end up in directory:</p> <pre><code>gs://mybucket/mydocs/UNSPECIFIED/\n</code></pre> <p>Note that placeholders must specify a default value so that a poorly formatted message doesn't cause a pipeline exception. A placeholder without a default will result in an <code>IllegalArgumentException</code> on pipeline startup.</p> <p>File-based outputs support the additional derived attributes <code>\"submission_date\"</code> and <code>\"submission_hour\"</code> which will be parsed from the value of the <code>submission_timestamp</code> attribute if it exists. These can be useful for making sure your output specification buckets messages into hourly directories.</p> <p>The templating and default syntax used here is based on the Apache commons-text <code>StringSubstitutor</code>, which in turn bases its syntax on common practice in bash and other Unix/Linux shells. Beware the need for proper escaping on the command line (use <code>\\$</code> in place of <code>$</code>), as your shell may try to substitute in values for your placeholders before they're passed to <code>Sink</code>.</p> <p>Google's PubsubMessage format allows arbitrary strings for attribute names and values. We place the following restrictions on attribute names and default values used in placeholders:</p> <ul> <li>attribute names may not contain the string <code>:-</code></li> <li>attribute names may not contain curly braces (<code>{</code> or <code>}</code>)</li> <li>default values may not contain curly braces (<code>{</code> or <code>}</code>)</li> </ul>"},{"location":"ingestion-beam/sink-job/#file-prefix","title":"File prefix","text":"<p>Individual files are named by replacing <code>:</code> with <code>-</code> in the default format discussed in the \"File naming\" section of Beam's <code>FileIO</code> Javadoc:</p> <pre><code>$prefix-$start-$end-$pane-$shard-of-$numShards$suffix$compressionSuffix\n</code></pre> <p>In our case, <code>$prefix</code> is determined from the last <code>/</code>-delimited piece of the <code>--output</code> path. If you specify a path ending in <code>/</code>, you'll end up with an empty prefix and your file names will begin with <code>-</code>. This is probably not what you want, so it's recommended to end your output path with a non-empty file prefix. We replace <code>:</code> with <code>-</code> because Hadoop can't handle <code>:</code> in file names.</p> <p>For example, given:</p> <pre><code>--output=/tmp/output/out\n</code></pre> <p>An output file might be:</p> <pre><code>/tmp/output/out--290308-12-21T20-00-00.000Z--290308-12-21T20-10-00.000Z-00000-of-00001.ndjson\n</code></pre>"},{"location":"ingestion-beam/sink-job/#executing","title":"Executing","text":"<p>Note: <code>-Dexec.args</code> does not handle newlines gracefully, but bash will remove <code>\\</code> escaped newlines in <code>\"</code>s.</p>"},{"location":"ingestion-beam/sink-job/#locally","title":"Locally","text":"<p>If you install Java and maven, you can invoke <code>mvn</code> in the following commands instead of using <code>./bin/mvn</code>; be aware, though, that Java 11 is the target JVM and some reflection warnings may be thrown on newer versions, though these are generally harmless.</p> <p>The provided <code>bin/mvn</code> script downloads and runs maven via docker so that less setup is needed on the local machine. For prolonged development performance is likely to be significantly better, especially in MacOS, if <code>mvn</code> is installed and run natively without docker.</p> <pre><code># create a test input file\nmkdir -p tmp/\necho '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' &gt; tmp/input.json\n\n# consume messages from the test file, decode and re-encode them, and write to a directory\n./bin/mvn compile exec:java -Dexec.args=\"\\\n    --inputFileFormat=json \\\n    --inputType=file \\\n    --input=tmp/input.json \\\n    --outputFileFormat=json \\\n    --outputType=file \\\n    --output=tmp/output/out \\\n    --errorOutputType=file \\\n    --errorOutput=tmp/error \\\n\"\n\n# check that the message was delivered\ncat tmp/output/*\n\n# write message payload straight to stdout\n./bin/mvn compile exec:java -Dexec.args=\"\\\n    --inputFileFormat=json \\\n    --inputType=file \\\n    --input=tmp/input.json \\\n    --outputFileFormat=text \\\n    --outputType=stdout \\\n    --errorOutputType=stderr \\\n\"\n\n# check the help page to see types of options\n./bin/mvn compile exec:java -Dexec.args=--help\n\n# check the SinkOptions help page for options specific to Sink\n./bin/mvn compile exec:java -Dexec.args=--help=SinkOptions\n</code></pre>"},{"location":"ingestion-beam/sink-job/#on-dataflow","title":"On Dataflow","text":"<pre><code># Pick a bucket to store files in\nBUCKET=\"gs://$(gcloud config get-value project)\"\n\n# create a test input file\necho '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' | gsutil cp - $BUCKET/input.json\n\n# Set credentials; beam is not able to use gcloud credentials\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/creds.json\"\n\n# consume messages from the test file, decode and re-encode them, and write to a bucket\n./bin/mvn compile exec:java -Dexec.args=\"\\\n    --runner=Dataflow \\\n    --inputFileFormat=json \\\n    --inputType=file \\\n    --input=$BUCKET/input.json \\\n    --outputFileFormat=json \\\n    --outputType=file \\\n    --output=$BUCKET/output \\\n    --errorOutputType=file \\\n    --errorOutput=$BUCKET/error \\\n\"\n\n# wait for the job to finish\ngcloud dataflow jobs list\n\n# check that the message was delivered\ngsutil cat $BUCKET/output/*\n</code></pre>"},{"location":"ingestion-beam/sink-job/#on-dataflow-with-flex-templates","title":"On Dataflow with Flex Templates","text":"<p>The Dataflow templates documentation explains:</p> <p>Dataflow templates allow you to stage your pipelines on Google Cloud and run them using the Google Cloud Console, the <code>gcloud</code> command-line tool, or REST API calls. [...] Flex Templates package the pipeline as a Docker image and stage these images on your project's Container Registry.</p> <pre><code># pick the project to store the docker image in\nPROJECT=$(gcloud config get-value project)\"\n\n# pick the region to run Dataflow jobs in\nPROJECT=$(gcloud config get-value compute/region)\"\n\n# pick the bucket to store files in\nBUCKET=\"gs://$PROJECT\"\n\n# configure gcloud credential helper for docker to push to GCR\ngcloud auth configure-docker\n\n# build the docker image for the Flex Template\nexport IMAGE=gcr.io/$PROJECT/ingestion-beam/sink:latest\ndocker-compose build --build-arg FLEX_TEMPLATE_JAVA_MAIN_CLASS=com.mozilla.telemetry.Sink\ndocker-compose push\n\n# create the Flex Template\ngcloud dataflow flex-template build \\\n    $BUCKET/sink/flex-templates/latest.json \\\n    --image $IMAGE \\\n    --sdk-language JAVA\n\n# create a test input file\necho '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' | gsutil cp - $BUCKET/input.json\n\n# run the dataflow Flex Template with gcloud\nJOBNAME=file-to-file1\nREGION=$(gcloud config get-value compute/region 2&gt;&amp;1 | sed 's/(unset)/us-central1/')\ngcloud dataflow flex-template run $JOBNAME \\\n    --template-file-gcs-location=$BUCKET/sink/flex-templates/latest.json \\\n    --region=$REGION \\\n    --parameters=inputType=file \\\n    --parameters=outputType=file \\\n    --parameters=errorOutputType=file \\\n    --parameters=inputFileFormat=json \\\n    --parameters=outputFileFormat=json \\\n    --parameters=input=$BUCKET/input.json \\\n    --parameters=output=$BUCKET/output/ \\\n    --parameters=errorOutput=$BUCKET/error/\n\n# get the job id\nJOB_ID=\"$(gcloud dataflow jobs list --region=$REGION --filter=name=$JOBNAME --format='value(JOB_ID)' --limit=1)\"\n\n# wait for the job to finish\ngcloud dataflow jobs show \"$JOB_ID\" --region=$REGION\n\n# check that the message was delivered\ngsutil cat $BUCKET/output/*\n</code></pre>"},{"location":"ingestion-beam/sink-job/#in-streaming-mode","title":"In streaming mode","text":"<p>If <code>--inputType=pubsub</code>, Beam will execute in streaming mode, requiring some extra configuration for file-based outputs. You will need to specify sharding like:</p> <pre><code>    --outputNumShards=10 \\\n    --errorOutputNumShards=10 \\\n</code></pre> <p>or for Flex Templates:</p> <pre><code>    --parameters=outputNumShards=10 \\\n    --parameters=errorOutputNumShards=10 \\\n</code></pre> <p>As discussed in the Beam documentation for <code>FileIO.Write#withNumShards</code>, batch mode is most efficient when the runner is left to determine sharding, so <code>numShards</code> options should normally be left to their default of <code>0</code>, but streaming mode can't perform the same optimizations thus an exception will be thrown during pipeline construction if sharding is not specified. As codified in apache/beam/pull/1952, the Dataflow runner suggests a reasonable starting point <code>numShards</code> is <code>2 * maxWorkers</code> or 10 if <code>--maxWorkers</code> is unspecified.</p>"},{"location":"ingestion-edge/","title":"Ingestion Edge Service","text":"<p>A simple service for delivering HTTP messages to Google Cloud PubSub</p> <p>The source code lives in the ingestion-edge subdirectory of the gcp-ingestion repository.</p>"},{"location":"ingestion-edge/#building","title":"Building","text":"<p>We assume that you have docker-compose installed.</p> <p>From inside the <code>ingestion-edge</code> subdirectory:</p> <pre><code># docker-compose\ndocker-compose build\n\n# pytest\nbin/build\n</code></pre>"},{"location":"ingestion-edge/#running","title":"Running","text":"<p>Use <code>docker-compose</code> to run a local development server that auto-detects changes:</p> <pre><code># run the web server and PubSub emulator\ndocker-compose up --detach web\n\n# manually check the server\ncurl http://localhost:8000/__version__\ncurl http://localhost:8000/__heartbeat__\ncurl http://localhost:8000/__lbheartbeat__\ncurl http://localhost:8000/submit/test -d \"test\"\n\n# check web logs\ndocker-compose logs web\n\n# clean up docker-compose environment\ndocker-compose down --timeout 0\n</code></pre>"},{"location":"ingestion-edge/#configuration","title":"Configuration","text":"<p>The ingestion-edge docker container accepts these configuration options from environment variables:</p> <ul> <li><code>ROUTE_TABLE</code>: a JSON list of mappings from <code>uri</code> to PubSub topic, <code>uri</code>   matches are detected in order, defaults to <code>[]</code>, each mapping is a list and   may include an optional third element that specifies a list of allowed   methods instead of the default <code>[\"POST\",\"PUT\"]</code></li> <li><code>QUEUE_PATH</code>: a filesystem path to a directory where a SQLite database will   be created to store requests for when PubSub is unavailable, paths may be   relative to the docker container <code>WORKDIR</code>, defaults to <code>queue</code></li> <li><code>MINIMUM_DISK_FREE_BYTES</code>: an integer indicating the threshold of free bytes   on the filesystem where <code>QUEUE_PATH</code> is mounted below which <code>/__heartbeat__</code>   will fail, defaults to <code>0</code> which disables the check</li> <li><code>METADATA_HEADERS</code>: a JSON list of headers to preserve as PubSub message   attributes, defaults to <code>[\"Content-Length\", \"Date\", \"DNT\", \"User-Agent\", \"X-Forwarded-For\", \"X-Pingsender-Version\", \"X-Pipeline-Proxy\", \"X-Debug-ID\", \"X-Telemetry-Agent\", \"X-Source-Tags\", \"X-Foxsec-IP-Reputation\", \"X-LB-Tags\"]</code>;   the message attribute name will be the header name in lowercase and with <code>-</code>   converted to <code>_</code></li> <li><code>PUBLISH_TIMEOUT_SECONDS</code>: a float indicating the maximum number of seconds   to wait for the PubSub client to complete a publish operation, defaults to 1   second and may require tuning</li> <li><code>FLUSH_CONCURRENT_MESSAGES</code>: an integer indicating the number of messages per   worker that may be read from the queue before waiting on publish results,   defaults to 1000 messages based on publish request   limits and may   require tuning</li> <li><code>FLUSH_CONCURRENT_BYTES</code>: an integer indicating the number of bytes per   worker that may be read from the queue before waiting on publish results,   which may be exceeded by one message and measures data bytes rather than   serialized message size, defaults to 10MB based on publish request   limits and may   require tuning</li> <li><code>FLUSH_SLEEP_SECONDS</code>: a float indicating the number of seconds waited   between flush attempts, defaults to 1 second and may require tuning</li> </ul>"},{"location":"ingestion-edge/#testing","title":"Testing","text":"<p>Run tests with CircleCI Local CLI, <code>docker-compose</code>, or <code>pytest</code> wrappers</p> <pre><code># circleci\n(cd .. &amp;&amp; circleci build --job ingestion-edge)\n\n# docker-compose\ndocker-compose run --rm test\n\n# pytest wrapper (pytest-all calls lint and pytest)\n./bin/pytest-all\n</code></pre> <p>The <code>pytest</code> wrappers add these options via the environment:</p> <ul> <li><code>CLEAN_RELOCATES</code> controls whether <code>bin/lint</code> and <code>bin/pytest</code> will remove   <code>.pyc</code> files not in <code>venv/</code> that do not contain <code>$PWD</code> to prevent errors when   switching between running in and out of docker, defaults to <code>true</code></li> <li><code>VENV</code> controls whether to use a python <code>venv</code> in <code>venv/$(uname)</code> in   <code>bin/lint</code> and <code>bin/pytest</code>, and in <code>bin/build</code> to create and use that   <code>venv</code>, defaults to <code>false</code> in <code>Dockerfile</code> and <code>true</code> otherwise</li> <li><code>REQUIRE_CODE_COVERAGE</code> controls whether <code>bin/pytest-all</code> will fail if   code coverage is less than 100%, defaults to <code>true</code></li> <li>Use comments like <code># pragma: no cov</code> to disable coverage for things that     cannot be reasonably covered in CircleCI, as described in the <code>coverage.py</code>     docs</li> </ul>"},{"location":"ingestion-edge/#style-checks","title":"Style Checks","text":"<p>Run style checks</p> <pre><code># docker-compose\ndocker-compose run --rm test bin/lint\n\n# pytest wrapper\n./bin/lint\n</code></pre>"},{"location":"ingestion-edge/#unit-tests","title":"Unit Tests","text":"<p>Run unit tests</p> <pre><code># docker-compose\ndocker-compose run --rm test bin/pytest tests/unit\n\n# pytest wrapper\n./bin/pytest tests/unit\n</code></pre>"},{"location":"ingestion-edge/#integration-tests","title":"Integration Tests","text":"<p>Run integration tests locally</p> <pre><code># docker-compose\ndocker-compose run --rm test bin/pytest tests/integration\n\n# pytest wrapper\n./bin/pytest tests/integration\n</code></pre> <p>Test a remote server (requires credentials to read PubSub)</p> <pre><code># define the same ROUTE_TABLE as your edge server\nexport ROUTE_TABLE='[[\"/submit/telemetry/&lt;suffix:path&gt;\",\"projects/PROJECT/topics/TOPIC\"]]'\n\n# docker using latest image and no git checkout\ndocker run --rm --tty --interactive --env ROUTE_TABLE mozilla/ingestion-edge:latest bin/pytest tests/integration --server https://myedgeserver.example.com\n\n# docker-compose\ndocker-compose run --rm -e ROUTE_TABLE test bin/pytest tests/integration --server https://myedgeserver.example.com\n\n# pytest wrapper\n./bin/pytest tests/integration --server https://myedgeserver.example.com\n</code></pre>"},{"location":"ingestion-edge/#load-tests","title":"Load Tests","text":"<p>Run a load test (defaults to a single GKE cluster and a PubSub emulator)</p> <pre><code># docker using latest image and no git checkout\ndocker run --rm --tty --interactive mozilla/ingestion-edge:latest bin/pytest tests/load\n\n# docker-compose\ndocker-compose run --rm test bin/pytest tests/load\n\n# pytest\n./bin/pytest tests/load\n</code></pre> <p>Load test options (from <code>./bin/test -h</code>)</p> <pre><code>  --min-success-rate=MIN_SUCCESS_RATE\n                        Minimum 200 responses per non-200 response to require\n                        during --test-period, default is 1000 (0.1% errors)\n  --min-throughput=MIN_THROUGHPUT\n                        Minimum 200 responses per second to require during\n                        --test-period, default is 15000\n  --test-period=TEST_PERIOD\n                        Number of seconds to evaluate after warmup, default is\n                        1800 (30 minutes)\n  --warmup-threshold=WARMUP_THRESHOLD\n                        Minimum 200 responses per second that indicate warmup\n                        is complete, default is 15000\n  --warmup-timeout=WARMUP_TIMEOUT\n                        Maximum number of seconds to wait for warmup to\n                        complete, default is 600 (10 minutes)\n  --cluster=CLUSTER     Name of GKE cluster to create for test resources,\n                        default is 'load-test', ignored when --load-balancer\n                        and --no-traffic-generator are both specified\n  --location=LOCATION   Location to use for --cluster, default is us-west1\n  --preemptible         Use preemptible instances for --cluster, default is\n                        False\n  --project=PROJECT     Project to use for --cluster, default is from\n                        credentials\n  --load-balancer=LOAD_BALANCER\n                        Load Balancing url map to monitor, implies --no-\n                        generator when --server-uri is not specified, ignores\n                        --image and --no-emulator\n  --server-uri=SERVER_URI\n                        Server uri like 'https://edge.stage.domain.com/submit/\n                        telemetry/suffix', ignored when --no-generator is\n                        specified or --load-balancer is missing\n  --image=IMAGE         Docker image for server deployment, default is\n                        'mozilla/ingestion-edge:latest', ignored when --load-\n                        balancer is specified\n  --no-emulator         Don't use a PubSub emulator, ignored when --load-\n                        balancer is specified\n  --topic=TOPIC         PubSub topic name, default is 'topic', ignored when\n                        --load-balancer is specified\n  --no-generator        Don't deploy a traffic generator, ignore --script\n  --script=SCRIPT       Lua script to use for traffic generator deployment,\n                        default is 'tests/load/wrk/telemetry.lua', ignored\n                        when --no-generator is specified\n</code></pre>"},{"location":"ingestion-sink/","title":"Ingestion Sink","text":"<p>A Java application that runs in Kubernetes, reading input from Google Cloud Pub/Sub and emitting records to batch-oriented outputs like GCS or BigQuery. Defined in the <code>ingestion-sink</code> package (source).</p>"},{"location":"ingestion-sink/#supported-input-and-outputs","title":"Supported Input and Outputs","text":"<p>Supported inputs:</p> <ul> <li>Google Cloud PubSub</li> </ul> <p>Supported outputs:</p> <ul> <li>Google Cloud PubSub</li> <li>Google Cloud Storage</li> <li>Google Cloud BigQuery</li> </ul>"},{"location":"ingestion-sink/#test-input-and-output","title":"Test Input and Output","text":"<p>Test inputs will stop when an exception is raised or the end of the pipe or file is reached. Supported test inputs:</p> <ul> <li><code>System.in</code> (stdin), by setting <code>INPUT_PIPE</code> to any of <code>-</code>, <code>0</code>, <code>in</code>,   <code>stdin</code>, <code>/dev/stdin</code></li> <li>A single file, by setting <code>INPUT_PIPE</code> to <code>/path/to/input_file</code></li> </ul> <p>Test outputs don't exercise batching and will write messages as newline delimited JSON in the order they are received. Supported test outputs:</p> <ul> <li><code>System.out</code> (stdout), by setting <code>OUTPUT_PIPE</code> to any of <code>-</code>, <code>1</code>, <code>out</code>,   <code>stdout</code>, <code>/dev/stdout</code></li> <li><code>System.err</code> (stderr), by setting <code>OUTPUT_PIPE</code> to any of <code>2</code>, <code>err</code>,   <code>stderr</code>, <code>/dev/stderr</code></li> <li>A single file, by setting <code>OUTPUT_PIPE</code> to <code>/path/to/output_file</code></li> </ul>"},{"location":"ingestion-sink/#configuration","title":"Configuration","text":"<p>All configuration is controlled by environment variables.</p>"},{"location":"ingestion-sink/#output-specification","title":"Output Specification","text":"<p>Depending on the environment variables provided, the application will automatically determine where to deliver messages.</p> <p>If <code>OUTPUT_BUCKET</code> is specified without <code>BIG_QUERY_OUTPUT_MODE</code>, then messages will be delivered to Google Cloud Storage.</p> <p>If <code>OUTPUT_TOPIC</code> is specified without <code>OUTPUT_BUCKET</code> or <code>BIG_QUERY_OUTPUT_MODE</code>, then messages will be delivered to Google Cloud Pub/Sub.</p> <p>If <code>OUTPUT_TABLE</code> is specified without <code>BIG_QUERY_OUTPUT_MODE</code> or with <code>BIG_QUERY_OUTPUT_MODE=streaming</code>, then messages will be delivered to BigQuery via the streaming API.</p> <p>If <code>OUTPUT_TABLE</code> is specified with <code>BIG_QUERY_OUTPUT_MODE=file_loads</code>, then messages will be delivered to Google Cloud Storage based on <code>OUTPUT_BUCKET</code> and for each blob a notification will be delivered to Google Cloud Pub/Sub based on <code>OUTPUT_TOPIC</code>. Separate instances of ingestion-sink must consume notifications from Google Cloud Pub/Sub and deliver messages to BigQuery via load jobs.</p> <p>If <code>OUTPUT_TABLE</code> is specified with <code>BIG_QUERY_OUTPUT_MODE=mixed</code>, then messages will be delivered to BigQuery via both the streaming API and load jobs, and <code>OUTPUT_BUCKET</code> is required. If <code>OUTPUT_TOPIC</code> is specified then it will be used the same as with <code>BIG_QUERY_OUTPUT_MODE=file_loads</code>, otherwise load jobs will be submitted by each running instance of ingestion-sink.</p> <p>If none of the above configuration options are provided, then messages must be notifications from <code>BIG_QUERY_OUTPUT_MODE=file_loads</code> or <code>BIG_QUERY_OUTPUT_MODE=mixed</code>, and the blobs they indicate will be submitted to BigQuery via load jobs.</p>"},{"location":"ingestion-sink/#bigquery","title":"BigQuery","text":"<p><code>OUTPUT_TABLE</code> must be a <code>tableSpec</code> of form <code>dataset.tablename</code> or the more verbose <code>projectId.dataset.tablename</code>. The values can contain attribute placeholders of form <code>${attribute_name}</code>. To set dataset to the document namespace and table name to the document type, specify:</p> <pre><code>OUTPUT_TABLE='${document_namespace}.${document_type}'\n</code></pre> <p>All <code>-</code> characters in the attributes will be converted to <code>_</code> per BigQuery naming restrictions. Additionally, document namespace and type values will be processed to ensure they are in snake case format (<code>untrustedModules</code> becomes <code>untrusted_modules</code>).</p> <p>Defaults for the placeholders using <code>${attribute_name:-default_value}</code> are supported, but likely don't make much sense since it's unlikely that there is a default table whose schema is compatible with all potential payloads.</p>"},{"location":"ingestion-sink/#attribute-placeholders","title":"Attribute placeholders","text":"<p>We support routing individual messages to different output locations based on the <code>PubsubMessage</code> attribute map. Routing is accomplished by adding placeholders of form <code>${attribute_name:-default_value}</code> to the path.</p> <p>For example, to route based on a <code>document_type</code> attribute, your path might look like:</p> <pre><code>OUTPUT_BUCKET=gs://mybucket/mydocs/${document_type:-UNSPECIFIED}/myfileprefix\n</code></pre> <p>Messages with <code>document_type</code> of \"main\" would be grouped together and end up in the following directory:</p> <pre><code>gs://mybucket/mydocs/main/\n</code></pre> <p>Messages with <code>document_type</code> set to <code>null</code> or missing that attribute completely would be grouped together and end up in directory:</p> <pre><code>gs://mybucket/mydocs/UNSPECIFIED/\n</code></pre> <p>Note that placeholders must specify a default value so that a poorly formatted message doesn't cause a pipeline exception. A placeholder without a default will result in an <code>IllegalArgumentException</code> on pipeline startup.</p> <p>File-based outputs support the additional derived attributes <code>\"submission_date\"</code> and <code>\"submission_hour\"</code> which will be parsed from the value of the <code>submission_timestamp</code> attribute if it exists. These can be useful for making sure your output specification buckets messages into hourly directories.</p> <p>The templating and default syntax used here is based on the Apache commons-text <code>StringSubstitutor</code>, which in turn bases its syntax on common practice in bash and other Unix/Linux shells. Beware the need for proper escaping on the command line (use <code>\\$</code> in place of <code>$</code>), as your shell may try to substitute in values for your placeholders before they're passed to <code>Sink</code>.</p> <p>Google's PubsubMessage format allows arbitrary strings for attribute names and values. We place the following restrictions on attribute names and default values used in placeholders:</p> <ul> <li>attribute names may not contain the string <code>:-</code></li> <li>attribute names may not contain curly braces (<code>{</code> or <code>}</code>)</li> <li>default values may not contain curly braces (<code>{</code> or <code>}</code>)</li> </ul>"},{"location":"ingestion-sink/#encoding","title":"Encoding","text":"<p>When writing messages to Google Cloud Storage or BigQuery, the message received from Google Cloud Pub/Sub will be encoded as a JSON object.</p> <p>When <code>OUTPUT_FORMAT</code> is unspecified or <code>raw</code>, messages will have bytes encoded as a <code>\"payload\"</code> field with base64 encoding, and each attribute encoded as field. This is the format used for <code>payload_bytes_raw.*</code> tables.</p> <p>When <code>OUTPUT_FORMAT</code> is <code>decoded</code> messages will have bytes encoded as with <code>OUTPUT_FORMAT=raw</code>, but attributes will be encoded using the nested metadata format of decoded pings. This is the format used for <code>payload_bytes_decoded.*</code> tables.</p> <p>When <code>OUTPUT_FORMAT</code> is <code>payload</code> messages will have bytes decoded as JSON, and will be transformed to coerce types and use snake case for compatibility with BigQuery. This is the format used for <code>*_live.*</code> tables. This requires specifying a local path to a gzipped tar archive that contains BigQuery table schemas as <code>SCHEMAS_LOCATION</code>. If messages bytes are compressed then <code>INPUT_COMPRESSION=gzip</code> must also be specified to ensure they are decompressed before they are decoded as JSON.</p> <p>When <code>OUTPUT_FORMAT</code> is <code>beam</code> messages will have bytes encoded as with <code>OUTPUT_FORMAT=raw</code>, but attributes will be encoded as an <code>\"attributeMap\"</code> field that contains a JSON object. This is the same format as produced by ingestion-beam when using <code>--outputType=file</code> and <code>--outputFileFormat=json</code>.</p>"},{"location":"ingestion-sink/#google-cloud-storage-file-prefix","title":"Google Cloud Storage file prefix","text":"<p>Google Cloud Storage files are named like:</p> <pre><code>$OUTPUT_BUCKET/{UUID.randomUUID().toString()}.ndjson\n</code></pre> <p>or if <code>OUTPUT_TABLE</code> and <code>BIG_QUERY_OUTPUT_MODE</code> are specified:</p> <pre><code>$OUTPUT_BUCKET/OUTPUT_TABLE=$OUTPUT_TABLE/{UUID.randomUUID().toString()}.ndjson\n</code></pre> <p>for example, with <code>OUTPUT_BUCKET=gs://test-bucket/test-output</code>:</p> <pre><code>gs://test-bucket/test-output/ad715b24-7500-45e2-9691-cb91e3b9c2cc.ndjson\n</code></pre> <p>or with <code>OUTPUT_BUCKET=gs://test-bucket/test-output</code>, <code>OUTPUT_TABLE=my_dataset.raw_table</code>, and <code>BIG_QUERY_OUTPUT_MODE=file_loads</code>:</p> <pre><code>gs://test-bucket/test-output/OUTPUT_TABLE=my_dataset.raw_table/3b17c648-f8b9-4250-bdc1-5c2e472fdc26.ndjson\n</code></pre>"},{"location":"ingestion-sink/#executing","title":"Executing","text":""},{"location":"ingestion-sink/#locally-with-docker","title":"Locally with Docker","text":"<p>The provided <code>bin/mvn</code> script downloads and runs maven via docker so that less setup is needed on the local machine. For prolonged development performance is likely to be significantly better, especially in MacOS, if <code>mvn</code> is installed and run natively without docker.</p> <pre><code># create a test input file\nmkdir -p tmp/\necho '{\"payload\":\"dGVzdA==\",\"attributeMap\":{\"host\":\"test\"}}' &gt; tmp/input.ndjson\n\n# consume messages from the test file, decode and re-encode them, and write to a directory\nPASS_ENV=\"INPUT_PIPE=tmp/input.ndjson OUTPUT_PIPE=tmp/output.ndjson\" ./bin/mvn compile exec:java\n\n# check that the message was delivered\ncat tmp/output.ndjson\n\n# read message from stdin and write to stdout\ncat tmp/input.ndjson | PASS_ENV=\"INPUT_PIPE=- OUTPUT_PIPE=-\" ./bin/mvn compile exec:java\n\n# read message from stdin and write to gcs\n# note that $ needs to be escaped with \\ to prevent shell substitution\ncat tmp/input.ndjson | PASS_ENV=\"\\\n  INPUT_PIPE=- \\\n  OUTPUT_BUCKET=gs://my_bucket/\\${document_type:-UNSPECIFIED}/ \\\n\" ./bin/mvn compile exec:java\n</code></pre>"},{"location":"ingestion-sink/#locally-without-docker","title":"Locally without Docker","text":"<p>If you install Java and maven, you can invoke <code>VAR=... mvn</code> in the above commands instead of using <code>PASS_ENV=\"VAR=...\" ./bin/mvn</code>. Be aware that Java 11 is the target JVM and some reflection warnings may be thrown on newer versions. Though these are generally harmless, you may need to comment out the <code>&lt;compilerArgument&gt;-Werror&lt;/compilerArgument&gt;</code> line in the <code>pom.xml</code> in the git root.</p> <pre><code># consume messages from the test file, decode and re-encode them, and write to a directory\nINPUT_PIPE=tmp/input.ndjson OUTPUT_PIPE=tmp/output.ndjson mvn compile exec:java\n\n# read message from stdin and write to stdout\ncat tmp/input.ndjson | INPUT_PIPE=- OUTPUT_PIPE=- ./bin/mvn compile exec:java\n</code></pre>"}]}