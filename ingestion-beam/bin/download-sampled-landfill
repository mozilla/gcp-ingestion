#!/usr/bin/env python3

"""Download samples of raw data from the AWS ingestion pipeline into ndjson.

The sanitized-landfill-sample.v3 dataset contain newline delimited json with the
JSON payload stored as a string. This sampled data is used to run integration
tests for the avro file-sink using generated avro schemas from
mozilla-pipeline-schemas.

This script requires boto3 and python-rapidjson to be installed on the system.
Additionally, the credentials for accessing
"telemetry-parquet/sanitized-landfill-sample" should be available on the system.
"""

import argparse
import base64
import logging
import json
import os
import tarfile
from datetime import datetime, timedelta
from functools import partial

import boto3
import rapidjson


INGESTION_BEAM_ROOT = os.path.realpath(
    os.path.join(os.path.dirname(os.path.realpath(__file__)), "..")
)


def parse_schema_name_archive(path):
    """Given a directory path to a json schema in the mps directory, generate
    the fully qualified name in the form `{namespace}.{doctype}.{docver}`."""
    elements = path.split("/")
    doctype, docver = elements[-1].split(".")[:-2]
    namespace = elements[-3]
    return f"{namespace}.{doctype}.{docver}"


def load_schemas(path):
    """Return a dictionary containing "{namespace}.{doctype}.{docver}" to validator"""
    schemas = {}
    with tarfile.open(path, "r") as tf:
        paths = [
            name
            for name in tf.getnames()
            if name.endswith(".schema.json") and name.split("/")[1] == "schemas"
        ]
        for path in paths:
            name = parse_schema_name_archive(path)
            schemas[name] = rapidjson.Validator(tf.extractfile(path).read())
    return schemas


def parse_schema_name_sample(key):
    """Return the fully qualified name in the form of `{namespace}.{doctype}.{docver}` for
    a key in s3 corresponding to `sanitized-landfill-sample`.

    Example path:
    ```
    sanitized-landfill-sample/v3/
        submission_date_s3=20190308/
        namespace=webpagetest/
        doc_type=webpagetest-run/
        doc_version=1/
            part-00122-tid-2954272513278013416-c06a39af-9979-41a5-8459-76412a4554b3-650.c000.json
    ```
    """
    params = dict([x.split("=") for x in key.split("/") if "=" in x])
    return ".".join(map(params.get, ["namespace", "doc_type", "doc_version"]))


def read_documents(schemas, bucket, prefix):
    """A generator for reading documents in the sampled landfill dataset that exist in the schemas."""
    s3 = boto3.client("s3")

    # enumerate all keys that end in json
    objects = s3.list_objects(Bucket=bucket, Prefix=prefix)
    keys = [obj["Key"] for obj in objects["Contents"] if obj["Key"].endswith(".json")]

    # yield the (fully-qualified name, ndjson string) for all valid keys
    for key in keys:
        name = parse_schema_name_sample(key)
        if name not in schemas:
            logging.info("schema does not exist for {}".format(name))
            continue
        data = (
            s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8").strip()
        )
        yield name, data


def generate_pubsub_message(namespace, doctype, docver, payload):
    """Generate a valid pubsub message that can be used for integration testing."""
    document = {
        "attributeMap": {
            "document_namespace": namespace,
            "document_type": doctype,
            "document_version": docver,
        },
        # payload is already a string
        "payload": base64.b64encode(payload.encode("utf-8")).decode("utf-8"),
    }
    return json.dumps(document)


def write_pubsub(fp, schemas, name, document):
    """Write sampled pings into serialized pubsub messages."""
    logging.info(f"Generating messages for {name}")

    success = 0
    error = 0
    namespace, doctype, docver = name.split(".")

    for line in document.split("\n"):
        content = json.loads(line).get("content")
        try:
            schemas[name](content)
        except ValueError:
            error += 1
            continue
        message = generate_pubsub_message(namespace, doctype, docver, content)
        fp.write(message + "\n")
        success += 1

    logging.info(f"Wrote {success} documents, skipped {error}")


def main(args):
    os.chdir(INGESTION_BEAM_ROOT)
    schemas = load_schemas(args.schema_file)

    bucket = args.bucket
    prefix = f"{args.prefix}/submission_date_s3={args.submission_date}"
    documents = read_documents(schemas, bucket, prefix)

    with open(args.output_file, "w") as fp:
        for name, document in documents:
            write_pubsub(fp, schemas, name, document)

    logging.info("Done!")


def parse_arguments():
    parser = argparse.ArgumentParser("download-sampled-landfill")
    parser.add_argument("--bucket", default="telemetry-parquet")
    parser.add_argument("--prefix", default="sanitized-landfill-sample/v3")
    parser.add_argument(
        "--submission-date", default=(datetime.now() - timedelta(1)).strftime("%Y%m%d")
    )
    parser.add_argument("--schema-file", default="schemas.tar.gz")
    parser.add_argument("--output-file", default="avro-landfill-integration.ndjson")
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main(parse_arguments())
